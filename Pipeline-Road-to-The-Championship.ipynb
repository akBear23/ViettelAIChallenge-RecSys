{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b0bd5b-ab7c-4f5f-b23d-7fd32c050577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Change file here\n",
    "pwd = os.getcwd()\n",
    "train_file = f\"{pwd}/data/training_set.csv\"\n",
    "public_test_file = f\"{pwd}data/public_testset.csv\"\n",
    "private_test_file = f\"{pwd}/data/test_set_private.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a102dc6-7a9b-4cab-9d4a-0ccd6d00db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store runs predict path\n",
    "out_path = f\"{pwd}/runs/private-test-attempt\"\n",
    "import os \n",
    "os.makedirs(out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f765770-0887-44e4-8527-39b66f25f1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 22 08:35:07 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 12.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000    Off  | 00000000:01:00.0 Off |                  Off |\n",
      "| 41%   40C    P8     9W / 140W |      2MiB / 16117MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000    Off  | 00000000:21:00.0 Off |                  Off |\n",
      "| 41%   37C    P8     9W / 140W |      2MiB / 16117MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1095162e-2472-4666-bc16-23897966b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f5efa-f6d2-4233-b0ef-9fc694f3c896",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SELFRec Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f3eb5c-a572-400c-a7be-c7a5f156b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/VHAC-track-ds/SELFRec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids-24.10/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd $pwd/SELFRec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a1e79fdf-4f52-4089-94e2-840ad15e22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from data.loader import FileIO\n",
    "from util.conf import ModelConf\n",
    "from util.sampler import next_batch_pairwise\n",
    "from util.loss_torch import bpr_loss, l2_reg_loss, InfoNCE\n",
    "\n",
    "from model.graph.LightGCN import *\n",
    "from model.graph.XSimGCL import *\n",
    "from model.graph.DirectAU import *\n",
    "from model.graph.SimGCL import *\n",
    "from SELFRec import SELFRec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b29f50-da83-42ef-8014-b268bad13aaf",
   "metadata": {},
   "source": [
    "## Get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4eef1dc2-f424-468e-92ac-48b5d2924ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_1</th>\n",
       "      <th>item_id_2</th>\n",
       "      <th>item_id_3</th>\n",
       "      <th>item_id_4</th>\n",
       "      <th>item_id_5</th>\n",
       "      <th>item_id_6</th>\n",
       "      <th>item_id_7</th>\n",
       "      <th>item_id_8</th>\n",
       "      <th>item_id_9</th>\n",
       "      <th>...</th>\n",
       "      <th>item_id_991</th>\n",
       "      <th>item_id_992</th>\n",
       "      <th>item_id_993</th>\n",
       "      <th>item_id_994</th>\n",
       "      <th>item_id_995</th>\n",
       "      <th>item_id_996</th>\n",
       "      <th>item_id_997</th>\n",
       "      <th>item_id_998</th>\n",
       "      <th>item_id_999</th>\n",
       "      <th>item_id_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0a9Fr9I8Gr</td>\n",
       "      <td>boqO5yZLGn</td>\n",
       "      <td>8ZkMmPx7LA</td>\n",
       "      <td>lY9LS9tm2o</td>\n",
       "      <td>GlcugvT0Yc</td>\n",
       "      <td>CgxImEqsPz</td>\n",
       "      <td>Gob6GYLouL</td>\n",
       "      <td>IRvMMJ0b7w</td>\n",
       "      <td>40N3veG6jC</td>\n",
       "      <td>ZDcoul5cdR</td>\n",
       "      <td>...</td>\n",
       "      <td>Z8xiRnRjZD</td>\n",
       "      <td>5MwdFYt8z0</td>\n",
       "      <td>IzLVUKs7uQ</td>\n",
       "      <td>Ic5BpxKXZg</td>\n",
       "      <td>F9jblvzWuK</td>\n",
       "      <td>xvKZji6pWJ</td>\n",
       "      <td>arvytMaCpS</td>\n",
       "      <td>XC7SvTAElR</td>\n",
       "      <td>oWsTKxTtvA</td>\n",
       "      <td>iLKSmk2T06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jg3UwpR1cA</td>\n",
       "      <td>FR8wGxRAuG</td>\n",
       "      <td>dYzjsJalSw</td>\n",
       "      <td>46UB3fKUvW</td>\n",
       "      <td>43pBc5JjXG</td>\n",
       "      <td>UMBBUX1BCV</td>\n",
       "      <td>QJMMePEj0j</td>\n",
       "      <td>bGCcG8kfTI</td>\n",
       "      <td>1S3CqINx38</td>\n",
       "      <td>iPCMUOS4gu</td>\n",
       "      <td>...</td>\n",
       "      <td>84YaBhvrnw</td>\n",
       "      <td>uUg3iZKEDz</td>\n",
       "      <td>VS9vm70j8u</td>\n",
       "      <td>mb8iada716</td>\n",
       "      <td>W4LaO2AnPF</td>\n",
       "      <td>oH6lYVrH2Z</td>\n",
       "      <td>RdMLs1XP8z</td>\n",
       "      <td>cjhX33aANo</td>\n",
       "      <td>JVR40qZevF</td>\n",
       "      <td>m5tbhqzC7m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k0mNOjn7pv</td>\n",
       "      <td>XY3JQdn2uK</td>\n",
       "      <td>VXSaWxoJhS</td>\n",
       "      <td>Jd9W82sCwm</td>\n",
       "      <td>APxSylqBUi</td>\n",
       "      <td>xSYbWMYXQ3</td>\n",
       "      <td>nO5G68rbFr</td>\n",
       "      <td>akZrO7ylK0</td>\n",
       "      <td>u2BPbQVjhn</td>\n",
       "      <td>0mCZtLlSSJ</td>\n",
       "      <td>...</td>\n",
       "      <td>LcT9aU6sDk</td>\n",
       "      <td>erjkmeJrO2</td>\n",
       "      <td>DtAWzeNoqo</td>\n",
       "      <td>9C1LcSgNrd</td>\n",
       "      <td>RfaOiwln47</td>\n",
       "      <td>EjaSqbgfWm</td>\n",
       "      <td>5MmscAd5ND</td>\n",
       "      <td>4CDZjAxiJ3</td>\n",
       "      <td>iPzOhrroRk</td>\n",
       "      <td>Or6J4bnaJo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uw0zXc8Sr8</td>\n",
       "      <td>CZIxh1jK1E</td>\n",
       "      <td>HDkA9U2Xcw</td>\n",
       "      <td>B5Y4WsLlpa</td>\n",
       "      <td>TtcC5Je1JD</td>\n",
       "      <td>4CGmtKJuwQ</td>\n",
       "      <td>Q3UHYGUanE</td>\n",
       "      <td>ZtFOy1wdyW</td>\n",
       "      <td>GDKM1k4EhP</td>\n",
       "      <td>CxB3Ckr0Qg</td>\n",
       "      <td>...</td>\n",
       "      <td>UmYJDzVAm1</td>\n",
       "      <td>6c9wVM3C7n</td>\n",
       "      <td>yy78DDyFwz</td>\n",
       "      <td>JBWS6uOW6t</td>\n",
       "      <td>uFtLv6hcKO</td>\n",
       "      <td>Z8p8xxx0cx</td>\n",
       "      <td>Py42KZCHJP</td>\n",
       "      <td>ewfqfR6fiC</td>\n",
       "      <td>rdZDIeJ1ia</td>\n",
       "      <td>OgmtWQ39sq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vLDKNL74C4</td>\n",
       "      <td>geQIsKSQMq</td>\n",
       "      <td>Fv1Qr0KyxC</td>\n",
       "      <td>rOrsptpv8t</td>\n",
       "      <td>x9uKie7B9k</td>\n",
       "      <td>j3BAm5H5Ht</td>\n",
       "      <td>rMoQ4U14Bz</td>\n",
       "      <td>HszEQFR8mO</td>\n",
       "      <td>Vjl5VpE6mQ</td>\n",
       "      <td>Be0DFaA4f2</td>\n",
       "      <td>...</td>\n",
       "      <td>MUWZaAPDNT</td>\n",
       "      <td>rnp6yH6OD5</td>\n",
       "      <td>45KRQI5RJi</td>\n",
       "      <td>iPRCkgoeGJ</td>\n",
       "      <td>6PtfgezcfB</td>\n",
       "      <td>w83nfFLEQF</td>\n",
       "      <td>aD2c2V2wYx</td>\n",
       "      <td>Cu2tQGKjrm</td>\n",
       "      <td>Ec3YhNEIRi</td>\n",
       "      <td>ax59cfAF2Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id   item_id_1   item_id_2   item_id_3   item_id_4   item_id_5  \\\n",
       "0  0a9Fr9I8Gr  boqO5yZLGn  8ZkMmPx7LA  lY9LS9tm2o  GlcugvT0Yc  CgxImEqsPz   \n",
       "1  Jg3UwpR1cA  FR8wGxRAuG  dYzjsJalSw  46UB3fKUvW  43pBc5JjXG  UMBBUX1BCV   \n",
       "2  k0mNOjn7pv  XY3JQdn2uK  VXSaWxoJhS  Jd9W82sCwm  APxSylqBUi  xSYbWMYXQ3   \n",
       "3  Uw0zXc8Sr8  CZIxh1jK1E  HDkA9U2Xcw  B5Y4WsLlpa  TtcC5Je1JD  4CGmtKJuwQ   \n",
       "4  vLDKNL74C4  geQIsKSQMq  Fv1Qr0KyxC  rOrsptpv8t  x9uKie7B9k  j3BAm5H5Ht   \n",
       "\n",
       "    item_id_6   item_id_7   item_id_8   item_id_9  ... item_id_991  \\\n",
       "0  Gob6GYLouL  IRvMMJ0b7w  40N3veG6jC  ZDcoul5cdR  ...  Z8xiRnRjZD   \n",
       "1  QJMMePEj0j  bGCcG8kfTI  1S3CqINx38  iPCMUOS4gu  ...  84YaBhvrnw   \n",
       "2  nO5G68rbFr  akZrO7ylK0  u2BPbQVjhn  0mCZtLlSSJ  ...  LcT9aU6sDk   \n",
       "3  Q3UHYGUanE  ZtFOy1wdyW  GDKM1k4EhP  CxB3Ckr0Qg  ...  UmYJDzVAm1   \n",
       "4  rMoQ4U14Bz  HszEQFR8mO  Vjl5VpE6mQ  Be0DFaA4f2  ...  MUWZaAPDNT   \n",
       "\n",
       "  item_id_992 item_id_993 item_id_994 item_id_995 item_id_996 item_id_997  \\\n",
       "0  5MwdFYt8z0  IzLVUKs7uQ  Ic5BpxKXZg  F9jblvzWuK  xvKZji6pWJ  arvytMaCpS   \n",
       "1  uUg3iZKEDz  VS9vm70j8u  mb8iada716  W4LaO2AnPF  oH6lYVrH2Z  RdMLs1XP8z   \n",
       "2  erjkmeJrO2  DtAWzeNoqo  9C1LcSgNrd  RfaOiwln47  EjaSqbgfWm  5MmscAd5ND   \n",
       "3  6c9wVM3C7n  yy78DDyFwz  JBWS6uOW6t  uFtLv6hcKO  Z8p8xxx0cx  Py42KZCHJP   \n",
       "4  rnp6yH6OD5  45KRQI5RJi  iPRCkgoeGJ  6PtfgezcfB  w83nfFLEQF  aD2c2V2wYx   \n",
       "\n",
       "  item_id_998 item_id_999 item_id_1000  \n",
       "0  XC7SvTAElR  oWsTKxTtvA   iLKSmk2T06  \n",
       "1  cjhX33aANo  JVR40qZevF   m5tbhqzC7m  \n",
       "2  4CDZjAxiJ3  iPzOhrroRk   Or6J4bnaJo  \n",
       "3  ewfqfR6fiC  rdZDIeJ1ia   OgmtWQ39sq  \n",
       "4  Cu2tQGKjrm  Ec3YhNEIRi   ax59cfAF2Y  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df = pd.read_csv(private_test_file, names=['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8e9e7090-75f7-4b48-b5ca-14b12dcc6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = {\n",
    "    \"LightGCN\": f\"{pwd}/runs/LightGCN/model.pkl\",\n",
    "    \"XSimGCL\": f\"{pwd}/runs/XSimGCL/model.pkl\",\n",
    "    \"DirectAU\": f\"{pwd}/runs/DirectAU/model.pkl\",\n",
    "    \"SimGCL\": f\"{pwd}/runs/SimGCL/model.pkl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fcc23993-8fe5-4d0d-84e6-24b90469bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.algorithm import find_k_largest\n",
    "def test(self):\n",
    "    def process_bar(num, total):\n",
    "        rate = float(num) / total\n",
    "        ratenum = int(50 * rate)\n",
    "        print(f'\\rProgress: [{\"+\" * ratenum}{\" \" * (50 - ratenum)}]{ratenum * 2}%', end='', flush=True)\n",
    "\n",
    "    rec_list = {}\n",
    "    data_train = pd.DataFrame(self.data.training_data, columns= ['uid', 'iid', 'rating'])\n",
    "    self.data.train_set = data_train[data_train['uid'].isin(test_user_id)].values.tolist()\n",
    "    user_count = len(self.data.train_set)\n",
    "    \n",
    "    for i, user in enumerate(self.data.train_set):\n",
    "        user = user[0]\n",
    "        candidates = self.predict(user)\n",
    "        rated_list, _ = self.data.user_rated(user)\n",
    "        for item in rated_list:\n",
    "            candidates[self.data.item[item]] = -10e8\n",
    "        # ids, scores = find_k_largest(1000, candidates)\n",
    "        item_names = predict_df[predict_df.user_id == user].values[0][1:]\n",
    "        scores = []\n",
    "        for item in item_names:\n",
    "            try:\n",
    "                id_tmp = self.data.item[item]\n",
    "                scores.append(candidates[id_tmp])\n",
    "            except:\n",
    "                # Cần sửa khuyến nghị cold start\n",
    "                scores.append(0)\n",
    "        \n",
    "        sorted_list = sorted(list(zip(item_names, scores)), key=lambda x: x[1], reverse=True)\n",
    "        rec_list[user] = sorted_list\n",
    "        if i % 1000 == 0:\n",
    "            process_bar(i, user_count)\n",
    "    process_bar(user_count, user_count)\n",
    "    print('')\n",
    "    return rec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4387bc1d-e7a3-4adb-a851-6c541678ca3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numba\n",
    "import gc\n",
    "import torch\n",
    "for model, path in model_path.items():\n",
    "    with open(path, \"rb\") as f:\n",
    "        rec = pickle.load(f)\n",
    "\n",
    "    rec_list = test(rec)\n",
    "\n",
    "    data = []\n",
    "    for user_id in test_user_id:\n",
    "        data.append([user_id] + [i[0] for i in rec_list[user_id]])\n",
    "\n",
    "    pd.DataFrame(data).to_csv(f'{out_path}/{model}_predict.csv', index = False, header=False)\n",
    "\n",
    "    del rec\n",
    "    gc.collect()  # collecting garbage\n",
    "    torch.cuda.empty_cache()  # cleaning GPU cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562fd541-8894-4b08-a12e-d99daa40064b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# RecVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf45690-3b82-4bde-b459-d0db4403fca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/VHAC-track-ds\n"
     ]
    }
   ],
   "source": [
    "%cd $pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "a75fb15a-be49-48b4-91b1-37d88815cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_checkpoint = f\"{{pwd}}/runs/RecVAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "bf7a7719-5740-4362-aee9-c6a75879e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{vae_checkpoint}/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "6bba6e80-ba1e-4008-830e-458af0c514f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(f\"{private_test_file}\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2890ace4-6848-43b7-abe1-a6c3abcffd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2item_df = pd.read_csv(f\"{vae_checkpoint}/unique_sid.txt\", header=None).rename(columns={0: \"ItemId\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "4419c37f-0aa4-44d0-8d00-c73ac5a2a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tr = df[df[\"UserId\"].isin(test[0].values)]\n",
    "test_tr = test_tr.merge(id2item_df, how=\"inner\", on=\"ItemId\").rename(columns={\"index\": \"sid\"})\n",
    "user_test = pd.DataFrame(test_tr[\"UserId\"].unique(), columns=[\"UserId\"]).reset_index().rename(columns={\"index\" : \"uid\"})\n",
    "test_tr = pd.merge(test_tr, user_test, on=\"UserId\")\n",
    "test_tr[[\"uid\", \"sid\"]].to_csv(f\"{vae_checkpoint}/testset_recvae.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "536ecb67-3abb-4636-98af-5920c0ea4aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "(4225, 82652)\n"
     ]
    }
   ],
   "source": [
    "!python recvae/infer.py --dataset $vae_checkpoint --hidden-dim 3072 --latent-dim 2048 --infer_data $vae_checkpoint/testset_recvae.csv --model_path $vae_checkpoint/model.pt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e3a4a883-ccb2-4c78-a4da-0774434c85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"{vae_checkpoint}/result_csp.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)\n",
    "\n",
    "id2profile = dict(user_test.values)\n",
    "profile2id = {value: key for key, value in id2profile.items()}\n",
    "id2item = dict(id2item_df.values)\n",
    "item2id = {value: key for key, value in id2item.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8403a09e-1e96-4fff-9ca2-1a9b88af33dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe839a4d6f664e0da41eabafab2aa2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "return_list = []\n",
    "for r in tqdm(test.merge(user_test, left_on=0, right_on=\"UserId\", how=\"left\").values):\n",
    "    userid = r[0]\n",
    "    list_item = r[1:1001]\n",
    "    uid = r[1001]\n",
    "    \n",
    "    # Process User not in Test By get the default list\n",
    "    if np.isnan(uid):\n",
    "        return_list.append([userid, *list_item])\n",
    "        continue\n",
    "\n",
    "    # Convert ItemId to indexs of sparse vector\n",
    "    item_indexs = []\n",
    "    for l in list_item:\n",
    "        try:\n",
    "            item_indexs.append((item2id[l], l))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Score and sorted to get to recommend item\n",
    "    scored_item = []\n",
    "    indexes, itemids = map(list,zip(*item_indexs))\n",
    "    for item, score in zip(itemids, result[int(uid)][indexes]):\n",
    "        scored_item.append((item, score))\n",
    "\n",
    "    scored_item = sorted(scored_item, key=lambda x : x[1], reverse=True)\n",
    "    recommend_list, _ = map(list,zip(*scored_item))\n",
    "\n",
    "    # Append to return list to make submit\n",
    "    return_list.append([userid, *recommend_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "df42bd55-e14e-4d40-91c3-a76f4b64622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df = pd.DataFrame(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "c11d3472-6dd7-4baa-9109-c89915d29c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0a9Fr9I8Gr</td>\n",
       "      <td>EALxnioXfL</td>\n",
       "      <td>PRanViElTN</td>\n",
       "      <td>6pPEL58mZO</td>\n",
       "      <td>xRrrBsbzL7</td>\n",
       "      <td>jZu5JNmd3k</td>\n",
       "      <td>sLoTA2aPEl</td>\n",
       "      <td>HvjPW7BXnD</td>\n",
       "      <td>ZXR0UOYMC5</td>\n",
       "      <td>Cpoe5fD1ft</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jg3UwpR1cA</td>\n",
       "      <td>Pyze1t4kzV</td>\n",
       "      <td>dYzjsJalSw</td>\n",
       "      <td>mECjdAi7IP</td>\n",
       "      <td>RVU66qOwYq</td>\n",
       "      <td>DyrmcAo1bZ</td>\n",
       "      <td>BJWIvAhZ5R</td>\n",
       "      <td>MtHzVtuxq5</td>\n",
       "      <td>4SqbvdwOGp</td>\n",
       "      <td>5g0f4SiT73</td>\n",
       "      <td>...</td>\n",
       "      <td>2rqp06ENF3</td>\n",
       "      <td>TYtJNgdTQT</td>\n",
       "      <td>LB4tn3Pu0R</td>\n",
       "      <td>kvxgqEihmT</td>\n",
       "      <td>jRTea0c0IA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k0mNOjn7pv</td>\n",
       "      <td>jAE0cnq4rG</td>\n",
       "      <td>y5jQg8Y9Jx</td>\n",
       "      <td>vDGrqJgbwa</td>\n",
       "      <td>wbKXQYFoKH</td>\n",
       "      <td>sBqcTvsj1B</td>\n",
       "      <td>FVFqnj3to2</td>\n",
       "      <td>mdabgpB5Xc</td>\n",
       "      <td>BEUea91tmp</td>\n",
       "      <td>nqeHWxod5x</td>\n",
       "      <td>...</td>\n",
       "      <td>lU3SwNH391</td>\n",
       "      <td>XEBugry5iK</td>\n",
       "      <td>rL2HJmNdlO</td>\n",
       "      <td>MFTqPsfgEv</td>\n",
       "      <td>YDE0Oh45lt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uw0zXc8Sr8</td>\n",
       "      <td>51ANpugrpL</td>\n",
       "      <td>Daytgu5kmV</td>\n",
       "      <td>tMe0DRMMyG</td>\n",
       "      <td>S41d9ELmhC</td>\n",
       "      <td>9H1pnwbhUi</td>\n",
       "      <td>7f7LkOKsAr</td>\n",
       "      <td>0LNX4ggWMG</td>\n",
       "      <td>4QqMKOUjwH</td>\n",
       "      <td>ZodOCtOkxr</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vLDKNL74C4</td>\n",
       "      <td>PMsUraEglo</td>\n",
       "      <td>uzts0Vtcry</td>\n",
       "      <td>gQCGJIJOaM</td>\n",
       "      <td>LBoipMRS7v</td>\n",
       "      <td>3btZEflaP0</td>\n",
       "      <td>nOcItm8Qkk</td>\n",
       "      <td>BPL6wy2Aig</td>\n",
       "      <td>WyogFojRKA</td>\n",
       "      <td>dmMV4izrqE</td>\n",
       "      <td>...</td>\n",
       "      <td>2fB6qZNgFp</td>\n",
       "      <td>nyh8MgNceK</td>\n",
       "      <td>Kw9YRCkD7o</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>SIFzQe6gnn</td>\n",
       "      <td>l6DuJC6Rg2</td>\n",
       "      <td>8f9Vtw6DqP</td>\n",
       "      <td>49OyD4VQvZ</td>\n",
       "      <td>0krwZa3Mav</td>\n",
       "      <td>XmphVhomUp</td>\n",
       "      <td>0ZnXQFRpOt</td>\n",
       "      <td>4Pm47rVPdy</td>\n",
       "      <td>RJwil82Eri</td>\n",
       "      <td>32FfxVfh3i</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>fa6Y1KVdoh</td>\n",
       "      <td>SUfmHWyfXf</td>\n",
       "      <td>Cqqm8eQ1If</td>\n",
       "      <td>lBocTTcnRM</td>\n",
       "      <td>J5KwfCDaaf</td>\n",
       "      <td>Jl8JMSuXVM</td>\n",
       "      <td>VWAFPk5W8O</td>\n",
       "      <td>a6Gsd0EzuU</td>\n",
       "      <td>owC4Cs0nZD</td>\n",
       "      <td>5M9RUEfrbb</td>\n",
       "      <td>...</td>\n",
       "      <td>leDHP1bybl</td>\n",
       "      <td>JfbZl5Hh4s</td>\n",
       "      <td>U9esFzogVx</td>\n",
       "      <td>Gct8qKZaEz</td>\n",
       "      <td>BeLbx6QVEs</td>\n",
       "      <td>JoT6E3RB1K</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>xO9WRD2Zbm</td>\n",
       "      <td>qi8qLmgATK</td>\n",
       "      <td>v9ELdECPMi</td>\n",
       "      <td>dch1WQaq9W</td>\n",
       "      <td>v4yt3Xk1RV</td>\n",
       "      <td>X2wn0fgqwH</td>\n",
       "      <td>ooHfH960JX</td>\n",
       "      <td>VQrdlRqIcc</td>\n",
       "      <td>Ibp0sHM5hA</td>\n",
       "      <td>MWRRsijMN6</td>\n",
       "      <td>...</td>\n",
       "      <td>yhSmJMOgf6</td>\n",
       "      <td>oWH4Dd5X7N</td>\n",
       "      <td>NbD4ouH6Rl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>EW403CecBe</td>\n",
       "      <td>kGYz42FXiw</td>\n",
       "      <td>gUQQbWny7I</td>\n",
       "      <td>RBLWPNcXTK</td>\n",
       "      <td>0tv68hDDTQ</td>\n",
       "      <td>3Mc5uFofhd</td>\n",
       "      <td>JopKOis5wb</td>\n",
       "      <td>jXVPOKYrBZ</td>\n",
       "      <td>OwVO8kJZ85</td>\n",
       "      <td>BTslUx1sTE</td>\n",
       "      <td>...</td>\n",
       "      <td>rlprN2eiuY</td>\n",
       "      <td>5de2lizMFg</td>\n",
       "      <td>xRUJJpZD7j</td>\n",
       "      <td>qdBPlaIUq2</td>\n",
       "      <td>dzUMySPGz9</td>\n",
       "      <td>lO9EWKFZ96</td>\n",
       "      <td>T5rJlVwG5H</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>Y4zCkSuqwM</td>\n",
       "      <td>iEfy9h6Nzm</td>\n",
       "      <td>TqlOP4qXJM</td>\n",
       "      <td>pim5qKFVYa</td>\n",
       "      <td>SrqidCPiZU</td>\n",
       "      <td>c9gvMzey8Y</td>\n",
       "      <td>yWbKxXNPjF</td>\n",
       "      <td>jOjQOaIFEZ</td>\n",
       "      <td>exFKETAM9y</td>\n",
       "      <td>BcjyFWaU7Y</td>\n",
       "      <td>...</td>\n",
       "      <td>QtB1AcWT2n</td>\n",
       "      <td>x71fbG9FmT</td>\n",
       "      <td>B5NHstVpGy</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4225 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0           1           2           3           4           5    \\\n",
       "0     0a9Fr9I8Gr  EALxnioXfL  PRanViElTN  6pPEL58mZO  xRrrBsbzL7  jZu5JNmd3k   \n",
       "1     Jg3UwpR1cA  Pyze1t4kzV  dYzjsJalSw  mECjdAi7IP  RVU66qOwYq  DyrmcAo1bZ   \n",
       "2     k0mNOjn7pv  jAE0cnq4rG  y5jQg8Y9Jx  vDGrqJgbwa  wbKXQYFoKH  sBqcTvsj1B   \n",
       "3     Uw0zXc8Sr8  51ANpugrpL  Daytgu5kmV  tMe0DRMMyG  S41d9ELmhC  9H1pnwbhUi   \n",
       "4     vLDKNL74C4  PMsUraEglo  uzts0Vtcry  gQCGJIJOaM  LBoipMRS7v  3btZEflaP0   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "4220  SIFzQe6gnn  l6DuJC6Rg2  8f9Vtw6DqP  49OyD4VQvZ  0krwZa3Mav  XmphVhomUp   \n",
       "4221  fa6Y1KVdoh  SUfmHWyfXf  Cqqm8eQ1If  lBocTTcnRM  J5KwfCDaaf  Jl8JMSuXVM   \n",
       "4222  xO9WRD2Zbm  qi8qLmgATK  v9ELdECPMi  dch1WQaq9W  v4yt3Xk1RV  X2wn0fgqwH   \n",
       "4223  EW403CecBe  kGYz42FXiw  gUQQbWny7I  RBLWPNcXTK  0tv68hDDTQ  3Mc5uFofhd   \n",
       "4224  Y4zCkSuqwM  iEfy9h6Nzm  TqlOP4qXJM  pim5qKFVYa  SrqidCPiZU  c9gvMzey8Y   \n",
       "\n",
       "             6           7           8           9    ...         990  \\\n",
       "0     sLoTA2aPEl  HvjPW7BXnD  ZXR0UOYMC5  Cpoe5fD1ft  ...        None   \n",
       "1     BJWIvAhZ5R  MtHzVtuxq5  4SqbvdwOGp  5g0f4SiT73  ...  2rqp06ENF3   \n",
       "2     FVFqnj3to2  mdabgpB5Xc  BEUea91tmp  nqeHWxod5x  ...  lU3SwNH391   \n",
       "3     7f7LkOKsAr  0LNX4ggWMG  4QqMKOUjwH  ZodOCtOkxr  ...        None   \n",
       "4     nOcItm8Qkk  BPL6wy2Aig  WyogFojRKA  dmMV4izrqE  ...  2fB6qZNgFp   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "4220  0ZnXQFRpOt  4Pm47rVPdy  RJwil82Eri  32FfxVfh3i  ...        None   \n",
       "4221  VWAFPk5W8O  a6Gsd0EzuU  owC4Cs0nZD  5M9RUEfrbb  ...  leDHP1bybl   \n",
       "4222  ooHfH960JX  VQrdlRqIcc  Ibp0sHM5hA  MWRRsijMN6  ...  yhSmJMOgf6   \n",
       "4223  JopKOis5wb  jXVPOKYrBZ  OwVO8kJZ85  BTslUx1sTE  ...  rlprN2eiuY   \n",
       "4224  yWbKxXNPjF  jOjQOaIFEZ  exFKETAM9y  BcjyFWaU7Y  ...  QtB1AcWT2n   \n",
       "\n",
       "             991         992         993         994         995         996  \\\n",
       "0           None        None        None        None        None        None   \n",
       "1     TYtJNgdTQT  LB4tn3Pu0R  kvxgqEihmT  jRTea0c0IA        None        None   \n",
       "2     XEBugry5iK  rL2HJmNdlO  MFTqPsfgEv  YDE0Oh45lt        None        None   \n",
       "3           None        None        None        None        None        None   \n",
       "4     nyh8MgNceK  Kw9YRCkD7o        None        None        None        None   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "4220        None        None        None        None        None        None   \n",
       "4221  JfbZl5Hh4s  U9esFzogVx  Gct8qKZaEz  BeLbx6QVEs  JoT6E3RB1K        None   \n",
       "4222  oWH4Dd5X7N  NbD4ouH6Rl        None        None        None        None   \n",
       "4223  5de2lizMFg  xRUJJpZD7j  qdBPlaIUq2  dzUMySPGz9  lO9EWKFZ96  T5rJlVwG5H   \n",
       "4224  x71fbG9FmT  B5NHstVpGy        None        None        None        None   \n",
       "\n",
       "       997   998   999  \n",
       "0     None  None  None  \n",
       "1     None  None  None  \n",
       "2     None  None  None  \n",
       "3     None  None  None  \n",
       "4     None  None  None  \n",
       "...    ...   ...   ...  \n",
       "4220  None  None  None  \n",
       "4221  None  None  None  \n",
       "4222  None  None  None  \n",
       "4223  None  None  None  \n",
       "4224  None  None  None  \n",
       "\n",
       "[4225 rows x 1000 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "917077f6-3da4-447c-abd7-0d9cd3fdeaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill remanining cells as nan value to make submit file eligible\n",
    "for i in range(len(return_df.columns), 1001):\n",
    "    return_df[i] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "08fa9cbb-88dd-4d9b-930f-b78712657458",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df.fillna(\"0\").to_csv(f\"{out_path}/predict_RecVAE.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267b1ba2-27d1-4bcd-a983-3affa7f95dab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7f1489-c63c-4f87-8811-77c1efe2bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "als_checkpoint = f\"{pwd}/runs/ALS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2776be61-58fe-4f1f-8ac0-f71c54ec6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "   \n",
    "with open(f\"{als_checkpoint}/model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open(f\"{als_checkpoint}/usermap.pkl\", \"rb\") as f:\n",
    "    user_map = pickle.load(f)\n",
    "\n",
    "with open(f\"{als_checkpoint}/itemmap.pkl\", \"rb\") as f:\n",
    "    item_map = pickle.load(f)\n",
    "\n",
    "with open(f\"{als_checkpoint}/csr_train.pkl\", \"rb\") as f:\n",
    "    csr_train = pickle.load(f)\n",
    "\n",
    "user_ids = {v:k for k, v in user_map.items()}\n",
    "item_ids = {v:k for k, v in item_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccecf3d4-f311-489d-84ab-af17e6bc05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['OPENBLAS_NUM_THREADS']='1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import implicit\n",
    "from scipy.sparse import coo_matrix\n",
    "from implicit.evaluation import mean_average_precision_at_k\n",
    "from implicit.evaluation import ndcg_at_k\n",
    "from implicit.gpu import matrix_factorization_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efc8fd72-54a6-4045-99be-8c245aeca275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(model, csr_train, test_user_id, item_names, submission_name=\"predict_BPR.csv\"):\n",
    "    preds = []\n",
    "    batch_size = 2000\n",
    "    # Make sure we're only predicting for users in test_user_id\n",
    "    to_generate = np.array([user_id for user_id in test_user_index])  # Make sure users exist in user_ids\n",
    "    \n",
    "    for startidx in range(0, len(to_generate), batch_size):\n",
    "        batch = to_generate[startidx:startidx + batch_size]\n",
    "        # print(batch)\n",
    "        ids, scores = model.recommend(batch, csr_train[batch], N=1000, filter_already_liked_items=True)\n",
    "        \n",
    "        for i, userid in enumerate(batch):\n",
    "            customer_id = user_ids[userid]\n",
    "            user_items = ids[i]\n",
    "            \n",
    "            # Filter the items to keep only those in item_names for the current user\n",
    "            valid_item_ids = [item for item in user_items if item_ids[item] in item_names[customer_id]]\n",
    "            \n",
    "            # If fewer than 1000 items are valid, fill the rest with invalid items or random items\n",
    "            # You can adjust the fill logic if needed, here it's just taking invalid items or the first items.\n",
    "            invalid_items = [item for item in user_items if item_ids[item] not in item_names[customer_id]]\n",
    "            filled_items = valid_item_ids + invalid_items[:(1000 - len(valid_item_ids))]\n",
    "            \n",
    "            # Ensure we have exactly 1000 items\n",
    "            article_ids = [item_ids[item_id] for item_id in filled_items[:1000]]\n",
    "            \n",
    "            preds.append([customer_id] + article_ids)\n",
    "    \n",
    "    # Create the DataFrame for submission\n",
    "    df_preds = pd.DataFrame(preds, columns=['customer_id'] + [f'item_{i}' for i in range(1000)])\n",
    "    df_preds.to_csv(submission_name, index=False, header=False)\n",
    "    \n",
    "    display(df_preds.head())\n",
    "    print(df_preds.shape)\n",
    "    \n",
    "    return df_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fc4877e-2867-4686-98df-d3364027714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(f\"{pwd}/data/test_set_private.csv\", names=['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "test_user_id = test['user_id'].values\n",
    "\n",
    "test['user_index'] = test['user_id'].map(user_map)\n",
    "test_user_index = test['user_index'].values\n",
    "\n",
    "item_names = {}\n",
    "for user_id in test_user_id:\n",
    "    item_names[user_id] = test[test.user_id == user_id].values[0][1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45bd1f-bfc2-4ef9-9413-9d0209f05da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_preds = submit(model, csr_train, test_user_id, item_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77bb8cbc-9b18-4586-bfdf-ced4241d42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.fillna(\"0\").to_csv(f\"{out_path}/predict_ALS.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1b8b4-2b4a-4ade-9abf-963a9ef213f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6a0ea76-10d3-4ca4-8b1b-8c909b9c7494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/VHAC-track-ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids-24.10/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd $pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7ab3dc-48f5-4e6e-9cdc-edcabafd272c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/test_set_private.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1001\u001b[39m)])\n\u001b[1;32m      2\u001b[0m test_user_id \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      4\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/test_set_private.csv', names=['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "test_user_id = test_df['user_id'].values\n",
    "\n",
    "test_df['user_id'] = test_df['user_id'].astype('category')\n",
    "for i in range(1,1001):\n",
    "    test_df[f'item_id_{i}'] = test_df[f'item_id_{i}'].astype('category')\n",
    "\n",
    "test_df['UserId'] = test_df['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df64403-a67d-4042-b770-65b5c0a80933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"runs/SAR/model.pkl\", \"rb\") as f\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fa609ca-71ca-4dad-9f1c-cf86cbf1b5d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AlternatingLeastSquares' object has no attribute 'recommend_k_items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_batch \u001b[38;5;129;01min\u001b[39;00m batch_generator(test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUserId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique(), batch_size):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Filter the test data for the current user batch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     user_batch_data \u001b[38;5;241m=\u001b[39m test_df[test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUserId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(user_batch)]\n\u001b[0;32m---> 12\u001b[0m     recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecommend_k_items\u001b[49m(user_batch_data, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1200\u001b[39m, remove_seen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m     all_recommendations\u001b[38;5;241m.\u001b[39mappend(recommendations)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Combine all recommendations\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AlternatingLeastSquares' object has no attribute 'recommend_k_items'"
     ]
    }
   ],
   "source": [
    "def batch_generator(data, batch_size):\n",
    "    for start in range(0, len(data), batch_size):\n",
    "        end = min(start + batch_size, len(data))\n",
    "        yield data[start:end]\n",
    "\n",
    "batch_size = 1000  # Adjust batch size as needed\n",
    "all_recommendations = []\n",
    "\n",
    "for user_batch in batch_generator(test_df['UserId'].unique(), batch_size):\n",
    "    # Filter the test data for the current user batch\n",
    "    user_batch_data = test_df[test_df['UserId'].isin(user_batch)]\n",
    "    recommendations = model.recommend_k_items(user_batch_data, top_k=1200, remove_seen=True)\n",
    "    all_recommendations.append(recommendations)\n",
    "\n",
    "# Combine all recommendations\n",
    "all_recommendations = pd.concat(all_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9aed85-4e59-42fa-8273-e41983e58190",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recommendations.to_csv('DS/data/SAR_recommendations_no_timestamp_tail19.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eeeddb-b390-4cf5-a4eb-dff0dfd1ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "preds = []  # Result storage\n",
    "\n",
    "for user_id in tqdm(test_user_id):\n",
    "    # Retrieve item names and candidate pairs for this user\n",
    "    item_names = predict_df[predict_df.user_id == user_id].values[0][1:]\n",
    "    candidates_array = all_recommendations[all_recommendations.UserId == user_id][['ItemId', 'prediction']].values\n",
    "    candidates = {item: score for item, score in candidates_array}\n",
    "    \n",
    "    scores = [\n",
    "        candidates.get(item, 1e-8) if item in candidates else 1e-8\n",
    "        for item in item_names\n",
    "    ]\n",
    "    \n",
    "    # Sort items based on score in descending order\n",
    "    sorted_list = sorted(zip(item_names, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    preds.append([user_id] + [i[0] for i in sorted_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc7919-24cb-42e8-9b94-7e7c696d69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preds).to_csv(f'{out_path}/predict_SAR.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8faa0-3fbb-4cf9-b9a9-89c08afb0864",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ensemble Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f62f83-3c21-4d25-a5c7-a726cbf932c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/conda/envs/rapids-24.10/bin/ipython Rerank.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c99dd-2071-4eb8-95d1-00b027acebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $pwd/runs/reranking/predict.csv $out_path/predict_RRK.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e901fe5-a444-4df7-b7af-fe3db49d4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv(f'{out_path}/XSimGCL_predict.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub2 = pd.read_csv(f'{out_path}/RecVAE_predict.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub3 = pd.read_csv(f'{out_path}/SimGCL_predict.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub4 = pd.read_csv(f'{out_path}/LightGCN_predict.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub5 = pd.read_csv(f'{out_path}/DirectAU_predict.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub6 = pd.read_csv(f'{out_path}/ALS_predict.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub7 = pd.read_csv(f'{out_path}/SAR_predict.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['user_id'] = sub1['user_id']\n",
    "sub1['prediction0'] = sub1.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub2['prediction1'] = sub2.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub3['prediction2'] = sub3.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub4['prediction3'] = sub4.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub5['prediction4'] = sub5.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub6['prediction5'] = sub6.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub7['prediction6'] = sub7.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "\n",
    "\n",
    "sub['prediction0'] = sub1['prediction0']\n",
    "sub['prediction1'] = sub2['prediction1']\n",
    "sub['prediction2'] = sub3['prediction2']\n",
    "sub['prediction3'] = sub4['prediction3']\n",
    "sub['prediction4'] = sub5['prediction4']\n",
    "sub['prediction5'] = sub6['prediction5']\n",
    "sub['prediction6'] = sub7['prediction6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5efce71-e3d8-431e-af8f-b124b6eb4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble dựa trên RRF\n",
    "def cust_blend(dt, W = [2, 2, 1.2, 1.5, 1.5, 1, 1]):\n",
    "    #Global ensemble weights\n",
    "    #W = [1.15,0.95,0.85]\n",
    "    \n",
    "    #Create a list of all model predictions\n",
    "    REC = []\n",
    "    REC.append(dt['prediction0'].split())\n",
    "    REC.append(dt['prediction1'].split())\n",
    "    REC.append(dt['prediction2'].split())\n",
    "    REC.append(dt['prediction3'].split())\n",
    "    REC.append(dt['prediction4'].split())\n",
    "    REC.append(dt['prediction5'].split())\n",
    "    REC.append(dt['prediction6'].split())\n",
    "    REC.append(dt['prediction7'].split())\n",
    "    REC.append(dt['prediction8'].split())   \n",
    "\n",
    "    #Create a dictionary of items recommended. \n",
    "    #Assign a weight according the order of appearance and multiply by global weights\n",
    "    res = {}\n",
    "    for M in range(len(REC)):\n",
    "        for n, v in enumerate(REC[M]):\n",
    "            if v in res:\n",
    "                res[v] += (W[M]/(n+1))\n",
    "            else:\n",
    "                res[v] = (W[M]/(n+1))\n",
    "    \n",
    "    # Sort dictionary by item weights\n",
    "    res = list(dict(sorted(res.items(), key=lambda item: -item[1])).keys())\n",
    "    \n",
    "    # Return the top 12 itens only\n",
    "    return ' '.join(res[:1000])\n",
    "\n",
    "sub['prediction'] = sub.apply(lambda x: cust_blend(x), axis=1)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df1df1-9e8d-48f1-ba83-fc25b89ec3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sub[['user_id', 'prediction']]\n",
    "value_lists = sub['prediction'].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646e260-509c-4d7c-a70b-99e32b286b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(value_lists.tolist(), index=sub['user_id']).reset_index()\n",
    "final.to_csv(f'{out_path}/predict_ensemble_7_file.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3dc40-7f45-4bf1-9bff-42a19f84a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $out_path/predict_ensemble_7_file.csv $pwd/submission/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827248f-fcda-4f98-bb23-04664d327963",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Co-Visitation-Matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01aad077-d8ce-41da-a399-8602c2a0a86b",
   "metadata": {},
   "source": [
    "!/opt/conda/envs/rapids-24.10/bin/ipython Co-Visitation-Matrix.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d7782-8721-4740-8aef-a6e192f5fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $pwd/runs/co-visitation-matrix/predict.csv $out_path/predict_CVM.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90a230-fc11-46ac-90ab-7dd9b3e6c840",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ReRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177dd13b-c1e1-424a-a351-8a72c8d4b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/conda/envs/rapids-24.10/bin/ipython Rerank.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e3059-fd37-46bc-af2d-41feb7814d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $pwd/runs/reranking/predict.csv $out_path/predict_RRK.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696901dc-a7f3-44ad-8d8f-884394a51524",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get Low Agreement user and User Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4e024-1bfb-4b89-8da1-eb1e22d78547",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Low Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c061766-81b3-47c4-bc42-1efa4b0799d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "recvae = pd.read_csv(f\"{out_path}/RecVAE_predict.csv\", header=None)\n",
    "als = pd.read_csv(f\"{out_path}/ALS_predict.csv\", header=None)\n",
    "lightgcn = pd.read_csv(f\"{out_path}/LightGCN_predict.csv\", header=None)\n",
    "sar = pd.read_csv(f\"{out_path}/SAR_predict.csv\", header=None)\n",
    "xsimgcl = pd.read_csv(f\"{out_path}/XSimGCL_predict.csv\", header=None)\n",
    "directau = pd.read_csv(f\"{out_path}/DirectAU_predict.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa33b28-aa0f-4582-8405-688408937c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{outpat}/predict_ensemble_7file.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace5b99-9407-4c78-bb04-66e0dfa2845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_and_process_recommendations(file_paths):\n",
    "    \"\"\"\n",
    "    Đọc và xử lý các file khuyến nghị từ nhiều mô hình\n",
    "    \"\"\"\n",
    "    model_predictions = {}\n",
    "    for model_name, file_path in file_paths.items():\n",
    "        df = pd.read_csv(file_path, header=None)[[0,1,2,3]]\n",
    "        # Chuyển đổi DataFrame thành dictionary với key là user_id và value là list các khuyến nghị\n",
    "        predictions = {str(row[0]): list(row[1:]) for _, row in df.iterrows()}\n",
    "        model_predictions[model_name] = predictions\n",
    "    return model_predictions\n",
    "\n",
    "def calculate_jaccard_similarity(set1, set2):\n",
    "    \"\"\"\n",
    "    Tính toán độ tương đồng Jaccard giữa hai tập hợp\n",
    "    \"\"\"\n",
    "    intersection = len(set(set1) & set(set2))\n",
    "    union = len(set(set1) | set(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def analyze_model_agreement(model_predictions):\n",
    "    \"\"\"\n",
    "    Phân tích độ đồng thuận giữa các mô hình\n",
    "    \"\"\"\n",
    "    user_similarities = defaultdict(list)\n",
    "    model_names = list(model_predictions.keys())\n",
    "    \n",
    "    # Với mỗi người dùng, tính toán độ tương đồng giữa các cặp mô hình\n",
    "    for user_id in model_predictions[model_names[0]].keys():\n",
    "        similarities = []\n",
    "        # So sánh từng cặp mô hình\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(i+1, len(model_names)):\n",
    "                model1, model2 = model_names[i], model_names[j]\n",
    "                recs1 = model_predictions[model1][user_id]\n",
    "                recs2 = model_predictions[model2][user_id]\n",
    "                similarity = calculate_jaccard_similarity(recs1, recs2)\n",
    "                similarities.append(similarity)\n",
    "        \n",
    "        # Tính trung bình độ tương đồng cho người dùng này\n",
    "        avg_similarity = np.mean(similarities)\n",
    "        user_similarities[user_id] = avg_similarity\n",
    "    \n",
    "    return user_similarities\n",
    "\n",
    "def get_extreme_cases(user_similarities, threshold_high=0.5, threshold_low=0.15 ):\n",
    "    \"\"\"\n",
    "    Lấy ra các trường hợp có độ đồng thuận cao và thấp\n",
    "    \"\"\"\n",
    "    high_agreement = {k: v for k, v in user_similarities.items() if v >= threshold_high}\n",
    "    low_agreement = {k: v for k, v in user_similarities.items() if v <= threshold_low}\n",
    "    \n",
    "    return high_agreement, low_agreement\n",
    "\n",
    "def analyze_recommendations():\n",
    "    # Định nghĩa đường dẫn đến các file\n",
    "    file_paths = {\n",
    "        'RecVAE': f\"{out_path}/RecVAE_predict.csv\",\n",
    "        'ALS': f\"{out_path}/ALS_predict.csv\",\n",
    "        'LightGCN': f\"{out_path}/LightGCN_predict.csv\",\n",
    "        'SAR': f\"{out_path}/SAR_predict.csv\",\n",
    "        'XSimGCL': f\"{out_path}/XSimGCL_predict.csv\"\n",
    "    }\n",
    "    \n",
    "    # Đọc và xử lý dữ liệu\n",
    "    model_predictions = load_and_process_recommendations(file_paths)\n",
    "    \n",
    "    # Phân tích độ đồng thuận\n",
    "    user_similarities = analyze_model_agreement(model_predictions)\n",
    "    \n",
    "    # Lấy ra các trường hợp đặc biệt\n",
    "    high_agreement, low_agreement = get_extreme_cases(user_similarities)\n",
    "    \n",
    "    # In kết quả phân tích\n",
    "    print(f\"Tổng số người dùng: {len(user_similarities)}\")\n",
    "    print(f\"Số người dùng có độ đồng thuận cao: {len(high_agreement)}\")\n",
    "    print(f\"Số người dùng có độ đồng thuận thấp: {len(low_agreement)}\")\n",
    "    \n",
    "    # In ra một vài ví dụ\n",
    "    print(\"\\nVí dụ về người dùng có độ đồng thuận cao:\")\n",
    "    for user_id, similarity in list(high_agreement.items())[:5]:\n",
    "        print(f\"User {user_id}: {similarity:.3f}\")\n",
    "        \n",
    "    print(\"\\nVí dụ về người dùng có độ đồng thuận thấp:\")\n",
    "    for user_id, similarity in list(low_agreement.items())[:5]:\n",
    "        print(f\"User {user_id}: {similarity:.3f}\")\n",
    "        \n",
    "    return high_agreement, low_agreement, user_similarities\n",
    "\n",
    "# Chạy phân tích\n",
    "high_agreement, low_agreement, user_similarities = analyze_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb268e-18c7-4332-a660-1508d70353e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_agreement_user = low_agreement.keys()\n",
    "\n",
    "with open(f\"{out_path}/low_agreement_user.txt\", \"w\") as f:\n",
    "    for key in low_agreement_user:\n",
    "        f.write(key + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5c6fc-1525-48b3-9b1a-ca57e92b3370",
   "metadata": {},
   "source": [
    "## New Item with Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8852430-423c-4590-b86d-bc4448b15d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f9839-8682-4b53-8eb7-7d1104814461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE  # You can also use UMAP from cuML or sklearn\n",
    "import plotly.express as px\n",
    "\n",
    "# Load the user embeddings\n",
    "embedding_file_path = 'runs/lightgcn/user_embedding.pkl'\n",
    "\n",
    "with open(embedding_file_path, 'rb') as f:\n",
    "    user_embeddings = pickle.load(f)\n",
    "\n",
    "# Convert the embeddings into a NumPy array (make sure it's in the right shape)\n",
    "embedding_matrix = np.array(list(user_embeddings.values()))\n",
    "\n",
    "# Dimensionality Reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, init='pca', metric='euclidean', method=\"barnes_hut\")\n",
    "X_tsne = tsne.fit_transform(embedding_matrix)\n",
    "\n",
    "# Optional: Get the t-SNE divergence (useful for diagnostic purposes)\n",
    "print(f\"t-SNE KL Divergence: {tsne.kl_divergence_}\")\n",
    "\n",
    "# Visualize the results using Plotly\n",
    "# Create a DataFrame for easy plotting\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X_tsne, columns=[\"TSNE Component 1\", \"TSNE Component 2\"])\n",
    "\n",
    "# If you have any labels (e.g., cluster IDs or user types), you can add them to the DataFrame\n",
    "# Example: df['Cluster'] = cluster_labels  # If you have cluster labels\n",
    "\n",
    "# Create the Plotly scatter plot\n",
    "fig = px.scatter(df, x=\"TSNE Component 1\", y=\"TSNE Component 2\",\n",
    "                 title=\"User Latent Space (t-SNE)\",\n",
    "                 labels={\"TSNE Component 1\": \"Dimension 1\", \"TSNE Component 2\": \"Dimension 2\"},\n",
    "                 template=\"plotly_dark\")  # Optional: use dark theme\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    title=\"User Latent Space\",\n",
    "    xaxis_title=\"t-SNE Component 1\",\n",
    "    yaxis_title=\"t-SNE Component 2\",\n",
    "    showlegend=False  # Set to True if you want to display legends (e.g., for clusters)\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581aac3d-6bc9-4d07-8059-f2dc9f406fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Compute the k-nearest neighbors distances\n",
    "min_samples = 5  # Typically, this is a small integer, like 5 or 10\n",
    "neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "neighbors_fit = neighbors.fit(X_tsne)  # Use your 2D reduced data (e.g., t-SNE embeddings)\n",
    "distances, indices = neighbors_fit.kneighbors(X_tsne)\n",
    "\n",
    "# Sort the distances\n",
    "distances = np.sort(distances[:, min_samples-1], axis=0)\n",
    "\n",
    "# Plot the k-distance graph (elbow method)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances)\n",
    "plt.title(f'k-distance Graph (k={min_samples})')\n",
    "plt.xlabel('Data points sorted by distance to k-th neighbor')\n",
    "plt.ylabel(f'{min_samples}-th Nearest Neighbor Distance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8ab8e-d03d-4122-9747-6c8cc2a5cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.9, min_samples=5, metric='euclidean')  # You may need to adjust these hyperparameters\n",
    "cluster_labels = dbscan.fit_predict(X_tsne)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "df = pd.DataFrame(X_tsne, columns=[\"TSNE Component 1\", \"TSNE Component 2\"])\n",
    "df['Cluster'] = cluster_labels\n",
    "\n",
    "# Visualize the result using Plotly\n",
    "fig = px.scatter(df, x=\"TSNE Component 1\", y=\"TSNE Component 2\", color=\"Cluster\",\n",
    "                 title=\"User Latent Space with DBSCAN Clusters\",\n",
    "                 labels={\"TSNE Component 1\": \"Dimension 1\", \"TSNE Component 2\": \"Dimension 2\"},\n",
    "                 template=\"plotly_dark\")  # Optional: use \"plotly_dark\" or other templates\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    title=\"User Latent Space (DBSCAN Clusters)\",\n",
    "    xaxis_title=\"t-SNE Component 1\",\n",
    "    yaxis_title=\"t-SNE Component 2\",\n",
    "    showlegend=True  # Show the legend to display the cluster colors\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567722d6-3c66-40b0-8b45-15b9d8f2af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = list(user_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c857138-4480-4023-a7c8-a320d0308a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('runs/lightgcn/user_clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc9ce7-a787-4f2c-9953-9c530d638c16",
   "metadata": {},
   "source": [
    "## Append New Item by Cluster and Low Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b60485-f3ac-44d6-a28d-6cb84e3547a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = pd.read_csv('runs/lightgcn/user_clusters.csv')\n",
    "cluster.head()\n",
    "\n",
    "user_df = test.copy()\n",
    "user_df = user_df.set_index('user_id')\n",
    "cluster = cluster.set_index('User ID')\n",
    "user_df = user_df.join(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f6498-13f2-4ad5-bd83-7323cf7f3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_most_common_items_for_cluster(user_df, cluster_column='Cluster', item_columns=None):\n",
    "    cluster_item_counts = {}\n",
    "\n",
    "    # Iterate over each cluster\n",
    "    for cluster in user_df[cluster_column].unique():\n",
    "        # Get users in this cluster\n",
    "        cluster_users = user_df[user_df[cluster_column] == cluster]\n",
    "        \n",
    "        # Get all item interactions for these users\n",
    "        cluster_items = cluster_users[item_columns].values.flatten()\n",
    "        \n",
    "        # Count the frequency of each item (ignore NaN or empty interactions)\n",
    "        cluster_items = [item for item in cluster_items if pd.notna(item)]\n",
    "        item_counter = Counter(cluster_items)\n",
    "        \n",
    "        # Save the most common items in this cluster\n",
    "        cluster_item_counts[cluster] = item_counter.most_common()\n",
    "    \n",
    "    return cluster_item_counts\n",
    "\n",
    "item_columns = [f'item_id_{i+1}' for i in range(1000)] \n",
    "cluster_item_counts = get_most_common_items_for_cluster(user_df, item_columns=item_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c08afd0-2751-402f-9903-08996b0d9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/training_set.csv\")\n",
    "list_all_train_item = train[\"ItemId\"].unique()\n",
    "items_df = test[item_columns]\n",
    "all_test_items = items_df.values.flatten()\n",
    "all_test_items = set(all_test_items)\n",
    "old_items = set(all_test_items).intersection(set(list_all_train_item))\n",
    "new_items = all_test_items - old_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56edc9d-abd0-49c5-a281-c867ac2d1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_items_dict = {}\n",
    "\n",
    "for user_idx, user_id in enumerate(test[\"user_id\"]):  # Assuming 'user_id' is the first column\n",
    "    # Get the list of items the user has interacted with\n",
    "    user_items = items_df.iloc[user_idx].values.tolist()\n",
    "    \n",
    "    # Filter out items that are NaN or empty\n",
    "    user_items = [item for item in user_items if pd.notna(item) and item != '']\n",
    "    \n",
    "    # Find new items by checking which of the user's items are in `new_items`\n",
    "    user_new_items = [item for item in user_items if item in new_items]\n",
    "    \n",
    "    # Add to the dictionary: user_id -> list of new items\n",
    "    new_items_dict[user_id] = user_new_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c3e87-04cb-44ad-ab14-e7d9131e6508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_items_for_user(user_id, cluster_item_counts, new_items_dict, user_df, item_columns, top_n=1000):\n",
    "    # Get the user's cluster\n",
    "    user_cluster = user_df.loc[user_id, 'Cluster']\n",
    "    # Get the most common items in this cluster\n",
    "    common_items = [(item, count) for item, count in cluster_item_counts[user_cluster]]\n",
    "    common_items = pd.DataFrame(common_items, columns=['item_id', 'count'])\n",
    "   \n",
    "    # Get the new items for this user\n",
    "    new_items = new_items_dict.get(user_id, [])\n",
    "    recommended_items = common_items[common_items['item_id'].isin(new_items)]\n",
    "    # print(recommended_items)\n",
    "    recommended_items = recommended_items.sort_values(by='count', ascending=False)['item_id'].tolist()\n",
    "    # Ensure the recommended list has 1000 items (pad with \"empty\" items if necessary)\n",
    "    recommended_items += [\"0\"] * (top_n - len(recommended_items))  # Pad with empty strings\n",
    "    # print(recommended_items)\n",
    "    return recommended_items\n",
    "\n",
    "# Create a list to hold all recommendations\n",
    "recommendations = []\n",
    "\n",
    "# Generate recommendations for each user\n",
    "for user_id in user_df.index:\n",
    "    recommended_items = recommend_items_for_user(user_id, cluster_item_counts, new_items_dict, user_df, item_columns)\n",
    "    recommendations.append([user_id] + recommended_items)\n",
    "# Convert the recommendations into a DataFrame\n",
    "recommendations_df = pd.DataFrame(recommendations, columns=['user_id'] + [f'item_{i+1}' for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63911b93-0008-4f20-8d9c-8e276819c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "last_1000_columns = recommendations_df.iloc[:, -1000:]\n",
    "\n",
    "# Count rows where there is at least one non-zero value in the last 1000 columns\n",
    "non_zero_rows = (last_1000_columns != \"0\").any(axis=1)\n",
    "\n",
    "# Count the number of such rows\n",
    "count_non_zero_rows = non_zero_rows.sum()\n",
    "\n",
    "print(\"Number of rows with non-zero values in the last 1000 columns:\", count_non_zero_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dccded-09ce-4111-a040-d5417c08c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df.to_csv('submission/predict_new.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fcba0e-4754-43af-88de-98837ab3b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = pd.read_csv(f'{out_path}/LightGCN_predict.csv', names=['user_id'] + [f'item_id_{i}' for i in range(1,1001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c861eb-2094-4b8b-a120-ec06d8f33a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read keys from a file into a list\n",
    "with open(f\"{out_path}/low_agreement_user.txt\", \"r\") as f:\n",
    "    keys = [line.strip() for line in f.readlines()]\n",
    "len(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2abe71-7e42-4c52-9631-d5d73cf7f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best.loc[recommendations_df['item_1'] != \"0\", 'item_id_10'] = recommendations_df['item_1']\n",
    "count = 0\n",
    "for idx, row in best.iterrows():\n",
    "    if row['user_id'] in keys:\n",
    "        # Get the corresponding recommendation for this user\n",
    "        recommendation = recommendations_df.loc[recommendations_df['user_id'] == row['user_id'], 'item_1'].values\n",
    "        \n",
    "        if recommendation and recommendation[0] != '0':  # Check if the recommendation is not '0'\n",
    "            count += 1\n",
    "        #     for i in range(9, 4, -1):  # Start from item_id_9 and shift down to item_id_2\n",
    "        #         best.loc[idx, f'item_id_{i+1}'] = best.loc[idx, f'item_id_{i}']\n",
    "            # Replace item_id_1 with item_1 from recommendations_df\n",
    "            best.loc[idx, 'item_id_10'] = recommendation[0]\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a653d0f-3c05-40b4-9fc9-ab5b7d176611",
   "metadata": {},
   "outputs": [],
   "source": [
    "best.to_csv(f'out_path}/LightGCN_new_cluster_top10.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f48709-defd-4ddc-9222-6559f08966c7",
   "metadata": {},
   "source": [
    "\n",
    "# Ensemble Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3de2efe9-2800-4a80-92df-ffaca1c04367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b0a9914-1052-4789-ad96-6b668a6226e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv(f'{out_path}/predict_XSim_ensemble_noscore.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub2 = pd.read_csv(f'{out_path}/RecVAE_05981.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub3 = pd.read_csv(f'{out_path}/SimGCL_predict.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub4 = pd.read_csv(f'{out_path}/LightGCN_new_cluster_top10.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub5 = pd.read_csv(f'{out_path}/DirectAU_predict.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub6 = pd.read_csv(f'{out_path}/ALS_new_cluster_top10.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub7 = pd.read_csv(f'{out_path}/SAR_std_data.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub8 = pd.read_csv(f'{out_path}/predict_full_rerank.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "sub9 = pd.read_csv(f'{out_path}/predict_ensemble_8file.csv', names = ['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['user_id'] = sub1['user_id']\n",
    "sub1['prediction0'] = sub1.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub2['prediction1'] = sub2.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub3['prediction2'] = sub3.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub4['prediction3'] = sub4.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub5['prediction4'] = sub5.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub6['prediction5'] = sub6.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub7['prediction6'] = sub7.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub8['prediction7'] = sub8.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "sub9['prediction8'] = sub9.apply(lambda row: ' '.join(str(row[f'item_id_{i+1}']) for i in range(1000)), axis=1)\n",
    "\n",
    "\n",
    "sub['prediction0'] = sub1['prediction0']\n",
    "sub['prediction1'] = sub2['prediction1']\n",
    "sub['prediction2'] = sub3['prediction2']\n",
    "sub['prediction3'] = sub4['prediction3']\n",
    "sub['prediction4'] = sub5['prediction4']\n",
    "sub['prediction5'] = sub6['prediction5']\n",
    "sub['prediction6'] = sub7['prediction6']\n",
    "sub['prediction7'] = sub8['prediction7']\n",
    "sub['prediction8'] = sub9['prediction8']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be4d0bed-2625-4150-aa66-b2e80770378e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prediction9'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/rapids-24.10/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prediction9'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Return the top 12 itens only\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(res[:\u001b[38;5;241m1000\u001b[39m])\n\u001b[0;32m---> 36\u001b[0m sub[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcust_blend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m sub\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/conda/envs/rapids-24.10/lib/python3.11/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/rapids-24.10/lib/python3.11/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/rapids-24.10/lib/python3.11/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m/opt/conda/envs/rapids-24.10/lib/python3.11/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[22], line 36\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Return the top 12 itens only\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(res[:\u001b[38;5;241m1000\u001b[39m])\n\u001b[0;32m---> 36\u001b[0m sub[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sub\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mcust_blend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m sub\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m, in \u001b[0;36mcust_blend\u001b[0;34m(dt, W)\u001b[0m\n\u001b[1;32m     15\u001b[0m REC\u001b[38;5;241m.\u001b[39mappend(dt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction7\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     16\u001b[0m REC\u001b[38;5;241m.\u001b[39mappend(dt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction8\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit())   \n\u001b[0;32m---> 17\u001b[0m REC\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprediction9\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit())   \n\u001b[1;32m     18\u001b[0m REC\u001b[38;5;241m.\u001b[39mappend(dt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction10\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit())   \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#Create a dictionary of items recommended. \u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#Assign a weight according the order of appearance and multiply by global weights\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/rapids-24.10/lib/python3.11/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/opt/conda/envs/rapids-24.10/lib/python3.11/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/conda/envs/rapids-24.10/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prediction9'"
     ]
    }
   ],
   "source": [
    "# Ensemble dựa trên RRF\n",
    "def cust_blend(dt, W = [2, 2, 1.2, 1.5, 1.5, 1, 1, 1, 2]):\n",
    "    #Global ensemble weights\n",
    "    #W = [1.15,0.95,0.85]\n",
    "    \n",
    "    #Create a list of all model predictions\n",
    "    REC = []\n",
    "    REC.append(dt['prediction0'].split())\n",
    "    REC.append(dt['prediction1'].split())\n",
    "    REC.append(dt['prediction2'].split())\n",
    "    REC.append(dt['prediction3'].split())\n",
    "    REC.append(dt['prediction4'].split())\n",
    "    REC.append(dt['prediction5'].split())\n",
    "    REC.append(dt['prediction6'].split())\n",
    "    REC.append(dt['prediction7'].split())\n",
    "    REC.append(dt['prediction8'].split())   \n",
    "    REC.append(dt['prediction9'].split())   \n",
    "    REC.append(dt['prediction10'].split())   \n",
    "\n",
    "    #Create a dictionary of items recommended. \n",
    "    #Assign a weight according the order of appearance and multiply by global weights\n",
    "    res = {}\n",
    "    for M in range(len(REC)):\n",
    "        for n, v in enumerate(REC[M]):\n",
    "            if v in res:\n",
    "                res[v] += (W[M]/(n+1))\n",
    "            else:\n",
    "                res[v] = (W[M]/(n+1))\n",
    "    \n",
    "    # Sort dictionary by item weights\n",
    "    res = list(dict(sorted(res.items(), key=lambda item: -item[1])).keys())\n",
    "    \n",
    "    # Return the top 12 itens only\n",
    "    return ' '.join(res[:1000])\n",
    "\n",
    "sub['prediction'] = sub.apply(lambda x: cust_blend(x), axis=1)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f4735-3325-4b72-998b-fa73141fbf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sub[['user_id', 'prediction']]\n",
    "value_lists = sub['prediction'].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3dcf3d-e352-4461-a675-57e921abebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(value_lists.tolist(), index=sub['user_id']).reset_index()\n",
    "final.to_csv('submission/predict.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf12f48-2ccc-4c36-86cb-fb3727c3d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd submission && zip CHAMPION_FINAL_SUBMISSION.zip predict.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710c3db-8b68-44dc-8178-851d870c1bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapidsai",
   "language": "python",
   "name": "rapidsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
