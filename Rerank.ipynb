{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00c63c8",
   "metadata": {
    "papermill": {
     "duration": 0.012033,
     "end_time": "2024-11-19T17:54:05.462925",
     "exception": false,
     "start_time": "2024-11-19T17:54:05.450892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* User feature như sau:\n",
    "+ User embed\n",
    "+ User mua bao nhiêu sản phẩm\n",
    "+ User tỷ lệ mua bao nhiêu\n",
    "* Item feature như sau:\n",
    "+ Item embed\n",
    "+ Item được tương tác bao lần\n",
    "+ Item hay được mua cùng như nào (Co-visitation matrix)\n",
    "+ Item có tỷ lệ mua là bao nhiêu\n",
    "* User-Item feature\n",
    "+ score embedding từ 4 model\n",
    "\n",
    "Tạo candidate bằng cách lấy dữ liệu thật có trường rating, negative sample bằng XSimGCL \n",
    "Tập test là tập user id trong test\n",
    "Infer bằng cách lấy top 20 sau khi rerank bằng XSimGCL rerank lại hoặc lấy luôn 1000 sản phẩm rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e41131",
   "metadata": {
    "papermill": {
     "duration": 3.230585,
     "end_time": "2024-11-19T17:54:08.704693",
     "exception": false,
     "start_time": "2024-11-19T17:54:05.474108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import gc\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748dff19-7f47-4372-a998-163d2aad7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = \"runs/reranking\"\n",
    "import os \n",
    "os.makedirs(outpath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6094e",
   "metadata": {
    "papermill": {
     "duration": 2.817414,
     "end_time": "2024-11-19T17:54:11.533465",
     "exception": false,
     "start_time": "2024-11-19T17:54:08.716051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_df = pd.read_csv('data/test_set_private.csv', names=['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "test_user_id = predict_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c5fb1",
   "metadata": {
    "papermill": {
     "duration": 0.019473,
     "end_time": "2024-11-19T17:54:11.566090",
     "exception": false,
     "start_time": "2024-11-19T17:54:11.546617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install gitpython --quiet\n",
    "# import git\n",
    "# git.Repo.clone_from('https://github.com/Coder-Yu/SELFRec.git', '/kaggle/working/sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48de4b8",
   "metadata": {
    "papermill": {
     "duration": 0.020822,
     "end_time": "2024-11-19T17:54:11.598898",
     "exception": false,
     "start_time": "2024-11-19T17:54:11.578076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cd /kaggle/working/sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f84090",
   "metadata": {
    "papermill": {
     "duration": 18.523176,
     "end_time": "2024-11-19T17:54:30.133071",
     "exception": false,
     "start_time": "2024-11-19T17:54:11.609895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# user_embedding_recvae = pd.read_pickle('weight-model/user_embedding_recvae.pkl')\n",
    "# item_embedding_recvae = pd.read_pickle('weight-model/item_embedding_recvae.pkl')\n",
    "\n",
    "user_embedding_xsim = pd.read_pickle('weight-model/emb_user_XSim_full.pickle')\n",
    "item_embedding_xsim = pd.read_pickle('weight-model/emb_item_XSim_full.pickle')\n",
    "\n",
    "user_embedding_xsim_standard = pd.read_pickle('weight-model/emb_user_XSim_standard.pickle')\n",
    "item_embedding_xsim_standard = pd.read_pickle('weight-model/emb_item_XSim_standard.pickle')\n",
    "\n",
    "user_embedding_xsim_denoise = pd.read_pickle('weight-model/emb_user_XSim_denoise.pickle')\n",
    "item_embedding_xsim_denoise = pd.read_pickle('weight-model/emb_item_XSim_denoise.pickle')\n",
    "\n",
    "user_embedding_simgcl = pd.read_pickle('weight-model/user_embedding_simgcl.pkl')\n",
    "item_embedding_simgcl = pd.read_pickle('weight-model/item_embedding_simgcl.pkl')\n",
    "\n",
    "user_embedding_lightgcn = pd.read_pickle('weight-model/user_embedding_lightgcn.pkl')\n",
    "item_embedding_lightgcn = pd.read_pickle('weight-model/item_embedding_lightgcn.pkl')\n",
    "\n",
    "user_embedding_xsim_128 = pd.read_pickle('weight-model/emb_user_XSim_128.pickle')\n",
    "item_embedding_xsim_128 = pd.read_pickle('weight-model/emb_item_XSim128.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff9c8d",
   "metadata": {
    "papermill": {
     "duration": 1.855377,
     "end_time": "2024-11-19T17:54:31.999475",
     "exception": false,
     "start_time": "2024-11-19T17:54:30.144098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "buy_order_full = pd.read_csv('runs/co-visitation-matrix/buy_order_full.csv')\n",
    "buy_order_standard = pd.read_csv('runs/co-visitation-matrix/buy_order_standard.csv')\n",
    "cart_order_full = pd.read_csv('runs/co-visitation-matrix/cart_order_full.csv')\n",
    "cart_order_standard = pd.read_csv('runs/co-visitation-matrix/cart_order_standard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92b27c",
   "metadata": {
    "papermill": {
     "duration": 0.346799,
     "end_time": "2024-11-19T17:54:32.357686",
     "exception": false,
     "start_time": "2024-11-19T17:54:32.010887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "buy_order_full = buy_order_full.groupby('ItemId_x')['wgt'].sum().reset_index().rename(columns={'ItemId_x': 'ItemId', 'wgt': 'buy_order_full'})\n",
    "buy_order_standard = buy_order_standard.groupby('ItemId_x')['wgt'].sum().reset_index().rename(columns={'ItemId_x': 'ItemId', 'wgt': 'buy_order_standard'})\n",
    "cart_order_full = cart_order_full.groupby('ItemId_x')['wgt'].sum().reset_index().rename(columns={'ItemId_x': 'ItemId','wgt': 'cart_order_full'})\n",
    "cart_order_standard = cart_order_standard.groupby('ItemId_x')['wgt'].sum().reset_index().rename(columns={'ItemId_x': 'ItemId','wgt': 'cart_order_standard'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84b271",
   "metadata": {
    "papermill": {
     "duration": 0.026426,
     "end_time": "2024-11-19T17:54:33.593080",
     "exception": false,
     "start_time": "2024-11-19T17:54:33.566654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_embeddings_to_df(id_name, embeddings_dict):\n",
    "    \"\"\"\n",
    "    Chuyển đổi dictionary chứa id và vector embedding thành pandas dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    embeddings_dict (dict): Dictionary với key là id và value là vector embedding 128 chiều\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame với 129 cột (1 cột id và 128 cột cho vector embedding)\n",
    "    \"\"\"\n",
    "    # Tạo list các records, mỗi record là list chứa id và 128 giá trị embedding\n",
    "    records = []\n",
    "    for id_, embedding in embeddings_dict.items():\n",
    "        record = [id_] + embedding.tolist() if isinstance(embedding, np.ndarray) else [id_] + embedding\n",
    "        records.append(record)\n",
    "    \n",
    "    # Tạo tên cho các cột\n",
    "    columns = [id_name] + [f'dim_{id_name}_{i}' for i in range(128)]\n",
    "    \n",
    "    # Tạo DataFrame\n",
    "    df = pd.DataFrame(records, columns=columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4eb28",
   "metadata": {
    "papermill": {
     "duration": 5.12055,
     "end_time": "2024-11-19T17:54:38.725679",
     "exception": false,
     "start_time": "2024-11-19T17:54:33.605129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_embed_feature = convert_embeddings_to_df('ItemId',item_embedding_xsim_128)\n",
    "user_embed_feature = convert_embeddings_to_df('UserId',user_embedding_xsim_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ffbbe",
   "metadata": {
    "papermill": {
     "duration": 20.727333,
     "end_time": "2024-11-19T17:54:59.464059",
     "exception": false,
     "start_time": "2024-11-19T17:54:38.736726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_emb = []\n",
    "user2id = {}\n",
    "cnt = 0\n",
    "user_list_xsim = []\n",
    "for user in user_embedding_xsim:\n",
    "    user_emb.append(user_embedding_xsim[user])\n",
    "    user_list_xsim.append(user)\n",
    "    user2id[user] = cnt\n",
    "    cnt+=1\n",
    "user_emb = torch.tensor(user_emb)\n",
    "\n",
    "item_emb = []\n",
    "item2id = {}\n",
    "cnt = 0\n",
    "item_list_xsim = []\n",
    "for item in item_embedding_xsim:\n",
    "    item_emb.append(item_embedding_xsim[item])\n",
    "    item_list_xsim.append(item)\n",
    "    item2id[item] = cnt\n",
    "    cnt+=1\n",
    "item_emb = torch.tensor(item_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af388dc",
   "metadata": {
    "papermill": {
     "duration": 0.022677,
     "end_time": "2024-11-19T17:54:59.497923",
     "exception": false,
     "start_time": "2024-11-19T17:54:59.475246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(user2id), len(item2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2790db",
   "metadata": {
    "papermill": {
     "duration": 0.434505,
     "end_time": "2024-11-19T17:54:59.944752",
     "exception": false,
     "start_time": "2024-11-19T17:54:59.510247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/training_set.csv')\n",
    "data.shape, data.UserId.nunique(), data.ItemId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac675f",
   "metadata": {
    "papermill": {
     "duration": 1.282696,
     "end_time": "2024-11-19T17:55:01.238817",
     "exception": false,
     "start_time": "2024-11-19T17:54:59.956121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_rated = data.groupby('UserId')['ItemId'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a8380",
   "metadata": {
    "papermill": {
     "duration": 0.028877,
     "end_time": "2024-11-19T17:55:01.279860",
     "exception": false,
     "start_time": "2024-11-19T17:55:01.250983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_k_largest(K, candidates):\n",
    "    n_candidates = []\n",
    "    for iid, score in enumerate(candidates[:K]):\n",
    "        n_candidates.append((score, iid))\n",
    "    heapq.heapify(n_candidates)\n",
    "    for iid, score in enumerate(candidates[K:]):\n",
    "        if score > n_candidates[0][0]:\n",
    "            heapq.heapreplace(n_candidates, (score, iid + K))\n",
    "    n_candidates.sort(key=lambda d: d[0], reverse=True)\n",
    "    ids = [item[1] for item in n_candidates]\n",
    "    k_largest_scores = [item[0] for item in n_candidates]\n",
    "    return ids, k_largest_scores\n",
    "    \n",
    "def generate_candidate(user_id_list):\n",
    "    def process_bar(num, total):\n",
    "        rate = float(num) / total\n",
    "        ratenum = int(50 * rate)\n",
    "        print(f'\\rProgress: [{\"+\" * ratenum}{\" \" * (50 - ratenum)}]{ratenum * 2}%', end='', flush=True)\n",
    "\n",
    "    rec_list = {}\n",
    "    user_count = len(user_id_list)\n",
    "    for i, user in enumerate(user_id_list):\n",
    "        candidates = torch.matmul(torch.tensor(user_embedding_xsim[user]), torch.tensor(item_emb.transpose(0, 1))).cpu().numpy()\n",
    "        rated_list = user_rated[user]\n",
    "        for item in rated_list:\n",
    "            try:\n",
    "                candidates[item2id[item]] = -10e8\n",
    "            except:\n",
    "                pass\n",
    "        ids, scores = find_k_largest(100, candidates)\n",
    "        item_names = [item_list_xsim[iid] for iid in ids]\n",
    "        rec_list[user] = list(zip(item_names, scores))\n",
    "        if i % 1000 == 0:\n",
    "            process_bar(i, user_count)\n",
    "    process_bar(user_count, user_count)\n",
    "    print('')\n",
    "    return rec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3bdc46",
   "metadata": {
    "papermill": {
     "duration": 0.02052,
     "end_time": "2024-11-19T17:55:01.312693",
     "exception": false,
     "start_time": "2024-11-19T17:55:01.292173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rec_list = generate_candidate(user_list_xsim[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d12da6",
   "metadata": {
    "papermill": {
     "duration": 2.00513,
     "end_time": "2024-11-19T17:55:03.328911",
     "exception": false,
     "start_time": "2024-11-19T17:55:01.323781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('rec_list.pickle', 'wb') as handle:\n",
    "#     pickle.dump(rec_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('weight-model/rec_list.pickle', 'rb') as handle:\n",
    "    rec_list = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854dd1ee",
   "metadata": {
    "papermill": {
     "duration": 2.844449,
     "end_time": "2024-11-19T17:55:06.185469",
     "exception": false,
     "start_time": "2024-11-19T17:55:03.341020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_sub_stage1 = pd.read_csv('submission/predict_ensemble_7_file.csv', names = ['UserId'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "data_sub_stage1 = data_sub_stage1[['UserId']+[f'item_id_{i}' for i in range(1,16)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393bf7c",
   "metadata": {
    "papermill": {
     "duration": 0.055072,
     "end_time": "2024-11-19T17:55:06.253584",
     "exception": false,
     "start_time": "2024-11-19T17:55:06.198512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chuyển từ dạng wide sang long\n",
    "df_long = data_sub_stage1.melt(id_vars=['UserId'], value_vars=[col for col in data_sub_stage1.columns if col.startswith('item_id')], \n",
    "                  var_name='item_col', value_name='ItemId')\n",
    "df_submit = df_long.drop(columns=['item_col'])\n",
    "df_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d25b5",
   "metadata": {
    "papermill": {
     "duration": 0.480803,
     "end_time": "2024-11-19T17:55:06.746652",
     "exception": false,
     "start_time": "2024-11-19T17:55:06.265849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_features = data.groupby('ItemId').agg({'UserId':'nunique','Purchase':'mean'})\n",
    "item_features.columns = ['item_user_count','item_buy_ratio']\n",
    "\n",
    "user_features = data.groupby('UserId').agg({'ItemId':'nunique','Purchase':'mean'})\n",
    "user_features.columns = ['user_item_count','user_buy_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d72667",
   "metadata": {
    "papermill": {
     "duration": 0.242653,
     "end_time": "2024-11-19T17:55:07.001156",
     "exception": false,
     "start_time": "2024-11-19T17:55:06.758503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_dict_to_dataframe(input_dict):\n",
    "    \"\"\"\n",
    "    Convert a dictionary with nested tuples to a DataFrame\n",
    "    \n",
    "    Args:\n",
    "        input_dict (dict): Dictionary with user_id as key and list of (item_id, score) tuples as value\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with user_id and item_id columns\n",
    "    \"\"\"\n",
    "    # Prepare lists to store data\n",
    "    user_ids = []\n",
    "    item_ids = []\n",
    "    \n",
    "    # Iterate through the dictionary\n",
    "    for user_id, items in input_dict.items():\n",
    "        for item_id, _ in items[:15]:\n",
    "            user_ids.append(user_id)\n",
    "            item_ids.append(item_id)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'UserId': user_ids,\n",
    "        'ItemId': item_ids\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "negative_sample = convert_dict_to_dataframe(rec_list)\n",
    "negative_sample['Click'] = 0\n",
    "negative_sample['Purchase'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498de673",
   "metadata": {
    "papermill": {
     "duration": 0.621772,
     "end_time": "2024-11-19T17:55:07.635043",
     "exception": false,
     "start_time": "2024-11-19T17:55:07.013271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_rerank = pd.concat([data, negative_sample])\n",
    "# # Chuyển embeddings thành array để dễ tính toán\n",
    "# embedding_dim = len(next(iter(item_emb.values())))  # = 1024\n",
    "    \n",
    "# Tìm item cuối cùng có rating = 1 cho mỗi user\n",
    "last_items = (data_rerank[data_rerank['Click'] >0]\n",
    "                 .sort_values('UserId')\n",
    "                 .groupby('UserId')\n",
    "                 .last()\n",
    "                 .reset_index()\n",
    "                 [['UserId', 'ItemId']])\n",
    "    \n",
    "# Tạo dictionary để map user_id với last_item_id\n",
    "user_last_item = dict(zip(last_items['UserId'], last_items['ItemId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d21e5",
   "metadata": {
    "papermill": {
     "duration": 1.630363,
     "end_time": "2024-11-19T17:55:09.277071",
     "exception": false,
     "start_time": "2024-11-19T17:55:07.646708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_rerank.to_csv(f'{outpath}/input.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34646cbd",
   "metadata": {
    "papermill": {
     "duration": 0.050712,
     "end_time": "2024-11-19T17:55:09.340964",
     "exception": false,
     "start_time": "2024-11-19T17:55:09.290252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScoreCalculator:\n",
    "    def __init__(self, embedding_configs, user_last_item, batch_size=1000):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        embedding_configs: dict\n",
    "            Cấu hình embeddings cho các model\n",
    "        user_last_item: dict\n",
    "            Dictionary mapping user đến item cuối cùng\n",
    "        batch_size: int\n",
    "            Số lượng rows xử lý mỗi batch\n",
    "        \"\"\"\n",
    "        self.embedding_configs = embedding_configs\n",
    "        self.user_last_item = user_last_item\n",
    "        self.batch_size = batch_size\n",
    "        # Initialize feature_columns with all possible columns at init\n",
    "        self.feature_columns = []\n",
    "        # Pre-generate all score column names\n",
    "        for model_name in self.embedding_configs:\n",
    "            self.feature_columns.extend([\n",
    "                f'score_{model_name}_dot',\n",
    "                f'score_{model_name}_cosine',\n",
    "                f'score_item_{model_name}_dot',\n",
    "                f'score_item_{model_name}_cosine'\n",
    "            ])\n",
    "\n",
    "    def _create_score_columns(self, df):\n",
    "        \"\"\"Khởi tạo các cột score\"\"\"\n",
    "        # Initialize all columns with 0.0\n",
    "        for col in self.feature_columns:\n",
    "            df[col] = 0.0\n",
    "        return df\n",
    "\n",
    "    def _process_batch(self, batch_df, user_matrix, item_matrix, user_to_idx, item_to_idx, \n",
    "                      user_last_items, model_name):\n",
    "        \"\"\"Xử lý một batch của DataFrame\"\"\"\n",
    "        try:\n",
    "            user_indices = batch_df['UserId'].map(user_to_idx).values\n",
    "            item_indices = batch_df['ItemId'].map(item_to_idx).values\n",
    "\n",
    "            # Lấy embeddings cho batch hiện tại\n",
    "            batch_user_embeddings = user_matrix[user_indices]\n",
    "            batch_item_embeddings = item_matrix[item_indices]\n",
    "\n",
    "            # Tính dot product scores\n",
    "            dot_scores = np.sum(batch_user_embeddings * batch_item_embeddings, axis=1)\n",
    "            batch_df[f'score_{model_name}_dot'] = dot_scores\n",
    "\n",
    "            # Tính cosine similarity với last item\n",
    "            valid_users = batch_df['UserId'].isin(self.user_last_item)\n",
    "            if valid_users.any():\n",
    "                last_item_embeddings = item_matrix[user_last_items[user_indices]]\n",
    "                \n",
    "                # Normalize và tính cosine similarity\n",
    "                last_item_norm = np.linalg.norm(last_item_embeddings, axis=1, keepdims=True)\n",
    "                current_item_norm = np.linalg.norm(batch_item_embeddings, axis=1, keepdims=True)\n",
    "                \n",
    "                cosine_scores = np.sum(\n",
    "                    (last_item_embeddings / np.maximum(last_item_norm, 1e-8)) * \n",
    "                    (batch_item_embeddings / np.maximum(current_item_norm, 1e-8)),\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                batch_df.loc[valid_users, f'score_{model_name}_cosine'] = cosine_scores[valid_users]\n",
    "                batch_df.loc[valid_users, f'score_item_{model_name}_dot'] = np.sum(\n",
    "                    last_item_embeddings[valid_users] * batch_item_embeddings[valid_users], \n",
    "                    axis=1\n",
    "                )\n",
    "                batch_df.loc[valid_users, f'score_item_{model_name}_cosine'] = cosine_scores[valid_users]\n",
    "\n",
    "            # Giải phóng bộ nhớ\n",
    "            del batch_user_embeddings, batch_item_embeddings\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing batch for {model_name}: {str(e)}\")\n",
    "            \n",
    "        return batch_df\n",
    "\n",
    "    def calculate_scores(self, df):\n",
    "        \"\"\"\n",
    "        Tính toán scores theo batch để tiết kiệm RAM\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting score calculation...\")\n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # Tính rating\n",
    "        result_df['rating'] = result_df['Click'] \n",
    "        result_df = result_df.drop(['Click', 'Purchase'], axis=1)\n",
    "        \n",
    "        # Khởi tạo cột scores\n",
    "        result_df = self._create_score_columns(result_df)\n",
    "        \n",
    "        # Xử lý từng model\n",
    "        for model_name, embeddings in self.embedding_configs.items():\n",
    "            logger.info(f\"Processing model: {model_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Chuẩn bị embeddings\n",
    "                unique_users = result_df['UserId'].unique()\n",
    "                unique_items = result_df['ItemId'].unique()\n",
    "                \n",
    "                user_to_idx = {uid: idx for idx, uid in enumerate(unique_users)}\n",
    "                item_to_idx = {iid: idx for idx, iid in enumerate(unique_items)}\n",
    "                \n",
    "                # Tạo user last items array\n",
    "                user_last_items = np.zeros(len(unique_users), dtype=int)\n",
    "                for uid, last_iid in self.user_last_item.items():\n",
    "                    if uid in user_to_idx and last_iid in item_to_idx:\n",
    "                        user_last_items[user_to_idx[uid]] = item_to_idx[last_iid]\n",
    "                \n",
    "                # Load embeddings vào matrix\n",
    "                dim = len(next(iter(embeddings['user_embedding'].values())))\n",
    "                user_matrix = np.zeros((len(unique_users), dim))\n",
    "                item_matrix = np.zeros((len(unique_items), dim))\n",
    "                \n",
    "                for uid, emb in embeddings['user_embedding'].items():\n",
    "                    if uid in user_to_idx:\n",
    "                        user_matrix[user_to_idx[uid]] = emb\n",
    "                        \n",
    "                for iid, emb in embeddings['item_embedding'].items():\n",
    "                    if iid in item_to_idx:\n",
    "                        item_matrix[item_to_idx[iid]] = emb\n",
    "                \n",
    "                # Xử lý theo batch\n",
    "                num_batches = len(result_df) // self.batch_size + 1\n",
    "                for i in tqdm(range(num_batches), desc=f\"Processing {model_name}\"):\n",
    "                    start_idx = i * self.batch_size\n",
    "                    end_idx = min((i + 1) * self.batch_size, len(result_df))\n",
    "                    batch_df = result_df.iloc[start_idx:end_idx].copy()\n",
    "                    \n",
    "                    batch_df = self._process_batch(\n",
    "                        batch_df, user_matrix, item_matrix,\n",
    "                        user_to_idx, item_to_idx, user_last_items,\n",
    "                        model_name\n",
    "                    )\n",
    "                    \n",
    "                    # Cập nhật kết quả\n",
    "                    result_df.iloc[start_idx:end_idx] = batch_df\n",
    "                    \n",
    "                    # Giải phóng bộ nhớ\n",
    "                    del batch_df\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Giải phóng bộ nhớ sau khi xử lý xong một model\n",
    "                del user_matrix, item_matrix\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing model {model_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(\"Score calculation completed\")\n",
    "        # Ensure consistent column order\n",
    "        return result_df[['UserId', 'ItemId', 'rating'] + self.feature_columns]\n",
    "\n",
    "\n",
    "def process_dataframe_in_chunks(input_path, output_dir, final_output_path, embedding_configs, \n",
    "                              user_last_item, chunk_size=10000, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Xử lý DataFrame theo chunks và lưu thành các file CSV riêng\n",
    "    \"\"\"\n",
    "    calculator = ScoreCalculator(embedding_configs, user_last_item, batch_size)\n",
    "    \n",
    "    # Tạo thư mục output nếu chưa tồn tại\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Xóa các file CSV cũ trong thư mục output (nếu có)\n",
    "    for f in glob.glob(os.path.join(output_dir, \"chunk_*.csv\")):\n",
    "        os.remove(f)\n",
    "    \n",
    "    try:\n",
    "        # Đọc và xử lý từng chunk\n",
    "        chunk_files = []\n",
    "        for chunk_idx, chunk in enumerate(pd.read_csv(input_path, chunksize=chunk_size)):\n",
    "            logger.info(f\"Processing chunk {chunk_idx + 1}\")\n",
    "            \n",
    "            # Tính scores cho chunk\n",
    "            result_chunk = calculator.calculate_scores(chunk)\n",
    "            \n",
    "            # Verify column count\n",
    "            expected_cols = 3 + len(calculator.feature_columns)  # UserId, ItemId, rating + feature columns\n",
    "            if len(result_chunk.columns) != expected_cols:\n",
    "                raise ValueError(f\"Unexpected number of columns in chunk {chunk_idx}: \"\n",
    "                               f\"got {len(result_chunk.columns)}, expected {expected_cols}\")\n",
    "            \n",
    "            # Lưu chunk vào file CSV\n",
    "            chunk_path = os.path.join(output_dir, f\"chunk_{chunk_idx:04d}.csv\")\n",
    "            result_chunk.to_csv(chunk_path, index=False)\n",
    "            chunk_files.append(chunk_path)\n",
    "            \n",
    "            # In thông tin debug\n",
    "            logger.info(f\"Chunk {chunk_idx} columns: {list(result_chunk.columns)}\")\n",
    "            \n",
    "            # Giải phóng bộ nhớ\n",
    "            del result_chunk\n",
    "            gc.collect()\n",
    "        \n",
    "        logger.info(\"Merging CSV files...\")\n",
    "        \n",
    "        # Đọc header từ chunk đầu tiên\n",
    "        first_chunk = pd.read_csv(chunk_files[0], nrows=0)\n",
    "        columns = first_chunk.columns.tolist()\n",
    "        \n",
    "        # Ghi file cuối cùng\n",
    "        with open(final_output_path, 'w', newline='') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerow(columns)\n",
    "            \n",
    "            for chunk_file in tqdm(chunk_files, desc=\"Merging files\"):\n",
    "                with open(chunk_file, 'r', newline='') as infile:\n",
    "                    next(infile)  # Skip header\n",
    "                    for line in infile:\n",
    "                        outfile.write(line)\n",
    "                        \n",
    "                try:\n",
    "                    os.remove(chunk_file)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not remove temporary file {chunk_file}: {str(e)}\")\n",
    "        \n",
    "        logger.info(f\"Final results saved to: {final_output_path}\")\n",
    "        return final_output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during processing: {str(e)}\")\n",
    "        # Cleanup temp files\n",
    "        for f in glob.glob(os.path.join(output_dir, \"chunk_*.csv\")):\n",
    "            try:\n",
    "                os.remove(f)\n",
    "            except:\n",
    "                pass\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6d1d8",
   "metadata": {
    "papermill": {
     "duration": 211.036687,
     "end_time": "2024-11-19T17:58:40.389168",
     "exception": false,
     "start_time": "2024-11-19T17:55:09.352481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gc\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Cách sử dụng:\n",
    "embedding_configs = {\n",
    "    'simgcl': {\n",
    "        'user_embedding': user_embedding_simgcl,\n",
    "        'item_embedding': item_embedding_simgcl\n",
    "    },\n",
    "    'lightgcn': {\n",
    "        'user_embedding': user_embedding_lightgcn,\n",
    "        'item_embedding': item_embedding_lightgcn\n",
    "    },\n",
    "    'xsim': {\n",
    "        'user_embedding': user_embedding_xsim,\n",
    "        'item_embedding': item_embedding_xsim\n",
    "    },\n",
    "    'xsim_denoise': {\n",
    "        'user_embedding': user_embedding_xsim_denoise,\n",
    "        'item_embedding': item_embedding_xsim_standard\n",
    "    },\n",
    "    'xsim_standard': {\n",
    "        'user_embedding': user_embedding_xsim_standard,\n",
    "        'item_embedding': item_embedding_xsim_standard\n",
    "    }\n",
    "}\n",
    "# Xử lý dữ liệu\n",
    "final_csv_path = process_dataframe_in_chunks(\n",
    "    input_path=f'{outpath}/input.csv',\n",
    "    output_dir=f'{outpath}/temp_csv',     # Thư mục chứa các file CSV tạm thời\n",
    "    final_output_path=f'{outpath}/output.csv',  # File CSV cuối cùng\n",
    "    embedding_configs=embedding_configs,\n",
    "    user_last_item=user_last_item,\n",
    "    chunk_size=400000,  # Số rows mỗi chunk\n",
    "    batch_size=400000   # Số rows mỗi batch trong tính toán\n",
    ")\n",
    "\n",
    "# # Nếu muốn đọc kết quả:\n",
    "# # result_df = pd.read_csv(final_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceeb63b",
   "metadata": {
    "papermill": {
     "duration": 0.033208,
     "end_time": "2024-11-19T17:58:40.438142",
     "exception": false,
     "start_time": "2024-11-19T17:58:40.404934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_embed_feature['ItemId'] = item_embed_feature['ItemId'].astype(str)\n",
    "user_embed_feature['UserId'] = user_embed_feature['UserId'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80479b",
   "metadata": {
    "papermill": {
     "duration": 30.891677,
     "end_time": "2024-11-19T17:59:11.345297",
     "exception": false,
     "start_time": "2024-11-19T17:58:40.453620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df = pd.read_csv(final_csv_path)\n",
    "result_df = result_df.merge(item_features, left_on='ItemId', right_index=True, how='left').fillna(0)\n",
    "result_df = result_df.merge(user_features, left_on='UserId', right_index=True, how='left').fillna(0)\n",
    "result_df = result_df.merge(item_embed_feature, on='ItemId', how='left').fillna(0)\n",
    "result_df = result_df.merge(user_embed_feature, on='UserId', how='left').fillna(0)\n",
    "result_df = result_df.merge(buy_order_full, on='ItemId', how='left').fillna(0)\n",
    "result_df = result_df.merge(buy_order_standard, on='ItemId', how='left').fillna(0)\n",
    "result_df = result_df.merge(cart_order_full, on='ItemId', how='left').fillna(0)\n",
    "result_df = result_df.merge(cart_order_standard, on='ItemId', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd91c0",
   "metadata": {
    "papermill": {
     "duration": 0.024029,
     "end_time": "2024-11-19T17:59:11.384738",
     "exception": false,
     "start_time": "2024-11-19T17:59:11.360709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in ['item_user_count', 'user_item_count']:\n",
    "#     data_rerank[i] = (data_rerank[i] - data_rerank[i].min())/(data_rerank[i].max()-data_rerank[i].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5a014",
   "metadata": {
    "papermill": {
     "duration": 19.330058,
     "end_time": "2024-11-19T17:59:30.730767",
     "exception": false,
     "start_time": "2024-11-19T17:59:11.400709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submit['Click'] = 0\n",
    "df_submit['Purchase'] = 0\n",
    "df_submit.to_csv(f'{outpath}/input_submit.csv', index = None)\n",
    "\n",
    "submit_csv_path = process_dataframe_in_chunks(\n",
    "    input_path=f'{outpath}/input_submit.csv',\n",
    "    output_dir=f'{outpath}/temp_csv',     # Thư mục chứa các file CSV tạm thời\n",
    "    final_output_path=f'{outpath}/output_submit.csv',  # File CSV cuối cùng\n",
    "    embedding_configs=embedding_configs,\n",
    "    user_last_item=user_last_item,\n",
    "    chunk_size=400000,  # Số rows mỗi chunk\n",
    "    batch_size=400000   # Số rows mỗi batch trong tính toán\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aaa493",
   "metadata": {
    "papermill": {
     "duration": 2.249877,
     "end_time": "2024-11-19T17:59:32.998341",
     "exception": false,
     "start_time": "2024-11-19T17:59:30.748464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submit = pd.read_csv(submit_csv_path)\n",
    "df_submit = df_submit.merge(item_features, left_on='ItemId', right_index=True, how='left').fillna(0)\n",
    "df_submit = df_submit.merge(user_features, left_on='UserId', right_index=True, how='left').fillna(0)\n",
    "df_submit = df_submit.merge(item_embed_feature, on='ItemId', how='left').fillna(0)\n",
    "df_submit = df_submit.merge(user_embed_feature, on='UserId', how='left').fillna(0)\n",
    "df_submit = df_submit.merge(buy_order_full, on='ItemId', how='left').fillna(0)\n",
    "df_submit = df_submit.merge(buy_order_standard, on='ItemId', how='left').fillna(0)\n",
    "df_submit = df_submit.merge(cart_order_full, on='ItemId', how='left').fillna(0)\n",
    "df_submit = df_submit.merge(cart_order_standard, on='ItemId', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d19c8",
   "metadata": {
    "papermill": {
     "duration": 2.326635,
     "end_time": "2024-11-19T17:59:35.341724",
     "exception": false,
     "start_time": "2024-11-19T17:59:33.015089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df_nostandard  = result_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1474be",
   "metadata": {
    "papermill": {
     "duration": 0.285544,
     "end_time": "2024-11-19T17:59:35.644288",
     "exception": false,
     "start_time": "2024-11-19T17:59:35.358744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in ['score_simgcl_dot', 'score_simgcl_cosine',\n",
    "       'score_item_simgcl_dot', 'score_item_simgcl_cosine',\n",
    "       'score_lightgcn_dot', 'score_lightgcn_cosine',\n",
    "       'score_item_lightgcn_dot', 'score_item_lightgcn_cosine',\n",
    "       'score_xsim_dot', 'score_xsim_cosine', 'score_item_xsim_dot',\n",
    "       'score_item_xsim_cosine', 'score_xsim_denoise_dot',\n",
    "       'score_xsim_denoise_cosine', 'score_item_xsim_denoise_dot',\n",
    "       'score_item_xsim_denoise_cosine', 'score_xsim_standard_dot',\n",
    "       'score_xsim_standard_cosine', 'score_item_xsim_standard_dot',\n",
    "       'score_item_xsim_standard_cosine']:\n",
    "    result_df[i] = (result_df[i] - result_df[i].min())/(result_df[i].max()-result_df[i].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35c9d5",
   "metadata": {
    "papermill": {
     "duration": 0.052081,
     "end_time": "2024-11-19T17:59:35.713620",
     "exception": false,
     "start_time": "2024-11-19T17:59:35.661539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in ['score_simgcl_dot', 'score_simgcl_cosine',\n",
    "       'score_item_simgcl_dot', 'score_item_simgcl_cosine',\n",
    "       'score_lightgcn_dot', 'score_lightgcn_cosine',\n",
    "       'score_item_lightgcn_dot', 'score_item_lightgcn_cosine',\n",
    "       'score_xsim_dot', 'score_xsim_cosine', 'score_item_xsim_dot',\n",
    "       'score_item_xsim_cosine', 'score_xsim_denoise_dot',\n",
    "       'score_xsim_denoise_cosine', 'score_item_xsim_denoise_dot',\n",
    "       'score_item_xsim_denoise_cosine', 'score_xsim_standard_dot',\n",
    "       'score_xsim_standard_cosine', 'score_item_xsim_standard_dot',\n",
    "       'score_item_xsim_standard_cosine']:\n",
    "    df_submit[i] = (df_submit[i] - df_submit[i].min())/(df_submit[i].max()-df_submit[i].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930f8ce",
   "metadata": {
    "papermill": {
     "duration": 1.285162,
     "end_time": "2024-11-19T17:59:37.015494",
     "exception": false,
     "start_time": "2024-11-19T17:59:35.730332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = result_df[result_df.UserId.isin(test_user_id)]\n",
    "train = result_df.drop(test.index)\n",
    "# infer = df_test_submit.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8fa0a",
   "metadata": {
    "papermill": {
     "duration": 0.169312,
     "end_time": "2024-11-19T17:59:37.203116",
     "exception": false,
     "start_time": "2024-11-19T17:59:37.033804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_baskets = train.groupby(['UserId'])['ItemId'].count().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbfa54",
   "metadata": {
    "papermill": {
     "duration": 0.754123,
     "end_time": "2024-11-19T17:59:37.974908",
     "exception": false,
     "start_time": "2024-11-19T17:59:37.220785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# columns_to_use = ['score_simgcl_dot', 'score_simgcl_cosine',\n",
    "#        'score_item_simgcl_dot', 'score_item_simgcl_cosine',\n",
    "#        'score_lightgcn_dot', 'score_lightgcn_cosine',\n",
    "#        'score_item_lightgcn_dot', 'score_item_lightgcn_cosine',\n",
    "#        'score_xsim_dot', 'score_xsim_cosine', 'score_item_xsim_dot',\n",
    "#        'score_item_xsim_cosine', 'score_xsim_denoise_dot',\n",
    "#        'score_xsim_denoise_cosine', 'score_item_xsim_denoise_dot',\n",
    "#        'score_item_xsim_denoise_cosine', 'score_xsim_standard_dot',\n",
    "#        'score_xsim_standard_cosine', 'score_item_xsim_standard_dot',\n",
    "#        'score_item_xsim_standard_cosine', 'item_user_count', 'item_buy_ratio',\n",
    "#        'user_item_count', 'user_buy_ratio']\n",
    "columns_to_use = train.columns[3:]\n",
    "train_X = train[columns_to_use]\n",
    "train_y = train['rating']\n",
    "\n",
    "test_X = test[columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a29a1c",
   "metadata": {
    "papermill": {
     "duration": 1.112255,
     "end_time": "2024-11-19T17:59:39.105756",
     "exception": false,
     "start_time": "2024-11-19T17:59:37.993501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMRanker\n",
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=150,\n",
    "    importance_type='gain',\n",
    "    verbose=10,\n",
    "    learning_rate = 0.05,\n",
    "    # max_depth = 8,\n",
    "    # num_leaves = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab945f",
   "metadata": {
    "papermill": {
     "duration": 109.430133,
     "end_time": "2024-11-19T18:01:28.553237",
     "exception": false,
     "start_time": "2024-11-19T17:59:39.123104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ranker = ranker.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    group=train_baskets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787dc3f0",
   "metadata": {
    "papermill": {
     "duration": 0.032711,
     "end_time": "2024-11-19T18:01:28.610366",
     "exception": false,
     "start_time": "2024-11-19T18:01:28.577655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# clf = lgb.LGBMClassifier(class_weight = {0: 0.3, 1:0.7})\n",
    "# clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94b764",
   "metadata": {
    "papermill": {
     "duration": 0.038068,
     "end_time": "2024-11-19T18:01:28.673467",
     "exception": false,
     "start_time": "2024-11-19T18:01:28.635399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "feature_important = []\n",
    "for i in ranker.feature_importances_.argsort()[::-1]:\n",
    "    cnt+=1\n",
    "    feature_important.append(columns_to_use[i])\n",
    "    print(columns_to_use[i], ranker.feature_importances_[i]/ranker.feature_importances_.sum())\n",
    "    if cnt == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b5635",
   "metadata": {
    "papermill": {
     "duration": 0.03655,
     "end_time": "2024-11-19T18:01:28.734568",
     "exception": false,
     "start_time": "2024-11-19T18:01:28.698018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_ndcg(predictions: dict, labels: dict, N: int = None) -> float:\n",
    "    \"\"\"\n",
    "    Tính NDCG cho predicted rankings so với ground truth labels\n",
    "    \n",
    "    Args:\n",
    "        predictions (dict): Dictionary chứa predicted rankings của mỗi user\n",
    "        labels (dict): Dictionary chứa ground truth items của mỗi user\n",
    "        N (int): Số lượng items tối đa để xét (NDCG@N)\n",
    "    \n",
    "    Returns:\n",
    "        float: Giá trị NDCG trung bình của tất cả users\n",
    "    \"\"\"\n",
    "    sum_ndcg = 0\n",
    "    \n",
    "    for user_id in predictions:\n",
    "        if user_id not in labels:\n",
    "            continue\n",
    "            \n",
    "        # Tính DCG cho predicted ranking\n",
    "        dcg = 0\n",
    "        for pos, item in enumerate(predictions[user_id][:N]):\n",
    "            # Kiểm tra xem item có trong labels không\n",
    "            if item in labels[user_id]:\n",
    "                dcg += 1.0 / math.log2(pos + 2)\n",
    "        \n",
    "        # Tính IDCG cho ground truth\n",
    "        idcg = 0\n",
    "        for pos in range(min(N, len(labels[user_id]))):\n",
    "            idcg += 1.0 / math.log2(pos + 2)\n",
    "        \n",
    "        # Cộng dồn NDCG của user hiện tại\n",
    "        if idcg != 0:\n",
    "            sum_ndcg += dcg / idcg\n",
    "    \n",
    "    # Tính trung bình NDCG của tất cả users\n",
    "    avg_ndcg = round(sum_ndcg / len(predictions), 5)\n",
    "    \n",
    "    return avg_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79bcc6",
   "metadata": {
    "papermill": {
     "duration": 1.852718,
     "end_time": "2024-11-19T18:01:30.612113",
     "exception": false,
     "start_time": "2024-11-19T18:01:28.759395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['pred'] = ranker.predict(test_X)\n",
    "tmp_test = test \\\n",
    "    .sort_values(['UserId', 'pred'], ascending=False) \\\n",
    "    .groupby('UserId')['ItemId'].apply(list).to_dict()\n",
    "label = test[test.rating != 0].sort_values(['UserId', 'rating'], ascending=False).groupby('UserId')['ItemId'].apply(list).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88683b97",
   "metadata": {
    "papermill": {
     "duration": 0.066357,
     "end_time": "2024-11-19T18:01:30.710410",
     "exception": false,
     "start_time": "2024-11-19T18:01:30.644053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "calculate_ndcg(tmp_test, label, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f885a1",
   "metadata": {
    "papermill": {
     "duration": 0.148353,
     "end_time": "2024-11-19T18:01:30.884420",
     "exception": false,
     "start_time": "2024-11-19T18:01:30.736067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_use = feature_important\n",
    "train_X = train[columns_to_use]\n",
    "train_y = train['rating']\n",
    "\n",
    "test_X = test[columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b23ea6b",
   "metadata": {
    "papermill": {
     "duration": 39.317197,
     "end_time": "2024-11-19T18:02:10.228302",
     "exception": false,
     "start_time": "2024-11-19T18:01:30.911105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=150,\n",
    "    importance_type='gain',\n",
    "    verbose=10,\n",
    "    learning_rate = 0.05,\n",
    "    # max_depth = 8,\n",
    "    # num_leaves = 10,\n",
    ")\n",
    "ranker = ranker.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    group=train_baskets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788837e",
   "metadata": {
    "papermill": {
     "duration": 0.03899,
     "end_time": "2024-11-19T18:02:10.299472",
     "exception": false,
     "start_time": "2024-11-19T18:02:10.260482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in clf.feature_importances_.argsort()[::-1]:\n",
    "#     print(columns_to_use[i], clf.feature_importances_[i]/clf.feature_importances_.sum(), clf.feature_importances_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433a780",
   "metadata": {
    "papermill": {
     "duration": 1.434775,
     "end_time": "2024-11-19T18:02:11.765263",
     "exception": false,
     "start_time": "2024-11-19T18:02:10.330488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['pred'] = ranker.predict(test_X)\n",
    "tmp_test = test \\\n",
    "    .sort_values(['UserId', 'pred'], ascending=False) \\\n",
    "    .groupby('UserId')['ItemId'].apply(list).to_dict()\n",
    "label = test[test.rating != 0].sort_values(['UserId', 'rating'], ascending=False).groupby('UserId')['ItemId'].apply(list).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3af13d",
   "metadata": {
    "papermill": {
     "duration": 0.068271,
     "end_time": "2024-11-19T18:02:11.864466",
     "exception": false,
     "start_time": "2024-11-19T18:02:11.796195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "calculate_ndcg(tmp_test, label, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7aeb25",
   "metadata": {
    "papermill": {
     "duration": 0.397688,
     "end_time": "2024-11-19T18:02:12.292970",
     "exception": false,
     "start_time": "2024-11-19T18:02:11.895282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submit['pred'] = ranker.predict(df_submit[columns_to_use])\n",
    "# test['pred'] = clf.predict_proba(test_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0469d8",
   "metadata": {
    "papermill": {
     "duration": 0.257358,
     "end_time": "2024-11-19T18:02:12.581664",
     "exception": false,
     "start_time": "2024-11-19T18:02:12.324306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_submit = df_submit \\\n",
    "    .sort_values(['UserId', 'pred'], ascending=False) \\\n",
    "    .groupby('UserId')['ItemId'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be79894",
   "metadata": {
    "papermill": {
     "duration": 2.081799,
     "end_time": "2024-11-19T18:02:14.694700",
     "exception": false,
     "start_time": "2024-11-19T18:02:12.612901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('data/test_set_private.csv', names = ['user_id']+[f'item_id_{i}' for i in range(1, 1001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab1b42",
   "metadata": {
    "papermill": {
     "duration": 0.039168,
     "end_time": "2024-11-19T18:02:14.764853",
     "exception": false,
     "start_time": "2024-11-19T18:02:14.725685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f016d58",
   "metadata": {
    "papermill": {
     "duration": 3.543815,
     "end_time": "2024-11-19T18:02:18.339905",
     "exception": false,
     "start_time": "2024-11-19T18:02:14.796090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(submit)):\n",
    "    uid = submit.loc[i, 'user_id']\n",
    "    list_item =  tmp_submit[uid] \n",
    "    for j in range(len(list_item)):\n",
    "        submit.loc[i, f'item_id_{j+1}'] = list_item[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93274c3f",
   "metadata": {
    "papermill": {
     "duration": 2.608019,
     "end_time": "2024-11-19T18:02:20.980173",
     "exception": false,
     "start_time": "2024-11-19T18:02:18.372154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit.to_csv(f'{outpath}/predict.csv', header = None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c3e3b",
   "metadata": {
    "papermill": {
     "duration": 0.07778,
     "end_time": "2024-11-19T18:02:21.093261",
     "exception": false,
     "start_time": "2024-11-19T18:02:21.015481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10213328,
     "datasetId": 6119502,
     "sourceId": 9950831,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10216535,
     "datasetId": 6115971,
     "sourceId": 9953703,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10216706,
     "datasetId": 6049052,
     "sourceId": 9953859,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10210382,
     "datasetId": 5965973,
     "sourceId": 9948179,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10194161,
     "datasetId": 6047721,
     "sourceId": 9933751,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10213534,
     "datasetId": 6076061,
     "sourceId": 9951016,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "rapidsai",
   "language": "python",
   "name": "rapidsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 502.073867,
   "end_time": "2024-11-19T18:02:23.760664",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-19T17:54:01.686797",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
