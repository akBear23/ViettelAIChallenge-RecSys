{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/VHAC-track-ds/SELFRec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids-24.10/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd SELFRec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from data.loader import FileIO\n",
    "from util.conf import ModelConf\n",
    "from util.sampler import next_batch_pairwise\n",
    "from util.loss_torch import bpr_loss, l2_reg_loss, InfoNCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((375372, 4), (14551, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/training_set.csv')\n",
    "predict_df = pd.read_csv('../data/public_testset.csv', names=['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "\n",
    "test_user_id = predict_df['user_id'].values\n",
    "\n",
    "item_columns = predict_df.columns[1:]  # Lấy tất cả cột trừ cột uid\n",
    "item_in_test_df = pd.unique(predict_df[item_columns].values.ravel())\n",
    "#Lọc bỏ user_id bị lẫn trong tập test\n",
    "user_list = df.UserId.unique()\n",
    "item_in_test_df = list(set(item_in_test_df).difference(set(user_list)))\n",
    "\n",
    "test_df = df[~df.ItemId.isin(item_in_test_df)].sample(frac=0.05, random_state=42)  # 90% for train\n",
    "# train_df = df\n",
    "train_df = df.drop(test_df.index).groupby('UserId').tail(20)  # Remaining 10% for test\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train and test sets as .txt files without headers, separated by a space\n",
    "train_df.to_csv(\"train.txt\", index=False, header=False, sep=\" \")\n",
    "test_df.to_csv(\"test.txt\", index=False, header=False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Click</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tyviMi4b8Q</td>\n",
       "      <td>tyviMi4b8Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tyviMi4b8Q</td>\n",
       "      <td>t3fB9Nq1VY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tyviMi4b8Q</td>\n",
       "      <td>X7ZdDwPBKM</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tyviMi4b8Q</td>\n",
       "      <td>r4bieaZn66</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tyviMi4b8Q</td>\n",
       "      <td>cRl2I3cVGB</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389918</th>\n",
       "      <td>16jkTS9Vj2</td>\n",
       "      <td>5wWKMYcpiw</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389919</th>\n",
       "      <td>16jkTS9Vj2</td>\n",
       "      <td>lHTwXgiMo7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389920</th>\n",
       "      <td>16jkTS9Vj2</td>\n",
       "      <td>ZBmAO23TI8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389921</th>\n",
       "      <td>16jkTS9Vj2</td>\n",
       "      <td>h9tLDUYcw6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389922</th>\n",
       "      <td>16jkTS9Vj2</td>\n",
       "      <td>IidvQKwM1B</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>389923 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            UserId      ItemId  Click  Purchase\n",
       "0       tyviMi4b8Q  tyviMi4b8Q      1         0\n",
       "1       tyviMi4b8Q  t3fB9Nq1VY      1         0\n",
       "2       tyviMi4b8Q  X7ZdDwPBKM      1         0\n",
       "3       tyviMi4b8Q  r4bieaZn66      1         0\n",
       "4       tyviMi4b8Q  cRl2I3cVGB      1         0\n",
       "...            ...         ...    ...       ...\n",
       "389918  16jkTS9Vj2  5wWKMYcpiw      1         0\n",
       "389919  16jkTS9Vj2  lHTwXgiMo7      1         0\n",
       "389920  16jkTS9Vj2  ZBmAO23TI8      1         0\n",
       "389921  16jkTS9Vj2  h9tLDUYcw6      1         0\n",
       "389922  16jkTS9Vj2  IidvQKwM1B      1         0\n",
       "\n",
       "[389923 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G44YOVuyss</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMG5jGqKx0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TAOplc7QCE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RH44ej4IUR</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N3Bg7PqmT4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserId  index\n",
       "0  G44YOVuyss      0\n",
       "1  PMG5jGqKx0      1\n",
       "2  TAOplc7QCE      2\n",
       "3  RH44ej4IUR      3\n",
       "4  N3Bg7PqmT4      4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_map = {UserId: index for index, UserId in enumerate(user_list_denoise)}\n",
    "user_map = pd.DataFrame(list(user_map.items()), columns=['UserId', 'index'])\n",
    "user_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemId</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cRl2I3cVGB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0N6lG60HQq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OSxJGDgCe0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A5IrTeoxuI</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOxmzBDWzA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemId  index\n",
       "0  cRl2I3cVGB      0\n",
       "1  0N6lG60HQq      1\n",
       "2  OSxJGDgCe0      2\n",
       "3  A5IrTeoxuI      3\n",
       "4  NOxmzBDWzA      4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_list = df.ItemId.unique()\n",
    "item_map = {ItemId:index for index, ItemId in enumerate(item_list)}\n",
    "item_map = pd.DataFrame(list(item_map.items()), columns=['ItemId', 'index'])\n",
    "item_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Click</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30504</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30504</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30504</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30504</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30504</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  ItemId  Click  Purchase\n",
       "0   30504       0      1         0\n",
       "1   30504       1      1         0\n",
       "2   30504       2      1         0\n",
       "3   30504       3      1         0\n",
       "4   30504       4      1         0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['UserId'] = df.UserId.map(dict(user_map[['UserId', 'index']].values))\n",
    "df['ItemId'] = df.ItemId.map(dict(item_map[['ItemId', 'index']].values))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ItemId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[4754, 12283, 1744, 61315, 13834]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[2589, 367, 7845, 34501, 55607]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[1677, 468, 11939, 8500, 8781, 47834, 37349, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[13030, 26968, 14389, 16387, 68546, 6199, 1544...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[6194, 5839, 6193, 5833, 3689]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId                                             ItemId\n",
       "0       0                  [4754, 12283, 1744, 61315, 13834]\n",
       "1       1                    [2589, 367, 7845, 34501, 55607]\n",
       "2       2  [1677, 468, 11939, 8500, 8781, 47834, 37349, 4...\n",
       "3       3  [13030, 26968, 14389, 16387, 68546, 6199, 1544...\n",
       "4       4                     [6194, 5839, 6193, 5833, 3689]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_user = df.groupby('UserId')['ItemId'].apply(list).reset_index()\n",
    "df_by_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list_test = user_map.copy()\n",
    "user_list_test\n",
    "train_df = []\n",
    "test_df = []\n",
    "for _, row in df_by_user.iterrows():\n",
    "    user_id = row['UserId']\n",
    "    item_ids = row['ItemId']\n",
    "    \n",
    "    if user_id in user_list_test['index'].values:\n",
    "        # Split last item into test_df and the rest into train_df\n",
    "        train_df.append({'UserId': user_id, 'ItemId': item_ids[:-1]})\n",
    "        test_df.append({'UserId': user_id, 'ItemId': [item_ids[-1]]})\n",
    "    else:\n",
    "        # All items go to train_df if user is not in user_list_test\n",
    "        train_df.append({'UserId': user_id, 'ItemId': item_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_df)\n",
    "test_df = pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_df['ItemId'] = df_by_user['ItemId'][:-1] if df_by_user['UserId'] is in user_list_test\n",
    "# train_df = df_by_user.sample(frac=0.95, random_state=42)  # 90% for train\n",
    "# # train_df = df\n",
    "# test_df = df_by_user.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_sequential.txt\", \"w\") as file:\n",
    "    for _, row in train_df.iterrows():\n",
    "        user_index = row['UserId']\n",
    "        item_indices = ' '.join(map(str, row['ItemId']))\n",
    "        file.write(f\"{user_index}:{item_indices}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_sequential.txt\", \"w\") as file:\n",
    "    for _, row in test_df.iterrows():\n",
    "        user_index = row['UserId']\n",
    "        item_indices = ' '.join(map(str, row['ItemId']))\n",
    "        file.write(f\"{user_index}:{item_indices}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from YAML file: {'SASRec': {'drop_rate': 0.2, 'mask_rate': 0.5, 'n_blocks': 2, 'n_heads': 1}, 'batch.size': 256, 'embedding.size': 128, 'item.ranking.topN': [10, 20], 'learning.rate': 0.001, 'max.epoch': 50, 'max.len': 50, 'model': {'name': 'BERT4Rec', 'type': 'sequential'}, 'output': './results/', 'reg.lambda': 0.0001, 'test.set': '/kaggle/working/sample/test_sequential.txt', 'training.set': '/kaggle/working/sample/train_sequential.txt'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"/kaggle/working/sample/train_sequential.txt\",\n",
    "    \"test.set\": \"/kaggle/working/sample/test_sequential.txt\",\n",
    "    # \"training.set\": \"./dataset/amazon-beauty/train.txt\",\n",
    "    # \"test.set\": \"./dataset/amazon-beauty/test.txt\",\n",
    "    \"model\": {\n",
    "        \"name\": \"BERT4Rec\",\n",
    "        \"type\": \"sequential\"\n",
    "    },\n",
    "    \"item.ranking.topN\": [10, 20],\n",
    "    \"embedding.size\": 128,\n",
    "    \"max.epoch\": 50,\n",
    "    \"batch.size\": 256,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "    \"max.len\": 50,\n",
    "    \"SASRec\": {\n",
    "        \"n_blocks\":2,\n",
    "        \"drop_rate\":0.2,\n",
    "        \"n_heads\":1, \n",
    "        \"mask_rate\": 0.5\n",
    "    },\n",
    "    \"output\": \"./results/\"\n",
    "}\n",
    "\n",
    "# Save this dictionary as a YAML file\n",
    "yaml_file_path = \"config.yaml\"\n",
    "with open(yaml_file_path, \"w\") as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "# Load the YAML file to verify content\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    conf = yaml.safe_load(file)\n",
    "\n",
    "print(\"Loaded data from YAML file:\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from base.seq_recommender import SequentialRecommender\n",
    "from util.sampler import next_batch_sequence\n",
    "from util.loss_torch import l2_reg_loss\n",
    "from util.structure import PointWiseFeedForward\n",
    "from math import floor\n",
    "import random\n",
    "\n",
    "\n",
    "# Paper: BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer, CIKM'19\n",
    "\n",
    "class BERT4Rec(SequentialRecommender):\n",
    "    def __init__(self, conf, training_set, test_set):\n",
    "        super(BERT4Rec, self).__init__(conf, training_set, test_set)\n",
    "        args =self.config['BERT4Rec']\n",
    "        block_num = int(args['n_blocks'])\n",
    "        drop_rate = float(args['drop_rate'])\n",
    "        head_num = int(args['n_heads'])\n",
    "        self.aug_rate = float(args['mask_rate'])\n",
    "        self.model = BERT_Encoder(self.data, self.emb_size, self.max_len, block_num,head_num,drop_rate)\n",
    "\n",
    "    def train(self):\n",
    "        model = self.model.cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lRate)\n",
    "        for epoch in range(self.maxEpoch):\n",
    "            model.train()\n",
    "            #self.fast_evaluation(epoch)\n",
    "            for n, batch in enumerate(next_batch_sequence(self.data, self.batch_size,max_len=self.max_len)):\n",
    "                seq, pos, y, neg_idx, seq_len = batch\n",
    "                aug_seq, masked, labels = self.item_mask_for_bert(seq, seq_len, self.aug_rate, self.data.item_num+1)\n",
    "                seq_emb = model.forward(aug_seq, pos)\n",
    "                # item mask\n",
    "                rec_loss = self.calculate_loss(seq_emb,masked,labels)\n",
    "                batch_loss = rec_loss+ l2_reg_loss(self.reg, model.item_emb)\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                if n % 50==0:\n",
    "                    print('training:', epoch + 1, 'batch', n, 'batch_loss:', batch_loss.item(), 'rec_loss:', rec_loss.item())\n",
    "            model.eval()\n",
    "            self.fast_evaluation(epoch)\n",
    "\n",
    "    def item_mask_for_bert(self,seq,seq_len, mask_ratio, mask_idx):\n",
    "        augmented_seq = seq.copy()\n",
    "        masked = np.zeros_like(augmented_seq)\n",
    "        labels = []\n",
    "        for i, s in enumerate(seq):\n",
    "            to_be_masked = random.sample(range(seq_len[i]), max(floor(seq_len[i]*mask_ratio),1))\n",
    "            masked[i, to_be_masked] = 1\n",
    "            labels += list(augmented_seq[i, to_be_masked])\n",
    "            augmented_seq[i, to_be_masked] = mask_idx\n",
    "        return augmented_seq, masked, np.array(labels)\n",
    "\n",
    "    def calculate_loss(self, seq_emb, masked, labels):\n",
    "        seq_emb = seq_emb[masked>0].view(-1, self.emb_size)\n",
    "        logits = torch.mm(seq_emb, self.model.item_emb.t())\n",
    "        loss = F.cross_entropy(logits, torch.tensor(labels).to(torch.int64).cuda())/labels.shape[0]\n",
    "        return loss\n",
    "\n",
    "    def predict(self,seq, pos,seq_len):\n",
    "        with torch.no_grad():\n",
    "            for i,length in enumerate(seq_len):\n",
    "                if length == self.max_len:\n",
    "                    seq[i,:length-1] = seq[i,1:]\n",
    "                    pos[i,:length-1] = pos[i,1:]\n",
    "                    pos[i, length-1] = length\n",
    "                    seq[i, length-1] = self.data.item_num+1\n",
    "                else:\n",
    "                    pos[i, length] = length+1\n",
    "                    seq[i,length] = self.data.item_num+1\n",
    "            seq_emb = self.model.forward(seq,pos)\n",
    "            last_item_embeddings = [seq_emb[i,last-1,:].view(-1,self.emb_size) for i,last in enumerate(seq_len)]\n",
    "            score = torch.matmul(torch.cat(last_item_embeddings,0), self.model.item_emb.transpose(0, 1))\n",
    "        return score.cpu().numpy()\n",
    "\n",
    "class BERT_Encoder(nn.Module):\n",
    "    def __init__(self, data, emb_size, max_len, n_blocks, n_heads, drop_rate):\n",
    "        super(BERT_Encoder, self).__init__()\n",
    "        self.data = data\n",
    "        self.emb_size = emb_size\n",
    "        self.block_num = n_blocks\n",
    "        self.head_num = n_heads\n",
    "        self.drop_rate = drop_rate\n",
    "        self.max_len = max_len\n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        self.item_emb = nn.Parameter(initializer(torch.empty(self.data.item_num+2, self.emb_size)))\n",
    "        self.pos_emb = nn.Parameter(initializer(torch.empty(self.max_len+2, self.emb_size)))\n",
    "        self.attention_layer_norms = torch.nn.ModuleList()\n",
    "        self.attention_layers = torch.nn.ModuleList()\n",
    "        self.forward_layer_norms = torch.nn.ModuleList()\n",
    "        self.forward_layers = torch.nn.ModuleList()\n",
    "        self.emb_dropout = torch.nn.Dropout(self.drop_rate)\n",
    "        self.last_layer_norm = torch.nn.LayerNorm(self.emb_size, eps=1e-8)\n",
    "\n",
    "        for n in range(self.block_num):\n",
    "            self.attention_layer_norms.append(torch.nn.LayerNorm(self.emb_size, eps=1e-8))\n",
    "            new_attn_layer =  torch.nn.MultiheadAttention(self.emb_size, self.head_num, self.drop_rate)\n",
    "            self.attention_layers.append(new_attn_layer)\n",
    "            self.forward_layer_norms.append(torch.nn.LayerNorm(self.emb_size, eps=1e-8))\n",
    "            new_fwd_layer = PointWiseFeedForward(self.emb_size, self.drop_rate,'gelu')\n",
    "            self.forward_layers.append(new_fwd_layer)\n",
    "\n",
    "    def forward(self, seq, pos):\n",
    "        seq_emb = self.item_emb[seq]\n",
    "        seq_emb *= self.emb_size ** 0.5\n",
    "        pos_emb = self.pos_emb[pos]\n",
    "        seq_emb += pos_emb\n",
    "        seq_emb = self.emb_dropout(seq_emb)\n",
    "        timeline_mask = torch.BoolTensor(seq == 0).cuda()\n",
    "        seq_emb *= ~timeline_mask.unsqueeze(-1)\n",
    "        # tl = seq_emb.shape[1]\n",
    "        # attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool).cuda())\n",
    "        for i in range(len(self.attention_layers)):\n",
    "            seq_emb = torch.transpose(seq_emb, 0, 1)\n",
    "            normalized_emb = self.attention_layer_norms[i](seq_emb)\n",
    "            mha_outputs, _ = self.attention_layers[i](normalized_emb, seq_emb, seq_emb, attn_mask=None)\n",
    "            seq_emb = normalized_emb + mha_outputs\n",
    "            seq_emb = torch.transpose(seq_emb, 0, 1)\n",
    "            seq_emb = self.forward_layer_norms[i](seq_emb)\n",
    "            seq_emb = self.forward_layers[i](seq_emb)\n",
    "            seq_emb *= ~timeline_mask.unsqueeze(-1)\n",
    "        seq_emb = self.last_layer_norm(seq_emb)\n",
    "        return seq_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import FileIO\n",
    "\n",
    "\n",
    "class SELFRec(object):\n",
    "    def __init__(self, config):\n",
    "        self.social_data = []\n",
    "        self.feature_data = []\n",
    "        self.config = config\n",
    "        self.training_data = FileIO.load_data_set(config['training.set'], config['model']['type'])\n",
    "        self.test_data = FileIO.load_data_set(config['test.set'], config['model']['type'])\n",
    "\n",
    "        self.kwargs = {}\n",
    "#         if config.contain('social.data'):\n",
    "#             social_data = FileIO.load_social_data(self.config['social.data'])\n",
    "#             self.kwargs['social.data'] = social_data\n",
    "        # if config.contains('feature.data'):\n",
    "        #     self.social_data = FileIO.loadFeature(config,self.config['feature.data'])\n",
    "        print('Reading data and preprocessing...')\n",
    "\n",
    "    def execute(self):\n",
    "        # import the model module\n",
    "#         import_str = f\"from model.{self.config['model']['type']}.{self.config['model']['name']} import {self.config['model']['name']}\"\n",
    "#         exec(import_str)\n",
    "        recommender = f\"{self.config['model']['name']}(self.config,self.training_data,self.test_data,**self.kwargs)\"\n",
    "        return eval(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file is not found!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT4Rec\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m conf \u001b[38;5;241m=\u001b[39m \u001b[43mModelConf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/sample/config.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m BERT4Rec \u001b[38;5;241m=\u001b[39m SELFRec(conf)\u001b[38;5;241m.\u001b[39mexecute()\n",
      "File \u001b[0;32m/workspace/viettel-ai-challenge-track-ds/SELFRec/util/conf.py:8\u001b[0m, in \u001b[0;36mModelConf.__init__\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/viettel-ai-challenge-track-ds/SELFRec/util/conf.py:22\u001b[0m, in \u001b[0;36mModelConf.read_configuration\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfig file is not found!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = 'BERT4Rec'\n",
    "conf = ModelConf('/kaggle/working/sample/config.yaml')\n",
    "BERT4Rec = SELFRec(conf).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT4Rec.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"/kaggle/working/sample/train_sequential.txt\",\n",
    "    \"test.set\": \"/kaggle/working/sample/test_sequential.txt\",\n",
    "    # \"training.set\": \"./dataset/amazon-beauty/train.txt\",\n",
    "    # \"test.set\": \"./dataset/amazon-beauty/test.txt\",\n",
    "    \"model\": {\n",
    "        \"name\": \"SASRec\",\n",
    "        \"type\": \"sequential\"\n",
    "    },\n",
    "    \"item.ranking.topN\": [10, 20],\n",
    "    \"embedding.size\": 64,\n",
    "    \"max.epoch\": 50,\n",
    "    \"batch.size\": 256,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "    \"max.len\": 50,\n",
    "    \"SASRec\": {\n",
    "        \"n_blocks\":2,\n",
    "        \"drop_rate\":0.2,\n",
    "        \"n_heads\":1\n",
    "    },\n",
    "    \"output\": \"./results/\"\n",
    "}\n",
    "\n",
    "# Save this dictionary as a YAML file\n",
    "yaml_file_path = \"config.yaml\"\n",
    "with open(yaml_file_path, \"w\") as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "# Load the YAML file to verify content\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    conf = yaml.safe_load(file)\n",
    "\n",
    "print(\"Loaded data from YAML file:\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from base.seq_recommender import SequentialRecommender\n",
    "from util.sampler import next_batch_sequence\n",
    "from util.structure import PointWiseFeedForward\n",
    "from util.loss_torch import l2_reg_loss\n",
    "\n",
    "\n",
    "# Paper: Self-Attentive Sequential Recommendation\n",
    "# Code Referred: https://github.com/pmixer/SASRec.pytorch/\n",
    "\n",
    "\n",
    "class SASRec(SequentialRecommender):\n",
    "    def __init__(self, conf, training_set, test_set):\n",
    "        super(SASRec, self).__init__(conf, training_set, test_set)\n",
    "        args = self.config['SASRec']\n",
    "        block_num = int(args['n_blocks'])\n",
    "        drop_rate = float(args['drop_rate'])\n",
    "        head_num = int(args['n_heads'])\n",
    "        self.model = SASRec_Model(self.data, self.emb_size, self.max_len, block_num,head_num,drop_rate)\n",
    "        self.rec_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def train(self):\n",
    "        model = self.model.cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lRate)\n",
    "        for epoch in range(self.maxEpoch):\n",
    "            model.train()\n",
    "            #self.fast_evaluation(epoch)\n",
    "            for n, batch in enumerate(next_batch_sequence(self.data, self.batch_size,max_len=self.max_len)):\n",
    "                seq, pos, y, neg_idx, _ = batch\n",
    "                seq_emb = model.forward(seq, pos)\n",
    "                rec_loss = self.calculate_loss(seq_emb, y, neg_idx, pos)\n",
    "                batch_loss = rec_loss+ l2_reg_loss(self.reg, model.item_emb)\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                if n % 50==0:\n",
    "                    print('training:', epoch + 1, 'batch', n, 'rec_loss:', batch_loss.item())\n",
    "            model.eval()\n",
    "            self.fast_evaluation(epoch)\n",
    "\n",
    "    def calculate_loss(self, seq_emb, y, neg,pos):\n",
    "        y_emb = self.model.item_emb[y]\n",
    "        neg_emb = self.model.item_emb[neg]\n",
    "        pos_logits = (seq_emb * y_emb).sum(dim=-1)\n",
    "        neg_logits = (seq_emb * neg_emb).sum(dim=-1)\n",
    "        pos_labels, neg_labels = torch.ones(pos_logits.shape).cuda(), torch.zeros(neg_logits.shape).cuda()\n",
    "        indices = np.where(pos != 0)\n",
    "        loss = self.rec_loss(pos_logits[indices], pos_labels[indices])\n",
    "        loss += self.rec_loss(neg_logits[indices], neg_labels[indices])\n",
    "        return loss\n",
    "\n",
    "    def predict(self,seq, pos,seq_len):\n",
    "        with torch.no_grad():\n",
    "            seq_emb = self.model.forward(seq,pos)\n",
    "            last_item_embeddings = [seq_emb[i,last-1,:].view(-1,self.emb_size) for i,last in enumerate(seq_len)]\n",
    "            score = torch.matmul(torch.cat(last_item_embeddings,0), self.model.item_emb.transpose(0, 1))\n",
    "        return score.cpu().numpy()\n",
    "\n",
    "\n",
    "class SASRec_Model(nn.Module):\n",
    "    def __init__(self, data, emb_size, max_len, n_blocks, n_heads, drop_rate):\n",
    "        super(SASRec_Model, self).__init__()\n",
    "        self.data = data\n",
    "        self.emb_size = emb_size\n",
    "        self.block_num = n_blocks\n",
    "        self.head_num = n_heads\n",
    "        self.drop_rate = drop_rate\n",
    "        self.max_len = max_len\n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        self.item_emb = nn.Parameter(initializer(torch.empty(self.data.item_num+1, self.emb_size)))\n",
    "        self.pos_emb = nn.Parameter(initializer(torch.empty(self.max_len+1, self.emb_size)))\n",
    "        self.attention_layer_norms = torch.nn.ModuleList()\n",
    "        self.attention_layers = torch.nn.ModuleList()\n",
    "        self.forward_layer_norms = torch.nn.ModuleList()\n",
    "        self.forward_layers = torch.nn.ModuleList()\n",
    "        self.emb_dropout = torch.nn.Dropout(self.drop_rate)\n",
    "        self.last_layer_norm = torch.nn.LayerNorm(self.emb_size, eps=1e-8)\n",
    "\n",
    "        for n in range(self.block_num):\n",
    "            self.attention_layer_norms.append(torch.nn.LayerNorm(self.emb_size, eps=1e-8))\n",
    "            new_attn_layer =  torch.nn.MultiheadAttention(self.emb_size, self.head_num, self.drop_rate)\n",
    "            self.attention_layers.append(new_attn_layer)\n",
    "            self.forward_layer_norms.append(torch.nn.LayerNorm(self.emb_size, eps=1e-8))\n",
    "            new_fwd_layer = PointWiseFeedForward(self.emb_size, self.drop_rate)\n",
    "            self.forward_layers.append(new_fwd_layer)\n",
    "\n",
    "    def forward(self, seq, pos):\n",
    "        seq_emb = self.item_emb[seq]\n",
    "        seq_emb *= self.emb_size ** 0.5\n",
    "        pos_emb = self.pos_emb[pos]\n",
    "        seq_emb += pos_emb\n",
    "        seq_emb = self.emb_dropout(seq_emb)\n",
    "        timeline_mask = torch.BoolTensor(seq == 0).cuda()\n",
    "        seq_emb *= ~timeline_mask.unsqueeze(-1)\n",
    "        tl = seq_emb.shape[1]\n",
    "        attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool).cuda())\n",
    "        for i in range(len(self.attention_layers)):\n",
    "            seq_emb = torch.transpose(seq_emb, 0, 1)\n",
    "            #attention_input = seq_emb\n",
    "            normalized_emb = self.attention_layer_norms[i](seq_emb)\n",
    "            mha_outputs, _ = self.attention_layers[i](normalized_emb, seq_emb, seq_emb, attn_mask=attention_mask)\n",
    "            seq_emb = normalized_emb + mha_outputs\n",
    "            seq_emb = torch.transpose(seq_emb, 0, 1)\n",
    "            seq_emb = self.forward_layer_norms[i](seq_emb)\n",
    "            seq_emb = self.forward_layers[i](seq_emb)\n",
    "            seq_emb *=  ~timeline_mask.unsqueeze(-1)\n",
    "        seq_emb = self.last_layer_norm(seq_emb)\n",
    "        return seq_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import FileIO\n",
    "\n",
    "\n",
    "class SELFRec(object):\n",
    "    def __init__(self, config):\n",
    "        self.social_data = []\n",
    "        self.feature_data = []\n",
    "        self.config = config\n",
    "        self.training_data = FileIO.load_data_set(config['training.set'], config['model']['type'])\n",
    "        self.test_data = FileIO.load_data_set(config['test.set'], config['model']['type'])\n",
    "\n",
    "        self.kwargs = {}\n",
    "#         if config.contain('social.data'):\n",
    "#             social_data = FileIO.load_social_data(self.config['social.data'])\n",
    "#             self.kwargs['social.data'] = social_data\n",
    "        # if config.contains('feature.data'):\n",
    "        #     self.social_data = FileIO.loadFeature(config,self.config['feature.data'])\n",
    "        print('Reading data and preprocessing...')\n",
    "\n",
    "    def execute(self):\n",
    "        # import the model module\n",
    "#         import_str = f\"from model.{self.config['model']['type']}.{self.config['model']['name']} import {self.config['model']['name']}\"\n",
    "#         exec(import_str)\n",
    "        recommender = f\"{self.config['model']['name']}(self.config,self.training_data,self.test_data,**self.kwargs)\"\n",
    "        return eval(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'SASRec'\n",
    "conf = ModelConf('/kaggle/working/sample/config.yaml')\n",
    "SASRec = SELFRec(conf).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SASRec.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.read_csv('/kaggle/input/vhac-recsys/public_testset.csv', names=['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "predict_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rec = XSimGCL\n",
    "# from util.algorithm import find_k_largest\n",
    "# def test(self):\n",
    "#     def process_bar(num, total):\n",
    "#         rate = float(num) / total\n",
    "#         ratenum = int(50 * rate)\n",
    "#         print(f'\\rProgress: [{\"+\" * ratenum}{\" \" * (50 - ratenum)}]{ratenum * 2}%', end='', flush=True)\n",
    "\n",
    "#     rec_list = {}\n",
    "#     data_train = pd.DataFrame(self.data.training_data, columns= ['uid', 'iid', 'rating'])\n",
    "#     self.data.train_set = data_train[data_train['uid'].isin(test_user_id)].values.tolist()\n",
    "#     user_count = len(self.data.train_set)\n",
    "    \n",
    "#     for i, user in enumerate(self.data.train_set):\n",
    "#         user = user[0]\n",
    "#         candidates = self.predict(user)\n",
    "#         rated_list, _ = self.data.user_rated(user)\n",
    "#         for item in rated_list:\n",
    "#             candidates[self.data.item[item]] = -10e8\n",
    "#         # ids, scores = find_k_largest(1000, candidates)\n",
    "#         item_names = predict_df[predict_df.user_id == user].values[0][1:]\n",
    "#         scores = []\n",
    "#         for item in item_names:\n",
    "#             try:\n",
    "#                 id_tmp = self.data.item[item]\n",
    "#                 scores.append(candidates[id_tmp])\n",
    "#             except:\n",
    "#                 # Cần sửa khuyến nghị cold start\n",
    "#                 scores.append(0)\n",
    "        \n",
    "#         sorted_list = sorted(list(zip(item_names, scores)), key=lambda x: x[1], reverse=True)\n",
    "#         rec_list[user] = sorted_list\n",
    "#         if i % 1000 == 0:\n",
    "#             process_bar(i, user_count)\n",
    "#     process_bar(user_count, user_count)\n",
    "#     print('')\n",
    "#     return rec_list\n",
    "    \n",
    "# rec_list = test(XSimGCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base.recommender import Recommender\n",
    "from data.sequence import Sequence\n",
    "from util.algorithm import find_k_largest\n",
    "from util.evaluation import ranking_evaluation\n",
    "from util.sampler import next_batch_sequence_for_test\n",
    "\n",
    "def test(self):\n",
    "    def process_bar(num, total):\n",
    "        rate = float(num) / total\n",
    "        ratenum = int(50 * rate)\n",
    "        print(f'\\rProgress: [{\"+\" * ratenum}{\" \" * (50 - ratenum)}]{ratenum * 2}%', end='', flush=True)\n",
    "\n",
    "    rec_list = {}\n",
    "    for n, batch in enumerate(next_batch_sequence_for_test(self.data, self.batch_size, max_len=self.max_len)):\n",
    "        seq, pos, seq_len = batch\n",
    "        seq_start = n * self.batch_size\n",
    "        seq_end = (n + 1) * self.batch_size\n",
    "        seq_names = [seq_full[0] for seq_full in self.data.original_seq[seq_start:seq_end]]\n",
    "        candidates = self.predict(seq, pos, seq_len)\n",
    "        for name, res in zip(seq_names, candidates):\n",
    "            ids, scores = find_k_largest(1000, res)\n",
    "            \n",
    "            item_names = [self.data.id2item[iid] for iid in ids if iid != 0 and iid <= self.data.item_num]\n",
    "            rec_list[name] = list(zip(item_names, scores))\n",
    "        if n % 100 == 0:\n",
    "            process_bar(n, self.data.raw_seq_num / self.batch_size)\n",
    "    process_bar(self.data.raw_seq_num, self.data.raw_seq_num)\n",
    "    print('')\n",
    "    return rec_list\n",
    "rec_list = test(SASRec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 'NcVPv81tzp'\n",
    "str(user_map[user_map['UserId']==user_id]['index'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = ['9307', '791', '17778', '3542', '5114', '22446', '10432', '2426', '987', '195', '266', '3311', '14310', '903', '2538', '31311', '3384', '33927', '7016', '11233']\n",
    "item_map[item_map['index']==int(items[0])]['ItemId'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = test_user_id[0]\n",
    "user_index = str(user_map[user_map['UserId']==user_id]['index'].values[0])\n",
    "x = rec_list[user_index]\n",
    "index = str(item_map[item_map['ItemId']=='IQYqanXAvK']['index'].values[0])\n",
    "y = x[x[0] == index][1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for user_id in test_user_id:\n",
    "    user_index = str(user_map[user_map['UserId']==user_id]['index'].values[0])\n",
    "    item_names = predict_df[predict_df.user_id == user_id].values[0][1:]\n",
    "    candidates = rec_list[user_index]\n",
    "    candidate_index = [candidate[0] for candidate in candidates]\n",
    "    # items = []\n",
    "    scores = []\n",
    "    # items = [item[0] for item in rec_list[user_index]]\n",
    "    # items = [item_map[item_map['index']==int(item)]['ItemId'].values[0] for item in items]\n",
    "    \n",
    "    for item in item_names:\n",
    "        try: \n",
    "            item_index = str(item_map[item_map['ItemId'] == item]['index'].values[0])\n",
    "            if item_index in candidate_index:\n",
    "                scores.append(candidates[candidates[0] == item_index][1])\n",
    "            else:\n",
    "                scores.append(1e-8)\n",
    "        except:\n",
    "            scores.append(1e-8)\n",
    "        \n",
    "    # for item, score in candidates:\n",
    "    #     item = item_map[item_map['index']==int(item)]['ItemId'].values[0]\n",
    "    #     if item not in item_names:\n",
    "    #         score = 1e-8    \n",
    "    #     items.append(item)\n",
    "    #     scores.append(score)\n",
    "        \n",
    "    sorted_list = sorted(list(zip(item_names, scores)), key=lambda x: x[1], reverse=True)\n",
    "#         rec_list[user] = sorted_list\n",
    "    # print(sorted_list)\n",
    "    # data.append([user_id] + sorted_list)\n",
    "    data.append([user_id] + [i[0] for i in sorted_list])\n",
    "    # print(data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# Precompute mappings for faster lookups\n",
    "user_map_dict = user_map.set_index('UserId')['index'].astype(str).to_dict()\n",
    "item_map_dict = item_map.set_index('ItemId')['index'].astype(str).to_dict()\n",
    "\n",
    "data = []  # Result storage\n",
    "\n",
    "for user_id in tqdm(test_user_id):\n",
    "    # Look up user index using the precomputed dictionary\n",
    "    user_index = user_map_dict.get(user_id)\n",
    "    if not user_index:\n",
    "        continue  # Skip if user index is not found\n",
    "\n",
    "    # Retrieve item names and candidate pairs for this user\n",
    "    item_names = predict_df[predict_df.user_id == user_id].values[0][1:]\n",
    "    candidates = rec_list[user_index]\n",
    "    candidate_index = {str(candidate[0]): candidate[1] for candidate in candidates}  # Map item index to score\n",
    "\n",
    "    # Create score list for item_names by checking candidate_index\n",
    "    scores = [\n",
    "        candidate_index.get(item_map_dict.get(item), 1e-8) if item in item_map_dict else 1e-8\n",
    "        for item in item_names\n",
    "    ]\n",
    "    \n",
    "    # Sort items based on score in descending order\n",
    "    sorted_list = sorted(zip(item_names, scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Append user_id and sorted items to data\n",
    "    data.append([user_id] + [i[0] for i in sorted_list])\n",
    "\n",
    "#     print(data)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).to_csv('predict_SAS.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink \n",
    "FileLink(r'/kaggle/working/sample/predict_SAS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_1</th>\n",
       "      <th>item_id_2</th>\n",
       "      <th>item_id_3</th>\n",
       "      <th>item_id_4</th>\n",
       "      <th>item_id_5</th>\n",
       "      <th>item_id_6</th>\n",
       "      <th>item_id_7</th>\n",
       "      <th>item_id_8</th>\n",
       "      <th>item_id_9</th>\n",
       "      <th>...</th>\n",
       "      <th>item_id_991</th>\n",
       "      <th>item_id_992</th>\n",
       "      <th>item_id_993</th>\n",
       "      <th>item_id_994</th>\n",
       "      <th>item_id_995</th>\n",
       "      <th>item_id_996</th>\n",
       "      <th>item_id_997</th>\n",
       "      <th>item_id_998</th>\n",
       "      <th>item_id_999</th>\n",
       "      <th>item_id_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NcVPv81tzp</td>\n",
       "      <td>NYRp1o1YKD</td>\n",
       "      <td>nyDJiBSznn</td>\n",
       "      <td>tMMpf5S62m</td>\n",
       "      <td>pYCyajy1Am</td>\n",
       "      <td>IQYqanXAvK</td>\n",
       "      <td>K0nTlcclVU</td>\n",
       "      <td>aTcR4pfQrM</td>\n",
       "      <td>md0Mi2Y3PN</td>\n",
       "      <td>BGpHuuj0eD</td>\n",
       "      <td>...</td>\n",
       "      <td>Xi7bQ3D5Nd</td>\n",
       "      <td>DnHOgmVXrS</td>\n",
       "      <td>6Uns6g05f7</td>\n",
       "      <td>20kEvp8BVk</td>\n",
       "      <td>vVjXZVhRDe</td>\n",
       "      <td>Z2h9Oni93D</td>\n",
       "      <td>uHuaWLAHdv</td>\n",
       "      <td>RYpgK2XiNS</td>\n",
       "      <td>cFfT0AEdOb</td>\n",
       "      <td>WCm27PLRAp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C5ihgxLcrb</td>\n",
       "      <td>1bWoiCKvUG</td>\n",
       "      <td>uBZeAeJAp1</td>\n",
       "      <td>wz3fiRi1XX</td>\n",
       "      <td>kcLQeDE9Y9</td>\n",
       "      <td>XoyZaOJ2hQ</td>\n",
       "      <td>cfkOAeEV6p</td>\n",
       "      <td>lywvWPArBx</td>\n",
       "      <td>41uJKXzbXd</td>\n",
       "      <td>1kVkVTBnhQ</td>\n",
       "      <td>...</td>\n",
       "      <td>j387AnkQH6</td>\n",
       "      <td>HkvDRz6oXT</td>\n",
       "      <td>fSv7hmSvI4</td>\n",
       "      <td>LHh3iij8he</td>\n",
       "      <td>mf2eKsaafi</td>\n",
       "      <td>rKwmw637oi</td>\n",
       "      <td>nXcPqalddS</td>\n",
       "      <td>CiGuUIbCEO</td>\n",
       "      <td>5QBxhJ93BT</td>\n",
       "      <td>bo1CjxPHpP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ljMBWT9UXW</td>\n",
       "      <td>1M0GtQfkUA</td>\n",
       "      <td>iytV2FTVjK</td>\n",
       "      <td>G2Sufv4xlY</td>\n",
       "      <td>AS24sgQN69</td>\n",
       "      <td>U1uDxeAwr4</td>\n",
       "      <td>yniLMM6GB7</td>\n",
       "      <td>ziFGlTdzPp</td>\n",
       "      <td>BddfNX8HRd</td>\n",
       "      <td>S9tgPjbv1a</td>\n",
       "      <td>...</td>\n",
       "      <td>VDBauWNjJF</td>\n",
       "      <td>NSMr3KjVnR</td>\n",
       "      <td>aJUzJBXEfY</td>\n",
       "      <td>tqC65yziRM</td>\n",
       "      <td>4zDIIH126P</td>\n",
       "      <td>M1tDsFuWBH</td>\n",
       "      <td>PTvaMJlQyK</td>\n",
       "      <td>HfvagfNvGp</td>\n",
       "      <td>XUWoQVdsmN</td>\n",
       "      <td>EdwIvwMRoZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WZXCQtOWJt</td>\n",
       "      <td>2J7erZG9Af</td>\n",
       "      <td>S0Q4XoexaS</td>\n",
       "      <td>hCZTh4mNx8</td>\n",
       "      <td>dUXbCsX8et</td>\n",
       "      <td>mA7s8zgFMR</td>\n",
       "      <td>YFdqrb8OGd</td>\n",
       "      <td>C9FNEwKKd8</td>\n",
       "      <td>mtsRoBIDbs</td>\n",
       "      <td>1b49EFCcJG</td>\n",
       "      <td>...</td>\n",
       "      <td>dwVDUM6ngC</td>\n",
       "      <td>IFwPdofoEP</td>\n",
       "      <td>sFGjZgyZnH</td>\n",
       "      <td>F10yUBudDF</td>\n",
       "      <td>XK550LREfU</td>\n",
       "      <td>3uZFqvw0zz</td>\n",
       "      <td>Dto8H87MIx</td>\n",
       "      <td>tiQGe5Ye7v</td>\n",
       "      <td>CzHVYocfQY</td>\n",
       "      <td>wc7rJoeZRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xlB2rvpQKu</td>\n",
       "      <td>fZIHEceEkK</td>\n",
       "      <td>FurfEc4uKO</td>\n",
       "      <td>4FoN0bcH4x</td>\n",
       "      <td>cslhbAk2FI</td>\n",
       "      <td>Wi8OmeoDhH</td>\n",
       "      <td>uHdZ7W1MOy</td>\n",
       "      <td>xqyiojw3KO</td>\n",
       "      <td>Hj1UDRyfAc</td>\n",
       "      <td>vSeyk7gadz</td>\n",
       "      <td>...</td>\n",
       "      <td>2IpMrt6ieN</td>\n",
       "      <td>odYnPAZSJv</td>\n",
       "      <td>5FV9Qd8ABf</td>\n",
       "      <td>se19TPTKGt</td>\n",
       "      <td>tXTRyaQqsA</td>\n",
       "      <td>8KqODVTepQ</td>\n",
       "      <td>1iqudkzZxj</td>\n",
       "      <td>H7ZyQBRUQq</td>\n",
       "      <td>0NDBaPifhZ</td>\n",
       "      <td>u0altqehZS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id   item_id_1   item_id_2   item_id_3   item_id_4   item_id_5  \\\n",
       "0  NcVPv81tzp  NYRp1o1YKD  nyDJiBSznn  tMMpf5S62m  pYCyajy1Am  IQYqanXAvK   \n",
       "1  C5ihgxLcrb  1bWoiCKvUG  uBZeAeJAp1  wz3fiRi1XX  kcLQeDE9Y9  XoyZaOJ2hQ   \n",
       "2  ljMBWT9UXW  1M0GtQfkUA  iytV2FTVjK  G2Sufv4xlY  AS24sgQN69  U1uDxeAwr4   \n",
       "3  WZXCQtOWJt  2J7erZG9Af  S0Q4XoexaS  hCZTh4mNx8  dUXbCsX8et  mA7s8zgFMR   \n",
       "4  xlB2rvpQKu  fZIHEceEkK  FurfEc4uKO  4FoN0bcH4x  cslhbAk2FI  Wi8OmeoDhH   \n",
       "\n",
       "    item_id_6   item_id_7   item_id_8   item_id_9  ... item_id_991  \\\n",
       "0  K0nTlcclVU  aTcR4pfQrM  md0Mi2Y3PN  BGpHuuj0eD  ...  Xi7bQ3D5Nd   \n",
       "1  cfkOAeEV6p  lywvWPArBx  41uJKXzbXd  1kVkVTBnhQ  ...  j387AnkQH6   \n",
       "2  yniLMM6GB7  ziFGlTdzPp  BddfNX8HRd  S9tgPjbv1a  ...  VDBauWNjJF   \n",
       "3  YFdqrb8OGd  C9FNEwKKd8  mtsRoBIDbs  1b49EFCcJG  ...  dwVDUM6ngC   \n",
       "4  uHdZ7W1MOy  xqyiojw3KO  Hj1UDRyfAc  vSeyk7gadz  ...  2IpMrt6ieN   \n",
       "\n",
       "  item_id_992 item_id_993 item_id_994 item_id_995 item_id_996 item_id_997  \\\n",
       "0  DnHOgmVXrS  6Uns6g05f7  20kEvp8BVk  vVjXZVhRDe  Z2h9Oni93D  uHuaWLAHdv   \n",
       "1  HkvDRz6oXT  fSv7hmSvI4  LHh3iij8he  mf2eKsaafi  rKwmw637oi  nXcPqalddS   \n",
       "2  NSMr3KjVnR  aJUzJBXEfY  tqC65yziRM  4zDIIH126P  M1tDsFuWBH  PTvaMJlQyK   \n",
       "3  IFwPdofoEP  sFGjZgyZnH  F10yUBudDF  XK550LREfU  3uZFqvw0zz  Dto8H87MIx   \n",
       "4  odYnPAZSJv  5FV9Qd8ABf  se19TPTKGt  tXTRyaQqsA  8KqODVTepQ  1iqudkzZxj   \n",
       "\n",
       "  item_id_998 item_id_999 item_id_1000  \n",
       "0  RYpgK2XiNS  cFfT0AEdOb   WCm27PLRAp  \n",
       "1  CiGuUIbCEO  5QBxhJ93BT   bo1CjxPHpP  \n",
       "2  HfvagfNvGp  XUWoQVdsmN   EdwIvwMRoZ  \n",
       "3  tiQGe5Ye7v  CzHVYocfQY   wc7rJoeZRQ  \n",
       "4  H7ZyQBRUQq  0NDBaPifhZ   u0altqehZS  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df = pd.read_csv('../data/public_testset.csv', names=['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n"
     ]
    }
   ],
   "source": [
    "# rec = XSimGCL\n",
    "from util.algorithm import find_k_largest\n",
    "def test(self):\n",
    "    def process_bar(num, total):\n",
    "        rate = float(num) / total\n",
    "        ratenum = int(50 * rate)\n",
    "        print(f'\\rProgress: [{\"+\" * ratenum}{\" \" * (50 - ratenum)}]{ratenum * 2}%', end='', flush=True)\n",
    "\n",
    "    rec_list = {}\n",
    "    data_train = pd.DataFrame(self.data.training_data, columns= ['uid', 'iid', 'rating'])\n",
    "    self.data.train_set = data_train[data_train['uid'].isin(test_user_id)].values.tolist()\n",
    "    user_count = len(self.data.train_set)\n",
    "    \n",
    "    for i, user in enumerate(self.data.train_set):\n",
    "        user = user[0]\n",
    "        candidates = self.predict(user)\n",
    "        rated_list, _ = self.data.user_rated(user)\n",
    "        for item in rated_list:\n",
    "            candidates[self.data.item[item]] = -10e8\n",
    "        # ids, scores = find_k_largest(1000, candidates)\n",
    "        item_names = predict_df[predict_df.user_id == user].values[0][1:]\n",
    "        scores = []\n",
    "        for item in item_names:\n",
    "            try:\n",
    "                id_tmp = self.data.item[item]\n",
    "                scores.append(candidates[id_tmp])\n",
    "            except:\n",
    "                # Cần sửa khuyến nghị cold start\n",
    "                scores.append(0)\n",
    "        \n",
    "        sorted_list = sorted(list(zip(item_names, scores)), key=lambda x: x[1], reverse=True)\n",
    "        rec_list[user] = sorted_list\n",
    "        if i % 1000 == 0:\n",
    "            process_bar(i, user_count)\n",
    "    process_bar(user_count, user_count)\n",
    "    print('')\n",
    "    return rec_list\n",
    "    \n",
    "rec_list = test(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for user_id in test_user_id:\n",
    "    data.append([user_id] + [i[0] for i in rec_list[user_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).to_csv('../runs/simgcl_final_data/predict.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NcVPv81tzp</td>\n",
       "      <td>mwOc6vrVPi</td>\n",
       "      <td>oNn8dJXhM8</td>\n",
       "      <td>av2irHMO5f</td>\n",
       "      <td>NZ6ZRExaU5</td>\n",
       "      <td>wvulby9Vbm</td>\n",
       "      <td>IzLVUKs7uQ</td>\n",
       "      <td>lS7aepxu9O</td>\n",
       "      <td>95vIbbmBfu</td>\n",
       "      <td>1qeuDBCyxi</td>\n",
       "      <td>...</td>\n",
       "      <td>dLwFHGHtdv</td>\n",
       "      <td>dc6cO5BULj</td>\n",
       "      <td>RHb5SNK7zK</td>\n",
       "      <td>Pnr2iO61A7</td>\n",
       "      <td>7E8otbRXlX</td>\n",
       "      <td>E10wOdLIAT</td>\n",
       "      <td>dH19PTV6Jl</td>\n",
       "      <td>eLPGZAbPBo</td>\n",
       "      <td>CzFLPHqKk0</td>\n",
       "      <td>Zn8Fq0tFmC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C5ihgxLcrb</td>\n",
       "      <td>6jy1DwPo7L</td>\n",
       "      <td>pUUgxbyvTC</td>\n",
       "      <td>f1dROAFg11</td>\n",
       "      <td>WjZUWFn3s9</td>\n",
       "      <td>kk3smPWhZf</td>\n",
       "      <td>u0KbwAhFgw</td>\n",
       "      <td>O5nMvWF9dA</td>\n",
       "      <td>41uJKXzbXd</td>\n",
       "      <td>3wBtYUn27K</td>\n",
       "      <td>...</td>\n",
       "      <td>SCGRmTW1Wj</td>\n",
       "      <td>UJIWN83FpE</td>\n",
       "      <td>5HVET1lIrc</td>\n",
       "      <td>HlFW2LD25j</td>\n",
       "      <td>5Y0zIMvK8G</td>\n",
       "      <td>bE2kBREPHh</td>\n",
       "      <td>azEzhk6eHL</td>\n",
       "      <td>PjHKY7afPo</td>\n",
       "      <td>OcsxmB3gEE</td>\n",
       "      <td>PfQbz0wOPX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ljMBWT9UXW</td>\n",
       "      <td>gKUj4wrdaF</td>\n",
       "      <td>Hf6NKeCrqS</td>\n",
       "      <td>qwL2TeeksN</td>\n",
       "      <td>pRQEBDA4WJ</td>\n",
       "      <td>eHslZ2QF3q</td>\n",
       "      <td>U9Leg1KZce</td>\n",
       "      <td>qfMR0pa5jB</td>\n",
       "      <td>I575KzrWvJ</td>\n",
       "      <td>ABKeMXJAqU</td>\n",
       "      <td>...</td>\n",
       "      <td>DC0latQPNG</td>\n",
       "      <td>I93CDmdKET</td>\n",
       "      <td>agHKmDMeVB</td>\n",
       "      <td>H2fl09Q0Qu</td>\n",
       "      <td>Zf2Cq8y3ZT</td>\n",
       "      <td>UMZmNCdpOh</td>\n",
       "      <td>xQMipz6UAT</td>\n",
       "      <td>oURQhu3On4</td>\n",
       "      <td>MtDmLkYlDi</td>\n",
       "      <td>XmDINMQQg1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WZXCQtOWJt</td>\n",
       "      <td>V6S3fAvyI9</td>\n",
       "      <td>dQoIYlX2wJ</td>\n",
       "      <td>04zWH3pkfR</td>\n",
       "      <td>rGAG9phqUn</td>\n",
       "      <td>xbfyEc4bwt</td>\n",
       "      <td>3LWXR1uoiB</td>\n",
       "      <td>QUYCjkVSVR</td>\n",
       "      <td>Pm6sxfSbjW</td>\n",
       "      <td>SzPkpQ35GK</td>\n",
       "      <td>...</td>\n",
       "      <td>TkQR3N0QUS</td>\n",
       "      <td>MRslj3nrZb</td>\n",
       "      <td>YHTUOJAXav</td>\n",
       "      <td>vOo5dADm09</td>\n",
       "      <td>bDHaQjazq6</td>\n",
       "      <td>MLyaPYLicR</td>\n",
       "      <td>x1ykeOR3DW</td>\n",
       "      <td>pOXFMQbFqv</td>\n",
       "      <td>kk3smPWhZf</td>\n",
       "      <td>cFMFQCkWWJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xlB2rvpQKu</td>\n",
       "      <td>TNCS6O3iSq</td>\n",
       "      <td>6s5PvCvsdC</td>\n",
       "      <td>HUTuM2kdCi</td>\n",
       "      <td>49BAzOjiJz</td>\n",
       "      <td>2Mqq3N13HR</td>\n",
       "      <td>asNrzYAAKy</td>\n",
       "      <td>hsIVBBv8ch</td>\n",
       "      <td>TBJg2ARZDr</td>\n",
       "      <td>r5ERTKY2E1</td>\n",
       "      <td>...</td>\n",
       "      <td>AHG0qYYdTB</td>\n",
       "      <td>s7GrD6m2D6</td>\n",
       "      <td>Cr2ep1s4YE</td>\n",
       "      <td>7kg8LNWZYA</td>\n",
       "      <td>3QoCBriOGk</td>\n",
       "      <td>sTdWl5NgBY</td>\n",
       "      <td>ocv4aRZDB1</td>\n",
       "      <td>7gZEpavqfO</td>\n",
       "      <td>fgONK18lzp</td>\n",
       "      <td>GuqfIY4OQL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0           1           2           3           4           5     \\\n",
       "0  NcVPv81tzp  mwOc6vrVPi  oNn8dJXhM8  av2irHMO5f  NZ6ZRExaU5  wvulby9Vbm   \n",
       "1  C5ihgxLcrb  6jy1DwPo7L  pUUgxbyvTC  f1dROAFg11  WjZUWFn3s9  kk3smPWhZf   \n",
       "2  ljMBWT9UXW  gKUj4wrdaF  Hf6NKeCrqS  qwL2TeeksN  pRQEBDA4WJ  eHslZ2QF3q   \n",
       "3  WZXCQtOWJt  V6S3fAvyI9  dQoIYlX2wJ  04zWH3pkfR  rGAG9phqUn  xbfyEc4bwt   \n",
       "4  xlB2rvpQKu  TNCS6O3iSq  6s5PvCvsdC  HUTuM2kdCi  49BAzOjiJz  2Mqq3N13HR   \n",
       "\n",
       "         6           7           8           9     ...        991   \\\n",
       "0  IzLVUKs7uQ  lS7aepxu9O  95vIbbmBfu  1qeuDBCyxi  ...  dLwFHGHtdv   \n",
       "1  u0KbwAhFgw  O5nMvWF9dA  41uJKXzbXd  3wBtYUn27K  ...  SCGRmTW1Wj   \n",
       "2  U9Leg1KZce  qfMR0pa5jB  I575KzrWvJ  ABKeMXJAqU  ...  DC0latQPNG   \n",
       "3  3LWXR1uoiB  QUYCjkVSVR  Pm6sxfSbjW  SzPkpQ35GK  ...  TkQR3N0QUS   \n",
       "4  asNrzYAAKy  hsIVBBv8ch  TBJg2ARZDr  r5ERTKY2E1  ...  AHG0qYYdTB   \n",
       "\n",
       "         992         993         994         995         996         997   \\\n",
       "0  dc6cO5BULj  RHb5SNK7zK  Pnr2iO61A7  7E8otbRXlX  E10wOdLIAT  dH19PTV6Jl   \n",
       "1  UJIWN83FpE  5HVET1lIrc  HlFW2LD25j  5Y0zIMvK8G  bE2kBREPHh  azEzhk6eHL   \n",
       "2  I93CDmdKET  agHKmDMeVB  H2fl09Q0Qu  Zf2Cq8y3ZT  UMZmNCdpOh  xQMipz6UAT   \n",
       "3  MRslj3nrZb  YHTUOJAXav  vOo5dADm09  bDHaQjazq6  MLyaPYLicR  x1ykeOR3DW   \n",
       "4  s7GrD6m2D6  Cr2ep1s4YE  7kg8LNWZYA  3QoCBriOGk  sTdWl5NgBY  ocv4aRZDB1   \n",
       "\n",
       "         998         999         1000  \n",
       "0  eLPGZAbPBo  CzFLPHqKk0  Zn8Fq0tFmC  \n",
       "1  PjHKY7afPo  OcsxmB3gEE  PfQbz0wOPX  \n",
       "2  oURQhu3On4  MtDmLkYlDi  XmDINMQQg1  \n",
       "3  pOXFMQbFqv  kk3smPWhZf  cFMFQCkWWJ  \n",
       "4  7gZEpavqfO  fgONK18lzp  GuqfIY4OQL  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: predict.csv (deflated 27%)\n"
     ]
    }
   ],
   "source": [
    "!cd ../runs/simgcl_final_data/ && zip predict_simgcl_final_data.zip predict.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XSimGCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"./train.txt\",\n",
    "    \"test.set\": \"./test.txt\",\n",
    "    \"model\": {\n",
    "        \"name\": \"XSimGCL\",\n",
    "        \"type\": \"graph\"\n",
    "    },\n",
    "    \"item.ranking.topN\": [10, 20],\n",
    "    \"embedding.size\": 1024,\n",
    "    \"max.epoch\": 30,\n",
    "    \"batch.size\": 2048,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "    \"XSimGCL\": {\n",
    "        \"n_layer\": 2,\n",
    "        \"l_star\": 1,\n",
    "        \"lambda\": 0.2,\n",
    "        \"eps\": 0.2,\n",
    "        \"tau\": 0.15\n",
    "    },\n",
    "    \"output\": \"./results/\"\n",
    "}\n",
    "\n",
    "# Save this dictionary as a YAML file\n",
    "yaml_file_path = \"config.yaml\"\n",
    "with open(yaml_file_path, \"w\") as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "# Load the YAML file to verify content\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    conf = yaml.safe_load(file)\n",
    "\n",
    "print(\"Loaded data from YAML file:\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'XSimGCL'\n",
    "conf = ModelConf('/kaggle/working/sample/config.yaml')\n",
    "rec = SELFRec(conf).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from YAML file: {'LightGCN': {'n_layer': 2}, 'batch.size': 2048, 'embedding.size': 1024, 'item.ranking.topN': [10, 20], 'learning.rate': 0.001, 'max.epoch': 100, 'model': {'name': 'LightGCN', 'type': 'graph'}, 'output': './results/', 'reg.lambda': 0.0001, 'test.set': './test.txt', 'training.set': './train.txt'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"./train.txt\",\n",
    "    \"test.set\": \"./test.txt\",\n",
    "    \"model\": {\n",
    "        \"name\": \"LightGCN\",\n",
    "        \"type\": \"graph\"\n",
    "    },\n",
    "    \"item.ranking.topN\": [10, 20],\n",
    "    \"embedding.size\": 1024,\n",
    "    \"max.epoch\": 100,\n",
    "    \"batch.size\": 2048,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "    \"LightGCN\": {\n",
    "        \"n_layer\": 2\n",
    "    },\n",
    "    \"output\": \"./results/\"\n",
    "}\n",
    "\n",
    "# Save this dictionary as a YAML file\n",
    "yaml_file_path = \"config.yaml\"\n",
    "with open(yaml_file_path, \"w\") as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "# Load the YAML file to verify content\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    conf = yaml.safe_load(file)\n",
    "\n",
    "print(\"Loaded data from YAML file:\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import FileIO\n",
    "\n",
    "\n",
    "class SELFRec(object):\n",
    "    def __init__(self, config):\n",
    "        self.social_data = []\n",
    "        self.feature_data = []\n",
    "        self.config = config\n",
    "        self.training_data = FileIO.load_data_set(config['training.set'], config['model']['type'])\n",
    "        self.test_data = FileIO.load_data_set(config['test.set'], config['model']['type'])\n",
    "\n",
    "        self.kwargs = {}\n",
    "#         if config.contain('social.data'):\n",
    "#             social_data = FileIO.load_social_data(self.config['social.data'])\n",
    "#             self.kwargs['social.data'] = social_data\n",
    "        # if config.contains('feature.data'):\n",
    "        #     self.social_data = FileIO.loadFeature(config,self.config['feature.data'])\n",
    "        print('Reading data and preprocessing...')\n",
    "\n",
    "    def execute(self):\n",
    "        # import the model module\n",
    "#         import_str = f\"from model.{self.config['model']['type']}.{self.config['model']['name']} import {self.config['model']['name']}\"\n",
    "#         exec(import_str)\n",
    "        recommender = f\"{self.config['model']['name']}(self.config,self.training_data,self.test_data,**self.kwargs)\"\n",
    "        return eval(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from util.sampler import next_batch_pairwise\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "from util.loss_torch import bpr_loss,l2_reg_loss\n",
    "# paper: LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. SIGIR'20\n",
    "\n",
    "\n",
    "class LightGCN(GraphRecommender):\n",
    "    def __init__(self, conf, training_set, test_set):\n",
    "        super(LightGCN, self).__init__(conf, training_set, test_set)\n",
    "        args = self.config['LightGCN']\n",
    "        self.n_layers = int(args['n_layer'])\n",
    "        self.model = LGCN_Encoder(self.data, self.emb_size, self.n_layers)\n",
    "\n",
    "    def train(self):\n",
    "        model = self.model.cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lRate)\n",
    "        for epoch in range(self.maxEpoch):\n",
    "            for n, batch in enumerate(next_batch_pairwise(self.data, self.batch_size)):\n",
    "                user_idx, pos_idx, neg_idx = batch\n",
    "                rec_user_emb, rec_item_emb = model()\n",
    "                user_emb, pos_item_emb, neg_item_emb = rec_user_emb[user_idx], rec_item_emb[pos_idx], rec_item_emb[neg_idx]\n",
    "                batch_loss = bpr_loss(user_emb, pos_item_emb, neg_item_emb) + l2_reg_loss(self.reg, model.embedding_dict['user_emb'][user_idx],model.embedding_dict['item_emb'][pos_idx],model.embedding_dict['item_emb'][neg_idx])/self.batch_size\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                if n % 100==0 and n>0:\n",
    "                    print('training:', epoch + 1, 'batch', n, 'batch_loss:', batch_loss.item())\n",
    "            with torch.no_grad():\n",
    "                self.user_emb, self.item_emb = model()\n",
    "            if epoch % 5 == 0:\n",
    "                self.fast_evaluation(epoch)\n",
    "        self.user_emb, self.item_emb = self.best_user_emb, self.best_item_emb\n",
    "\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        with torch.no_grad():\n",
    "            self.best_user_emb, self.best_item_emb = self.model.forward()\n",
    "\n",
    "    def predict(self, u):\n",
    "        u = self.data.get_user_id(u)\n",
    "        score = torch.matmul(self.user_emb[u], self.item_emb.transpose(0, 1))\n",
    "        return score.cpu().numpy()\n",
    "\n",
    "\n",
    "class LGCN_Encoder(nn.Module):\n",
    "    def __init__(self, data, emb_size, n_layers):\n",
    "        super(LGCN_Encoder, self).__init__()\n",
    "        self.data = data\n",
    "        self.latent_size = emb_size\n",
    "        self.layers = n_layers\n",
    "        self.norm_adj = data.norm_adj\n",
    "        self.embedding_dict = self._init_model()\n",
    "        self.sparse_norm_adj = TorchGraphInterface.convert_sparse_mat_to_tensor(self.norm_adj).cuda()\n",
    "\n",
    "    def _init_model(self):\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(initializer(torch.empty(self.data.user_num, self.latent_size))),\n",
    "            'item_emb': nn.Parameter(initializer(torch.empty(self.data.item_num, self.latent_size))),\n",
    "        })\n",
    "        return embedding_dict\n",
    "\n",
    "    def forward(self):\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'], self.embedding_dict['item_emb']], 0)\n",
    "        all_embeddings = [ego_embeddings]\n",
    "        for k in range(self.layers):\n",
    "            ego_embeddings = torch.sparse.mm(self.sparse_norm_adj, ego_embeddings)\n",
    "            all_embeddings += [ego_embeddings]\n",
    "        all_embeddings = torch.stack(all_embeddings, dim=1)\n",
    "        all_embeddings = torch.mean(all_embeddings, dim=1)\n",
    "        user_all_embeddings = all_embeddings[:self.data.user_num]\n",
    "        item_all_embeddings = all_embeddings[self.data.user_num:]\n",
    "        return user_all_embeddings, item_all_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/viettel-ai-challenge-track-ds/SELFRec/base/torch_interface.py:13: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1730833640211/work/torch/csrc/utils/tensor_new.cpp:651.)\n",
      "  return torch.sparse.FloatTensor(i, v, coo.shape)\n"
     ]
    }
   ],
   "source": [
    "model = 'LightGCN'\n",
    "conf = ModelConf('./config.yaml')\n",
    "rec = SELFRec(conf).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LightGCN\n",
      "Training Set: /workspace/viettel-ai-challenge-track-ds/SELFRec/train.txt\n",
      "Test Set: /workspace/viettel-ai-challenge-track-ds/SELFRec/test.txt\n",
      "Embedding Dimension: 1024\n",
      "Maximum Epoch: 100\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 2048\n",
      "Regularization Parameter: 0.0001\n",
      "Specific parameters: n_layer:2\n",
      "Training Set Size: (user number: 36751, item number: 82673, interaction number: 375372)\n",
      "Test Set Size: (user number: 9370, item number: 9256, interaction number: 14551)\n",
      "================================================================================\n",
      "Initializing and building model...\n",
      "Training Model...\n",
      "training: 1 batch 100 batch_loss: 0.38305675983428955\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 1, Hit Ratio: 0.08671, Precision: 0.00653, Recall: 0.10335, NDCG: 0.04769\n",
      "*Best Performance*\n",
      "Epoch: 1, Hit Ratio: 0.08671, Precision: 0.00653, Recall: 0.10335, NDCG: 0.04769\n",
      "--------------------------------------------------------------------------------\n",
      "training: 2 batch 100 batch_loss: 0.18848279118537903\n",
      "training: 3 batch 100 batch_loss: 0.10197591036558151\n",
      "training: 4 batch 100 batch_loss: 0.07550816982984543\n",
      "training: 5 batch 100 batch_loss: 0.05050355941057205\n",
      "training: 6 batch 100 batch_loss: 0.03788322955369949\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 6, Hit Ratio: 0.15009, Precision: 0.0113, Recall: 0.18251, NDCG: 0.08231\n",
      "*Best Performance*\n",
      "Epoch: 6, Hit Ratio: 0.15009, Precision: 0.0113, Recall: 0.18251, NDCG: 0.08231\n",
      "--------------------------------------------------------------------------------\n",
      "training: 7 batch 100 batch_loss: 0.03206920251250267\n",
      "training: 8 batch 100 batch_loss: 0.02815219573676586\n",
      "training: 9 batch 100 batch_loss: 0.016373692080378532\n",
      "training: 10 batch 100 batch_loss: 0.018886979669332504\n",
      "training: 11 batch 100 batch_loss: 0.016056936234235764\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 11, Hit Ratio: 0.1754, Precision: 0.0132, Recall: 0.21314, NDCG: 0.09683\n",
      "*Best Performance*\n",
      "Epoch: 11, Hit Ratio: 0.1754, Precision: 0.0132, Recall: 0.21314, NDCG: 0.09683\n",
      "--------------------------------------------------------------------------------\n",
      "training: 12 batch 100 batch_loss: 0.018514111638069153\n",
      "training: 13 batch 100 batch_loss: 0.011319058015942574\n",
      "training: 14 batch 100 batch_loss: 0.009944616816937923\n",
      "training: 15 batch 100 batch_loss: 0.01008668728172779\n",
      "training: 16 batch 100 batch_loss: 0.007432110607624054\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 16, Hit Ratio: 0.19078, Precision: 0.01436, Recall: 0.23329, NDCG: 0.1066\n",
      "*Best Performance*\n",
      "Epoch: 16, Hit Ratio: 0.19078, Precision: 0.01436, Recall: 0.23329, NDCG: 0.1066\n",
      "--------------------------------------------------------------------------------\n",
      "training: 17 batch 100 batch_loss: 0.011682253330945969\n",
      "training: 18 batch 100 batch_loss: 0.008139518089592457\n",
      "training: 19 batch 100 batch_loss: 0.005001546815037727\n",
      "training: 20 batch 100 batch_loss: 0.0041450634598731995\n",
      "training: 21 batch 100 batch_loss: 0.0034154632594436407\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 21, Hit Ratio: 0.20432, Precision: 0.01538, Recall: 0.24971, NDCG: 0.11516\n",
      "*Best Performance*\n",
      "Epoch: 21, Hit Ratio: 0.20432, Precision: 0.01538, Recall: 0.24971, NDCG: 0.11516\n",
      "--------------------------------------------------------------------------------\n",
      "training: 22 batch 100 batch_loss: 0.0038868114352226257\n",
      "training: 23 batch 100 batch_loss: 0.005540220532566309\n",
      "training: 24 batch 100 batch_loss: 0.0034926787484437227\n",
      "training: 25 batch 100 batch_loss: 0.002807173179462552\n",
      "training: 26 batch 100 batch_loss: 0.0031979007180780172\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 26, Hit Ratio: 0.21106, Precision: 0.01589, Recall: 0.25882, NDCG: 0.12004\n",
      "*Best Performance*\n",
      "Epoch: 26, Hit Ratio: 0.21106, Precision: 0.01589, Recall: 0.25882, NDCG: 0.12004\n",
      "--------------------------------------------------------------------------------\n",
      "training: 27 batch 100 batch_loss: 0.004478510469198227\n",
      "training: 28 batch 100 batch_loss: 0.001403154106810689\n",
      "training: 29 batch 100 batch_loss: 0.004094607196748257\n",
      "training: 30 batch 100 batch_loss: 0.0046455212868750095\n",
      "training: 31 batch 100 batch_loss: 0.005308548454195261\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 31, Hit Ratio: 0.22134, Precision: 0.01666, Recall: 0.27161, NDCG: 0.12638\n",
      "*Best Performance*\n",
      "Epoch: 31, Hit Ratio: 0.22134, Precision: 0.01666, Recall: 0.27161, NDCG: 0.12638\n",
      "--------------------------------------------------------------------------------\n",
      "training: 32 batch 100 batch_loss: 0.001192726893350482\n",
      "training: 33 batch 100 batch_loss: 0.0017183022573590279\n",
      "training: 34 batch 100 batch_loss: 0.007010209374129772\n",
      "training: 35 batch 100 batch_loss: 0.0018072961829602718\n",
      "training: 36 batch 100 batch_loss: 0.0016160017112269998\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 36, Hit Ratio: 0.22673, Precision: 0.01707, Recall: 0.27789, NDCG: 0.13085\n",
      "*Best Performance*\n",
      "Epoch: 36, Hit Ratio: 0.22673, Precision: 0.01707, Recall: 0.27789, NDCG: 0.13085\n",
      "--------------------------------------------------------------------------------\n",
      "training: 37 batch 100 batch_loss: 0.00211356021463871\n",
      "training: 38 batch 100 batch_loss: 0.002176029374822974\n",
      "training: 39 batch 100 batch_loss: 0.0008515276131220162\n",
      "training: 40 batch 100 batch_loss: 0.0032497423235327005\n",
      "training: 41 batch 100 batch_loss: 0.003571893787011504\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 41, Hit Ratio: 0.23375, Precision: 0.01759, Recall: 0.28733, NDCG: 0.13488\n",
      "*Best Performance*\n",
      "Epoch: 41, Hit Ratio: 0.23375, Precision: 0.01759, Recall: 0.28733, NDCG: 0.13488\n",
      "--------------------------------------------------------------------------------\n",
      "training: 42 batch 100 batch_loss: 0.0015967644285410643\n",
      "training: 43 batch 100 batch_loss: 0.0030320098157972097\n",
      "training: 44 batch 100 batch_loss: 0.0014335880987346172\n",
      "training: 45 batch 100 batch_loss: 0.0017546913586556911\n",
      "training: 46 batch 100 batch_loss: 0.001293222769163549\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 46, Hit Ratio: 0.23779, Precision: 0.0179, Recall: 0.29271, NDCG: 0.13681\n",
      "*Best Performance*\n",
      "Epoch: 46, Hit Ratio: 0.23779, Precision: 0.0179, Recall: 0.29271, NDCG: 0.13681\n",
      "--------------------------------------------------------------------------------\n",
      "training: 47 batch 100 batch_loss: 0.0004617284575942904\n",
      "training: 48 batch 100 batch_loss: 0.0018266281113028526\n",
      "training: 49 batch 100 batch_loss: 0.0012845487799495459\n",
      "training: 50 batch 100 batch_loss: 0.000887954025529325\n",
      "training: 51 batch 100 batch_loss: 0.00045136408880352974\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 51, Hit Ratio: 0.24069, Precision: 0.01812, Recall: 0.29526, NDCG: 0.13991\n",
      "*Best Performance*\n",
      "Epoch: 51, Hit Ratio: 0.24069, Precision: 0.01812, Recall: 0.29526, NDCG: 0.13991\n",
      "--------------------------------------------------------------------------------\n",
      "training: 52 batch 100 batch_loss: 0.0012734205229207873\n",
      "training: 53 batch 100 batch_loss: 0.00033916160464286804\n",
      "training: 54 batch 100 batch_loss: 0.0003166111127939075\n",
      "training: 55 batch 100 batch_loss: 0.0004269174241926521\n",
      "training: 56 batch 100 batch_loss: 0.0005690050311386585\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 56, Hit Ratio: 0.24601, Precision: 0.01852, Recall: 0.30075, NDCG: 0.14186\n",
      "*Best Performance*\n",
      "Epoch: 56, Hit Ratio: 0.24601, Precision: 0.01852, Recall: 0.30075, NDCG: 0.14186\n",
      "--------------------------------------------------------------------------------\n",
      "training: 57 batch 100 batch_loss: 0.000569281168282032\n",
      "training: 58 batch 100 batch_loss: 0.00026507824077270925\n",
      "training: 59 batch 100 batch_loss: 0.0007781791500747204\n",
      "training: 60 batch 100 batch_loss: 0.00032976767397485673\n",
      "training: 61 batch 100 batch_loss: 0.0020437599159777164\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 61, Hit Ratio: 0.2492, Precision: 0.01876, Recall: 0.30452, NDCG: 0.14405\n",
      "*Best Performance*\n",
      "Epoch: 61, Hit Ratio: 0.2492, Precision: 0.01876, Recall: 0.30452, NDCG: 0.14405\n",
      "--------------------------------------------------------------------------------\n",
      "training: 62 batch 100 batch_loss: 0.00020335677254479378\n",
      "training: 63 batch 100 batch_loss: 0.0002754982269834727\n",
      "training: 64 batch 100 batch_loss: 0.0015219658380374312\n",
      "training: 65 batch 100 batch_loss: 0.00036440600524656475\n",
      "training: 66 batch 100 batch_loss: 0.00025192208704538643\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 66, Hit Ratio: 0.25204, Precision: 0.01897, Recall: 0.30859, NDCG: 0.14577\n",
      "*Best Performance*\n",
      "Epoch: 66, Hit Ratio: 0.25204, Precision: 0.01897, Recall: 0.30859, NDCG: 0.14577\n",
      "--------------------------------------------------------------------------------\n",
      "training: 67 batch 100 batch_loss: 0.0004740330623462796\n",
      "training: 68 batch 100 batch_loss: 0.00013624112762045115\n",
      "training: 69 batch 100 batch_loss: 0.0002152301894966513\n",
      "training: 70 batch 100 batch_loss: 0.00016512225556652993\n",
      "training: 71 batch 100 batch_loss: 0.002989799715578556\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 71, Hit Ratio: 0.2558, Precision: 0.01925, Recall: 0.31381, NDCG: 0.14852\n",
      "*Best Performance*\n",
      "Epoch: 71, Hit Ratio: 0.2558, Precision: 0.01925, Recall: 0.31381, NDCG: 0.14852\n",
      "--------------------------------------------------------------------------------\n",
      "training: 72 batch 100 batch_loss: 0.0012147566303610802\n",
      "training: 73 batch 100 batch_loss: 0.001114924089051783\n",
      "training: 74 batch 100 batch_loss: 0.0008855605847202241\n",
      "training: 75 batch 100 batch_loss: 0.00038662791484966874\n",
      "training: 76 batch 100 batch_loss: 9.850011701928452e-05\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 76, Hit Ratio: 0.25792, Precision: 0.01941, Recall: 0.31716, NDCG: 0.15211\n",
      "*Best Performance*\n",
      "Epoch: 76, Hit Ratio: 0.25792, Precision: 0.01941, Recall: 0.31716, NDCG: 0.15211\n",
      "--------------------------------------------------------------------------------\n",
      "training: 77 batch 100 batch_loss: 0.00026821374194696546\n",
      "training: 78 batch 100 batch_loss: 0.0022999481298029423\n",
      "training: 79 batch 100 batch_loss: 0.0009729003650136292\n",
      "training: 80 batch 100 batch_loss: 0.0008858593064360321\n",
      "training: 81 batch 100 batch_loss: 0.00017822555673774332\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 81, Hit Ratio: 0.26026, Precision: 0.01959, Recall: 0.31942, NDCG: 0.15193\n",
      "*Best Performance*\n",
      "Epoch: 81, Hit Ratio: 0.26026, Precision: 0.01959, Recall: 0.31942, NDCG: 0.15193\n",
      "--------------------------------------------------------------------------------\n",
      "training: 82 batch 100 batch_loss: 0.0002625132619868964\n",
      "training: 83 batch 100 batch_loss: 0.0002296641905559227\n",
      "training: 84 batch 100 batch_loss: 0.0002871936885640025\n",
      "training: 85 batch 100 batch_loss: 0.0001335411798208952\n",
      "training: 86 batch 100 batch_loss: 0.0003708665317390114\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 86, Hit Ratio: 0.26437, Precision: 0.0199, Recall: 0.32391, NDCG: 0.15395\n",
      "*Best Performance*\n",
      "Epoch: 86, Hit Ratio: 0.26437, Precision: 0.0199, Recall: 0.32391, NDCG: 0.15395\n",
      "--------------------------------------------------------------------------------\n",
      "training: 87 batch 100 batch_loss: 5.590599903371185e-05\n",
      "training: 88 batch 100 batch_loss: 0.00023093554773367941\n",
      "training: 89 batch 100 batch_loss: 0.00015694359899498522\n",
      "training: 90 batch 100 batch_loss: 0.0016108978306874633\n",
      "training: 91 batch 100 batch_loss: 0.0014503516722470522\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 91, Hit Ratio: 0.26664, Precision: 0.02007, Recall: 0.32728, NDCG: 0.15686\n",
      "*Best Performance*\n",
      "Epoch: 91, Hit Ratio: 0.26664, Precision: 0.02007, Recall: 0.32728, NDCG: 0.15686\n",
      "--------------------------------------------------------------------------------\n",
      "training: 92 batch 100 batch_loss: 7.288908091140911e-05\n",
      "training: 93 batch 100 batch_loss: 0.0002784701355267316\n",
      "training: 94 batch 100 batch_loss: 0.0004904919187538326\n",
      "training: 95 batch 100 batch_loss: 8.634416735731065e-05\n",
      "training: 96 batch 100 batch_loss: 0.00012711441377177835\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 96, Hit Ratio: 0.26877, Precision: 0.02023, Recall: 0.32954, NDCG: 0.1579\n",
      "*Best Performance*\n",
      "Epoch: 96, Hit Ratio: 0.26877, Precision: 0.02023, Recall: 0.32954, NDCG: 0.1579\n",
      "--------------------------------------------------------------------------------\n",
      "training: 97 batch 100 batch_loss: 0.00012102151231374592\n",
      "training: 98 batch 100 batch_loss: 0.0007876601302996278\n",
      "training: 99 batch 100 batch_loss: 0.000553405552636832\n",
      "training: 100 batch 100 batch_loss: 4.018427716800943e-05\n",
      "Testing...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Evaluating...\n",
      "The result has been output to  /workspace/viettel-ai-challenge-track-ds/SELFRec/results .\n",
      "The result of LightGCN:\n",
      "Top 10\n",
      "Hit Ratio:0.18355\n",
      "Precision:0.02763\n",
      "Recall:0.22693\n",
      "NDCG:0.13085\n",
      "Top 20\n",
      "Hit Ratio:0.26877\n",
      "Precision:0.02023\n",
      "Recall:0.32954\n",
      "NDCG:0.1579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rec.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../runs/lightgcn_private_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"../runs/lightgcn_private_data/model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "emb_user = dict([(k, v) for k, v in zip(rec.data.user.keys(), rec.best_user_emb.cpu().numpy())])\n",
    "with open(f\"../runs/lightgcn_private_data/user_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_user, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "emb_item = dict([(k, v) for k, v in zip(rec.data.item.keys(), rec.best_item_emb.cpu().numpy())])\n",
    "with open(f\"../runs/lightgcn_private_data/item_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_item, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SSL4Rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"/kaggle/working/sample/train.txt\",\n",
    "    \"test.set\": \"/kaggle/working/sample/test.txt\",\n",
    "    \"model\":{\n",
    "      \"name\": \"SSL4Rec\",\n",
    "      \"type\": \"graph\"\n",
    "    },\n",
    "\n",
    "    \"item.ranking.topN\": [20],\n",
    "\n",
    "    \"embedding.size\": 256,\n",
    "    \"max.epoch\": 100,\n",
    "    \"batch.size\": 2048,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "\n",
    "    \"SSL4Rec\":{\n",
    "      \"tau\": 0.07,\n",
    "      \"alpha\": 0.1,\n",
    "      \"drop\": 0.1\n",
    "    },\n",
    "\n",
    "    \"output\": \"./results/\"\n",
    "}\n",
    "\n",
    "# Save this dictionary as a YAML file\n",
    "yaml_file_path = \"config.yaml\"\n",
    "with open(yaml_file_path, \"w\") as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "# Load the YAML file to verify content\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    conf = yaml.safe_load(file)\n",
    "\n",
    "print(\"Loaded data from YAML file:\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import FileIO\n",
    "\n",
    "\n",
    "class SELFRec(object):\n",
    "    def __init__(self, config):\n",
    "        self.social_data = []\n",
    "        self.feature_data = []\n",
    "        self.config = config\n",
    "        self.training_data = FileIO.load_data_set(config['training.set'], config['model']['type'])\n",
    "        self.test_data = FileIO.load_data_set(config['test.set'], config['model']['type'])\n",
    "\n",
    "        self.kwargs = {}\n",
    "#         if config.contain('social.data'):\n",
    "#             social_data = FileIO.load_social_data(self.config['social.data'])\n",
    "#             self.kwargs['social.data'] = social_data\n",
    "        # if config.contains('feature.data'):\n",
    "        #     self.social_data = FileIO.loadFeature(config,self.config['feature.data'])\n",
    "        print('Reading data and preprocessing...')\n",
    "\n",
    "    def execute(self):\n",
    "        # import the model module\n",
    "#         import_str = f\"from model.{self.config['model']['type']}.{self.config['model']['name']} import {self.config['model']['name']}\"\n",
    "#         exec(import_str)\n",
    "        recommender = f\"{self.config['model']['name']}(self.config,self.training_data,self.test_data,**self.kwargs)\"\n",
    "        return eval(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from util.sampler import next_batch_pairwise\n",
    "from util.loss_torch import l2_reg_loss, InfoNCE, batch_softmax_loss\n",
    "\n",
    "# Paper: Self-supervised Learning for Large-scale Item Recommendations. CIKM'21\n",
    "\n",
    "\"\"\" \n",
    "Note: This version of code conducts feature dropout on the item embeddings \n",
    "because items features are not always available in many academic datasets.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SSL4Rec(GraphRecommender):\n",
    "    def __init__(self, conf, training_set, test_set):\n",
    "        super(SSL4Rec, self).__init__(conf, training_set, test_set)\n",
    "        args = self.config['SSL4Rec']\n",
    "        self.cl_rate = float(args['alpha'])\n",
    "        self.tau = float(args['tau'])\n",
    "        self.drop_rate = float(args['drop'])\n",
    "        self.model = DNN_Encoder(self.data, self.emb_size, self.drop_rate, self.tau)\n",
    "\n",
    "    def train(self):\n",
    "        model = self.model.cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lRate)\n",
    "        for epoch in range(self.maxEpoch):\n",
    "            for n, batch in enumerate(next_batch_pairwise(self.data, self.batch_size)):\n",
    "                query_idx, item_idx, _neg = batch\n",
    "                model.train()\n",
    "                query_emb, item_emb = model(query_idx, item_idx)\n",
    "                rec_loss = batch_softmax_loss(query_emb, item_emb, self.tau)\n",
    "                cl_loss = self.cl_rate * model.cal_cl_loss(item_idx)\n",
    "                batch_loss = rec_loss + l2_reg_loss(self.reg, query_emb, item_emb) + cl_loss\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                if n % 100 == 0:\n",
    "                    print('training:', epoch + 1, 'batch', n, 'rec_loss:', rec_loss.item(), 'cl_loss', cl_loss.item())\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                self.query_emb, self.item_emb = self.model(list(range(self.data.user_num)),list(range(self.data.item_num)))\n",
    "            self.fast_evaluation(epoch)\n",
    "        self.query_emb, self.item_emb = self.best_query_emb, self.best_item_emb\n",
    "\n",
    "    def save(self):\n",
    "        with torch.no_grad():\n",
    "            self.best_query_emb, self.best_item_emb = self.model.forward(list(range(self.data.user_num)),list(range(self.data.item_num)))\n",
    "\n",
    "    def predict(self, u):\n",
    "        u = self.data.get_user_id(u)\n",
    "        score = torch.matmul(self.query_emb[u], self.item_emb.transpose(0, 1))\n",
    "        return score.cpu().numpy()\n",
    "\n",
    "\n",
    "class DNN_Encoder(nn.Module):\n",
    "    def __init__(self, data, emb_size, drop_rate, temperature):\n",
    "        super(DNN_Encoder, self).__init__()\n",
    "        self.data = data\n",
    "        self.emb_size = emb_size\n",
    "        self.tau = temperature\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(self.emb_size, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(self.emb_size, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        self.initial_user_emb = nn.Parameter(initializer(torch.empty(self.data.user_num, self.emb_size)))\n",
    "        self.initial_item_emb = nn.Parameter(initializer(torch.empty(self.data.item_num, self.emb_size)))\n",
    "\n",
    "    def forward(self, q, x):\n",
    "        q_emb = self.initial_user_emb[q]\n",
    "        i_emb = self.initial_item_emb[x]\n",
    "\n",
    "        q_emb = self.user_tower(q_emb)\n",
    "        i_emb = self.item_tower(i_emb)\n",
    "\n",
    "        return q_emb, i_emb\n",
    "\n",
    "    def item_encoding(self, x):\n",
    "        i_emb = self.initial_item_emb[x]\n",
    "        i1_emb = self.dropout(i_emb)\n",
    "        i2_emb = self.dropout(i_emb)\n",
    "\n",
    "        i1_emb = self.item_tower(i1_emb)\n",
    "        i2_emb = self.item_tower(i2_emb)\n",
    "\n",
    "        return i1_emb, i2_emb\n",
    "\n",
    "    def cal_cl_loss(self, idx):\n",
    "        item_view1, item_view_2 = self.item_encoding(idx)       \n",
    "        cl_loss = InfoNCE(item_view1, item_view_2, self.tau)\n",
    "        return cl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'SSL4Rec'\n",
    "conf = ModelConf('/kaggle/working/sample/config.yaml')\n",
    "rec = SELFRec(conf).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SimGCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from YAML file: {'SimGCL': {'eps': 0.1, 'lambda': 0.5, 'n_layer': 3}, 'batch.size': 2048, 'embedding.size': 1024, 'item.ranking.topN': [10, 20], 'learning.rate': 0.001, 'max.epoch': 50, 'model': {'name': 'SimGCL', 'type': 'graph'}, 'output': './results/', 'reg.lambda': 0.0001, 'test.set': './test.txt', 'training.set': './train.txt'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"./train.txt\",\n",
    "    \"test.set\": \"./test.txt\",\n",
    "    \"model\":{\n",
    "      \"name\": \"SimGCL\",\n",
    "      \"type\": \"graph\"\n",
    "    },\n",
    "\n",
    "    \"item.ranking.topN\": [10, 20],\n",
    "\n",
    "    \"embedding.size\": 1024,\n",
    "    \"max.epoch\": 50,\n",
    "    \"batch.size\": 2048,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "\n",
    "    \"SimGCL\": {\n",
    "      \"n_layer\": 3,\n",
    "      \"lambda\": 0.5,\n",
    "      \"eps\": 0.1\n",
    "    },\n",
    "\n",
    "    \"output\": \"./results/\"\n",
    "}\n",
    "\n",
    "# Save this dictionary as a YAML file\n",
    "yaml_file_path = \"config.yaml\"\n",
    "with open(yaml_file_path, \"w\") as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "# Load the YAML file to verify content\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    conf = yaml.safe_load(file)\n",
    "\n",
    "print(\"Loaded data from YAML file:\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import FileIO\n",
    "\n",
    "\n",
    "class SELFRec(object):\n",
    "    def __init__(self, config):\n",
    "        self.social_data = []\n",
    "        self.feature_data = []\n",
    "        self.config = config\n",
    "        self.training_data = FileIO.load_data_set(config['training.set'], config['model']['type'])\n",
    "        self.test_data = FileIO.load_data_set(config['test.set'], config['model']['type'])\n",
    "\n",
    "        self.kwargs = {}\n",
    "#         if config.contain('social.data'):\n",
    "#             social_data = FileIO.load_social_data(self.config['social.data'])\n",
    "#             self.kwargs['social.data'] = social_data\n",
    "        # if config.contains('feature.data'):\n",
    "        #     self.social_data = FileIO.loadFeature(config,self.config['feature.data'])\n",
    "        print('Reading data and preprocessing...')\n",
    "\n",
    "    def execute(self):\n",
    "        # import the model module\n",
    "#         import_str = f\"from model.{self.config['model']['type']}.{self.config['model']['name']} import {self.config['model']['name']}\"\n",
    "#         exec(import_str)\n",
    "        recommender = f\"{self.config['model']['name']}(self.config,self.training_data,self.test_data,**self.kwargs)\"\n",
    "        return eval(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from util.sampler import next_batch_pairwise\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "from util.loss_torch import bpr_loss, l2_reg_loss, InfoNCE\n",
    "\n",
    "# Paper: Are graph augmentations necessary? simple graph contrastive learning for recommendation. SIGIR'22\n",
    "\n",
    "\n",
    "class SimGCL(GraphRecommender):\n",
    "    def __init__(self, conf, training_set, test_set):\n",
    "        super(SimGCL, self).__init__(conf, training_set, test_set)\n",
    "        args = self.config['SimGCL']\n",
    "        self.cl_rate = float(args['lambda'])\n",
    "        self.eps = float(args['eps'])\n",
    "        self.n_layers = int(args['n_layer'])\n",
    "        self.model = SimGCL_Encoder(self.data, self.emb_size, self.eps, self.n_layers)\n",
    "\n",
    "    def train(self):\n",
    "        model = self.model.cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lRate)\n",
    "        for epoch in range(self.maxEpoch):\n",
    "            for n, batch in enumerate(next_batch_pairwise(self.data, self.batch_size)):\n",
    "                user_idx, pos_idx, neg_idx = batch\n",
    "                rec_user_emb, rec_item_emb = model()\n",
    "                user_emb, pos_item_emb, neg_item_emb = rec_user_emb[user_idx], rec_item_emb[pos_idx], rec_item_emb[neg_idx]\n",
    "                rec_loss = bpr_loss(user_emb, pos_item_emb, neg_item_emb)\n",
    "                cl_loss = self.cl_rate * self.cal_cl_loss([user_idx,pos_idx])\n",
    "                batch_loss =  rec_loss + l2_reg_loss(self.reg, user_emb, pos_item_emb) + cl_loss\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                if n % 100==0 and n>0:\n",
    "                    print('training:', epoch + 1, 'batch', n, 'rec_loss:', rec_loss.item(), 'cl_loss', cl_loss.item())\n",
    "            with torch.no_grad():\n",
    "                self.user_emb, self.item_emb = self.model()\n",
    "            self.fast_evaluation(epoch)\n",
    "        self.user_emb, self.item_emb = self.best_user_emb, self.best_item_emb\n",
    "\n",
    "    def cal_cl_loss(self, idx):\n",
    "        u_idx = torch.unique(torch.Tensor(idx[0]).type(torch.long)).cuda()\n",
    "        i_idx = torch.unique(torch.Tensor(idx[1]).type(torch.long)).cuda()\n",
    "        user_view_1, item_view_1 = self.model(perturbed=True)\n",
    "        user_view_2, item_view_2 = self.model(perturbed=True)\n",
    "        user_cl_loss = InfoNCE(user_view_1[u_idx], user_view_2[u_idx], 0.2)\n",
    "        item_cl_loss = InfoNCE(item_view_1[i_idx], item_view_2[i_idx], 0.2)\n",
    "        return user_cl_loss + item_cl_loss\n",
    "\n",
    "    def save(self):\n",
    "        with torch.no_grad():\n",
    "            self.best_user_emb, self.best_item_emb = self.model.forward()\n",
    "\n",
    "    def predict(self, u):\n",
    "        u = self.data.get_user_id(u)\n",
    "        score = torch.matmul(self.user_emb[u], self.item_emb.transpose(0, 1))\n",
    "        return score.cpu().numpy()\n",
    "\n",
    "\n",
    "class SimGCL_Encoder(nn.Module):\n",
    "    def __init__(self, data, emb_size, eps, n_layers):\n",
    "        super(SimGCL_Encoder, self).__init__()\n",
    "        self.data = data\n",
    "        self.eps = eps\n",
    "        self.emb_size = emb_size\n",
    "        self.n_layers = n_layers\n",
    "        self.norm_adj = data.norm_adj\n",
    "        self.embedding_dict = self._init_model()\n",
    "        self.sparse_norm_adj = TorchGraphInterface.convert_sparse_mat_to_tensor(self.norm_adj).cuda()\n",
    "\n",
    "    def _init_model(self):\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(initializer(torch.empty(self.data.user_num, self.emb_size))),\n",
    "            'item_emb': nn.Parameter(initializer(torch.empty(self.data.item_num, self.emb_size))),\n",
    "        })\n",
    "        return embedding_dict\n",
    "\n",
    "    def forward(self, perturbed=False):\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'], self.embedding_dict['item_emb']], 0)\n",
    "        all_embeddings = []\n",
    "        for k in range(self.n_layers):\n",
    "            ego_embeddings = torch.sparse.mm(self.sparse_norm_adj, ego_embeddings)\n",
    "            if perturbed:\n",
    "                random_noise = torch.rand_like(ego_embeddings).cuda()\n",
    "                ego_embeddings += torch.sign(ego_embeddings) * F.normalize(random_noise, dim=-1) * self.eps\n",
    "            all_embeddings.append(ego_embeddings)\n",
    "        all_embeddings = torch.stack(all_embeddings, dim=1)\n",
    "        all_embeddings = torch.mean(all_embeddings, dim=1)\n",
    "        user_all_embeddings, item_all_embeddings = torch.split(all_embeddings, [self.data.user_num, self.data.item_num])\n",
    "        return user_all_embeddings, item_all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/viettel-ai-challenge-track-ds/SELFRec/base/torch_interface.py:13: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1730833640211/work/torch/csrc/utils/tensor_new.cpp:651.)\n",
      "  return torch.sparse.FloatTensor(i, v, coo.shape)\n"
     ]
    }
   ],
   "source": [
    "model = 'SimGCL'\n",
    "conf = ModelConf('./config.yaml')\n",
    "rec = SELFRec(conf).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SimGCL\n",
      "Training Set: /workspace/viettel-ai-challenge-track-ds/SELFRec/train.txt\n",
      "Test Set: /workspace/viettel-ai-challenge-track-ds/SELFRec/test.txt\n",
      "Embedding Dimension: 1024\n",
      "Maximum Epoch: 50\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 2048\n",
      "Regularization Parameter: 0.0001\n",
      "Specific parameters: eps:0.1  lambda:0.5  n_layer:3\n",
      "Training Set Size: (user number: 36751, item number: 82673, interaction number: 375372)\n",
      "Test Set Size: (user number: 9370, item number: 9256, interaction number: 14551)\n",
      "================================================================================\n",
      "Initializing and building model...\n",
      "Training Model...\n",
      "training: 1 batch 100 rec_loss: 0.6677468419075012 cl_loss 2.6896183490753174\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 1, Hit Ratio: 0.26118, Precision: 0.01966, Recall: 0.32661, NDCG: 0.16001\n",
      "*Best Performance*\n",
      "Epoch: 1, Hit Ratio: 0.26118, Precision: 0.01966, Recall: 0.32661, NDCG: 0.16001\n",
      "--------------------------------------------------------------------------------\n",
      "training: 2 batch 100 rec_loss: 0.5843576192855835 cl_loss 2.66092848777771\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 2, Hit Ratio: 0.25721, Precision: 0.01936, Recall: 0.32185, NDCG: 0.15605\n",
      "*Best Performance*\n",
      "Epoch: 1, Hit Ratio: 0.26118, Precision: 0.01966, Recall: 0.32661, NDCG: 0.16001\n",
      "--------------------------------------------------------------------------------\n",
      "training: 3 batch 100 rec_loss: 0.4372444748878479 cl_loss 2.644124746322632\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 3, Hit Ratio: 0.27139, Precision: 0.02043, Recall: 0.33776, NDCG: 0.1635\n",
      "*Best Performance*\n",
      "Epoch: 3, Hit Ratio: 0.27139, Precision: 0.02043, Recall: 0.33776, NDCG: 0.1635\n",
      "--------------------------------------------------------------------------------\n",
      "training: 4 batch 100 rec_loss: 0.2892249822616577 cl_loss 2.656543254852295\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 4, Hit Ratio: 0.28373, Precision: 0.02136, Recall: 0.3509, NDCG: 0.1717\n",
      "*Best Performance*\n",
      "Epoch: 4, Hit Ratio: 0.28373, Precision: 0.02136, Recall: 0.3509, NDCG: 0.1717\n",
      "--------------------------------------------------------------------------------\n",
      "training: 5 batch 100 rec_loss: 0.168147474527359 cl_loss 2.654907703399658\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 5, Hit Ratio: 0.29344, Precision: 0.02209, Recall: 0.36301, NDCG: 0.17675\n",
      "*Best Performance*\n",
      "Epoch: 5, Hit Ratio: 0.29344, Precision: 0.02209, Recall: 0.36301, NDCG: 0.17675\n",
      "--------------------------------------------------------------------------------\n",
      "training: 6 batch 100 rec_loss: 0.11177769303321838 cl_loss 2.664780616760254\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 6, Hit Ratio: 0.30138, Precision: 0.02268, Recall: 0.37122, NDCG: 0.18137\n",
      "*Best Performance*\n",
      "Epoch: 6, Hit Ratio: 0.30138, Precision: 0.02268, Recall: 0.37122, NDCG: 0.18137\n",
      "--------------------------------------------------------------------------------\n",
      "training: 7 batch 100 rec_loss: 0.07264742255210876 cl_loss 2.6479568481445312\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 7, Hit Ratio: 0.30528, Precision: 0.02298, Recall: 0.37589, NDCG: 0.18446\n",
      "*Best Performance*\n",
      "Epoch: 7, Hit Ratio: 0.30528, Precision: 0.02298, Recall: 0.37589, NDCG: 0.18446\n",
      "--------------------------------------------------------------------------------\n",
      "training: 8 batch 100 rec_loss: 0.05260667949914932 cl_loss 2.643172264099121\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 8, Hit Ratio: 0.30705, Precision: 0.02311, Recall: 0.37792, NDCG: 0.18693\n",
      "*Best Performance*\n",
      "Epoch: 8, Hit Ratio: 0.30705, Precision: 0.02311, Recall: 0.37792, NDCG: 0.18693\n",
      "--------------------------------------------------------------------------------\n",
      "training: 9 batch 100 rec_loss: 0.04316502436995506 cl_loss 2.6577680110931396\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 9, Hit Ratio: 0.30925, Precision: 0.02328, Recall: 0.38084, NDCG: 0.18951\n",
      "*Best Performance*\n",
      "Epoch: 9, Hit Ratio: 0.30925, Precision: 0.02328, Recall: 0.38084, NDCG: 0.18951\n",
      "--------------------------------------------------------------------------------\n",
      "training: 10 batch 100 rec_loss: 0.031127583235502243 cl_loss 2.6463851928710938\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 10, Hit Ratio: 0.31145, Precision: 0.02344, Recall: 0.38351, NDCG: 0.1914\n",
      "*Best Performance*\n",
      "Epoch: 10, Hit Ratio: 0.31145, Precision: 0.02344, Recall: 0.38351, NDCG: 0.1914\n",
      "--------------------------------------------------------------------------------\n",
      "training: 11 batch 100 rec_loss: 0.024739060550928116 cl_loss 2.6393446922302246\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 11, Hit Ratio: 0.31216, Precision: 0.0235, Recall: 0.38473, NDCG: 0.19254\n",
      "*Best Performance*\n",
      "Epoch: 11, Hit Ratio: 0.31216, Precision: 0.0235, Recall: 0.38473, NDCG: 0.19254\n",
      "--------------------------------------------------------------------------------\n",
      "training: 12 batch 100 rec_loss: 0.02001611515879631 cl_loss 2.6363298892974854\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 12, Hit Ratio: 0.31287, Precision: 0.02355, Recall: 0.38555, NDCG: 0.19357\n",
      "*Best Performance*\n",
      "Epoch: 12, Hit Ratio: 0.31287, Precision: 0.02355, Recall: 0.38555, NDCG: 0.19357\n",
      "--------------------------------------------------------------------------------\n",
      "training: 13 batch 100 rec_loss: 0.016601260751485825 cl_loss 2.6385273933410645\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 13, Hit Ratio: 0.31365, Precision: 0.02361, Recall: 0.38721, NDCG: 0.1943\n",
      "*Best Performance*\n",
      "Epoch: 13, Hit Ratio: 0.31365, Precision: 0.02361, Recall: 0.38721, NDCG: 0.1943\n",
      "--------------------------------------------------------------------------------\n",
      "training: 14 batch 100 rec_loss: 0.015117036178708076 cl_loss 2.627420425415039\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 15 batch 100 rec_loss: 0.013928624801337719 cl_loss 2.6303491592407227\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 15, Hit Ratio: 0.314, Precision: 0.02363, Recall: 0.38719, NDCG: 0.19431\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 16 batch 100 rec_loss: 0.011872450821101665 cl_loss 2.6296873092651367\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 16, Hit Ratio: 0.31308, Precision: 0.02356, Recall: 0.38632, NDCG: 0.19387\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 17 batch 100 rec_loss: 0.009577992372214794 cl_loss 2.6288652420043945\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 17, Hit Ratio: 0.31166, Precision: 0.02346, Recall: 0.38491, NDCG: 0.19374\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 18 batch 100 rec_loss: 0.00901417713612318 cl_loss 2.631424903869629\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 18, Hit Ratio: 0.31138, Precision: 0.02344, Recall: 0.38482, NDCG: 0.19366\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 19 batch 100 rec_loss: 0.008148693479597569 cl_loss 2.6237223148345947\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 19, Hit Ratio: 0.31117, Precision: 0.02342, Recall: 0.38469, NDCG: 0.19362\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 20 batch 100 rec_loss: 0.006398431025445461 cl_loss 2.6250810623168945\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 20, Hit Ratio: 0.30968, Precision: 0.02331, Recall: 0.38281, NDCG: 0.19283\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 21 batch 100 rec_loss: 0.006520085968077183 cl_loss 2.623847723007202\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 21, Hit Ratio: 0.30819, Precision: 0.0232, Recall: 0.3812, NDCG: 0.19261\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 22 batch 100 rec_loss: 0.005105309188365936 cl_loss 2.617417335510254\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 22, Hit Ratio: 0.3067, Precision: 0.02308, Recall: 0.37988, NDCG: 0.1916\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 23 batch 100 rec_loss: 0.00493535865098238 cl_loss 2.6193430423736572\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 23, Hit Ratio: 0.30457, Precision: 0.02292, Recall: 0.37775, NDCG: 0.19075\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 24 batch 100 rec_loss: 0.0043639205396175385 cl_loss 2.6215338706970215\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 24, Hit Ratio: 0.30294, Precision: 0.0228, Recall: 0.37583, NDCG: 0.18966\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 25 batch 100 rec_loss: 0.003655680688098073 cl_loss 2.6225616931915283\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 25, Hit Ratio: 0.30074, Precision: 0.02264, Recall: 0.37316, NDCG: 0.1884\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 26 batch 100 rec_loss: 0.003500014077872038 cl_loss 2.612621307373047\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 26, Hit Ratio: 0.30039, Precision: 0.02261, Recall: 0.37262, NDCG: 0.18784\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 27 batch 100 rec_loss: 0.0030854083597660065 cl_loss 2.6269426345825195\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 27, Hit Ratio: 0.2989, Precision: 0.0225, Recall: 0.37088, NDCG: 0.18689\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 28 batch 100 rec_loss: 0.003040851792320609 cl_loss 2.6291399002075195\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 28, Hit Ratio: 0.29777, Precision: 0.02241, Recall: 0.3692, NDCG: 0.18583\n",
      "*Best Performance*\n",
      "Epoch: 14, Hit Ratio: 0.31372, Precision: 0.02361, Recall: 0.38734, NDCG: 0.19439\n",
      "--------------------------------------------------------------------------------\n",
      "training: 29 batch 100 rec_loss: 0.002477915259078145 cl_loss 2.6144607067108154\n",
      "Evaluating the model...\n",
      "Progress: [+++++                                             ]10%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/viettel-ai-challenge-track-ds/SELFRec/base/recommender.py:79\u001b[0m, in \u001b[0;36mRecommender.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m rec_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest()\n",
      "Cell \u001b[0;32mIn[11], line 40\u001b[0m, in \u001b[0;36mSimGCL.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_emb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel()\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_emb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_user_emb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_item_emb\n",
      "File \u001b[0;32m/workspace/viettel-ai-challenge-track-ds/SELFRec/base/graph_recommender.py:83\u001b[0m, in \u001b[0;36mGraphRecommender.fast_evaluation\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfast_evaluation\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating the model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m     rec_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     measure \u001b[38;5;241m=\u001b[39m ranking_evaluation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtest_set, rec_list, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_N])\n\u001b[1;32m     86\u001b[0m     performance \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mfloat\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m measure[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m [m\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)]}\n",
      "File \u001b[0;32m/workspace/viettel-ai-challenge-track-ds/SELFRec/base/graph_recommender.py:51\u001b[0m, in \u001b[0;36mGraphRecommender.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m rated_list:\n\u001b[1;32m     50\u001b[0m     candidates[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem[item]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10e8\u001b[39m\n\u001b[0;32m---> 51\u001b[0m ids, scores \u001b[38;5;241m=\u001b[39m \u001b[43mfind_k_largest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m item_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mid2item[iid] \u001b[38;5;28;01mfor\u001b[39;00m iid \u001b[38;5;129;01min\u001b[39;00m ids]\n\u001b[1;32m     53\u001b[0m rec_list[user] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(item_names, scores))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rec.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"../runs/simgcl_final_data/simgcl.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "emb_user = dict([(k, v) for k, v in zip(rec.data.user.keys(), rec.best_user_emb.cpu().numpy())])\n",
    "with open(f\"../runs/simgcl_final_data/user_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_user, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "emb_item = dict([(k, v) for k, v in zip(rec.data.item.keys(), rec.best_item_emb.cpu().numpy())])\n",
    "with open(f\"../runs/simgcl_final_data/item_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_item, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DirectAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from YAML file: {'DirectAU': {'gamma': 2, 'n_layers': 2}, 'batch.size': 1024, 'embedding.size': 1024, 'item.ranking.topN': [10, 20], 'learning.rate': 0.001, 'max.epoch': 50, 'model': {'name': 'DirectAU', 'type': 'graph'}, 'output': './results/', 'reg.lambda': 0.0001, 'test.set': './test.txt', 'training.set': './train.txt'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"./train.txt\",\n",
    "    \"test.set\": \"./test.txt\",\n",
    "    \"model\":{\n",
    "      \"name\": \"DirectAU\",\n",
    "      \"type\": \"graph\"\n",
    "    },\n",
    "\n",
    "    \"item.ranking.topN\": [10, 20],\n",
    "\n",
    "    \"embedding.size\": 1024,\n",
    "    \"max.epoch\": 50,\n",
    "    \"batch.size\": 1024,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "\n",
    "    \"DirectAU\": {\n",
    "      \"n_layers\": 2,\n",
    "      \"gamma\": 2,\n",
    "    },\n",
    "\n",
    "    \"output\": \"./results/\"\n",
    "}\n",
    "\n",
    "# Save this dictionary as a YAML file\n",
    "yaml_file_path = \"config.yaml\"\n",
    "with open(yaml_file_path, \"w\") as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "# Load the YAML file to verify content\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    conf = yaml.safe_load(file)\n",
    "\n",
    "print(\"Loaded data from YAML file:\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import FileIO\n",
    "\n",
    "\n",
    "class SELFRec(object):\n",
    "    def __init__(self, config):\n",
    "        self.social_data = []\n",
    "        self.feature_data = []\n",
    "        self.config = config\n",
    "        self.training_data = FileIO.load_data_set(config['training.set'], config['model']['type'])\n",
    "        self.test_data = FileIO.load_data_set(config['test.set'], config['model']['type'])\n",
    "\n",
    "        self.kwargs = {}\n",
    "#         if config.contain('social.data'):\n",
    "#             social_data = FileIO.load_social_data(self.config['social.data'])\n",
    "#             self.kwargs['social.data'] = social_data\n",
    "        # if config.contains('feature.data'):\n",
    "        #     self.social_data = FileIO.loadFeature(config,self.config['feature.data'])\n",
    "        print('Reading data and preprocessing...')\n",
    "\n",
    "    def execute(self):\n",
    "        # import the model module\n",
    "#         import_str = f\"from model.{self.config['model']['type']}.{self.config['model']['name']} import {self.config['model']['name']}\"\n",
    "#         exec(import_str)\n",
    "        recommender = f\"{self.config['model']['name']}(self.config,self.training_data,self.test_data,**self.kwargs)\"\n",
    "        return eval(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from util.sampler import next_batch_pairwise\n",
    "from util.loss_torch import bpr_loss,l2_reg_loss\n",
    "from model.graph.MF import Matrix_Factorization\n",
    "from model.graph.LightGCN import LGCN_Encoder\n",
    "\n",
    "class DirectAU(GraphRecommender):\n",
    "    def __init__(self, conf, training_set, test_set):\n",
    "        super(DirectAU, self).__init__(conf, training_set, test_set)\n",
    "        args = self.config['DirectAU']\n",
    "        self.gamma = float(args['gamma'])\n",
    "        self.n_layers= int(args['n_layers'])\n",
    "        self.model = LGCN_Encoder(self.data, self.emb_size,self.n_layers)\n",
    "\n",
    "    def train(self):\n",
    "        model = self.model.cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lRate)\n",
    "        for epoch in range(self.maxEpoch):\n",
    "            for n, batch in enumerate(next_batch_pairwise(self.data, self.batch_size)):\n",
    "                user_idx, pos_idx, neg_idx = batch\n",
    "                rec_user_emb, rec_item_emb = model()\n",
    "                user_emb, pos_item_emb = rec_user_emb[user_idx], rec_item_emb[pos_idx]\n",
    "                batch_loss = self.calculate_loss(user_emb, pos_item_emb)+ l2_reg_loss(self.reg, user_emb,pos_item_emb)/self.batch_size\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                if n % 100==0 and n>0:\n",
    "                    print('training:', epoch + 1, 'batch', n, 'batch_loss:', batch_loss.item())\n",
    "            with torch.no_grad():\n",
    "                self.user_emb, self.item_emb = self.model()\n",
    "            self.fast_evaluation(epoch)\n",
    "        self.user_emb, self.item_emb = self.best_user_emb, self.best_item_emb\n",
    "\n",
    "    def alignment(self,x, y):\n",
    "        x, y = F.normalize(x, dim=-1), F.normalize(y, dim=-1)\n",
    "        return (x - y).norm(p=2, dim=1).pow(2).mean()\n",
    "\n",
    "    def uniformity(self,x, t=2):\n",
    "        x = F.normalize(x, dim=-1)\n",
    "        return torch.pdist(x, p=2).pow(2).mul(-t).exp().mean().log()\n",
    "\n",
    "    def calculate_loss(self,user_emb,item_emb):\n",
    "        align = self.alignment(user_emb, item_emb)\n",
    "        uniform = self.gamma * (self.uniformity(user_emb) + self.uniformity(item_emb)) / 2\n",
    "        return align + uniform\n",
    "\n",
    "    def save(self):\n",
    "        with torch.no_grad():\n",
    "            self.best_user_emb, self.best_item_emb = self.model.forward()\n",
    "\n",
    "    def predict(self, u):\n",
    "        with torch.no_grad():\n",
    "            u = self.data.get_user_id(u)\n",
    "            score = torch.matmul(self.user_emb[u], self.item_emb.transpose(0, 1))\n",
    "            return score.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and preprocessing...\n"
     ]
    }
   ],
   "source": [
    "model = 'DirectAU'\n",
    "conf = ModelConf('./config.yaml')\n",
    "rec = SELFRec(conf).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DirectAU\n",
      "Training Set: /workspace/viettel-ai-challenge-track-ds/SELFRec/train.txt\n",
      "Test Set: /workspace/viettel-ai-challenge-track-ds/SELFRec/test.txt\n",
      "Embedding Dimension: 1024\n",
      "Maximum Epoch: 50\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 1024\n",
      "Regularization Parameter: 0.0001\n",
      "Specific parameters: gamma:2  n_layers:2\n",
      "Training Set Size: (user number: 36751, item number: 82673, interaction number: 375372)\n",
      "Test Set Size: (user number: 9370, item number: 9256, interaction number: 14551)\n",
      "================================================================================\n",
      "Initializing and building model...\n",
      "Training Model...\n",
      "training: 1 batch 100 batch_loss: -7.018003940582275\n",
      "training: 1 batch 200 batch_loss: -7.028926849365234\n",
      "training: 1 batch 300 batch_loss: -7.029053688049316\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 1, Hit Ratio: 0.25721, Precision: 0.01936, Recall: 0.31872, NDCG: 0.14983\n",
      "*Best Performance*\n",
      "Epoch: 1, Hit Ratio: 0.25721, Precision: 0.01936, Recall: 0.31872, NDCG: 0.14983\n",
      "--------------------------------------------------------------------------------\n",
      "training: 2 batch 100 batch_loss: -7.224115371704102\n",
      "training: 2 batch 200 batch_loss: -7.156414985656738\n",
      "training: 2 batch 300 batch_loss: -7.161381721496582\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 2, Hit Ratio: 0.26962, Precision: 0.02029, Recall: 0.3347, NDCG: 0.15748\n",
      "*Best Performance*\n",
      "Epoch: 2, Hit Ratio: 0.26962, Precision: 0.02029, Recall: 0.3347, NDCG: 0.15748\n",
      "--------------------------------------------------------------------------------\n",
      "training: 3 batch 100 batch_loss: -7.241375923156738\n",
      "training: 3 batch 200 batch_loss: -7.223694324493408\n",
      "training: 3 batch 300 batch_loss: -7.177080154418945\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 3, Hit Ratio: 0.27061, Precision: 0.02037, Recall: 0.33591, NDCG: 0.15923\n",
      "*Best Performance*\n",
      "Epoch: 3, Hit Ratio: 0.27061, Precision: 0.02037, Recall: 0.33591, NDCG: 0.15923\n",
      "--------------------------------------------------------------------------------\n",
      "training: 4 batch 100 batch_loss: -7.24445104598999\n",
      "training: 4 batch 200 batch_loss: -7.227782726287842\n",
      "training: 4 batch 300 batch_loss: -7.217264652252197\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 4, Hit Ratio: 0.27324, Precision: 0.02057, Recall: 0.33899, NDCG: 0.16172\n",
      "*Best Performance*\n",
      "Epoch: 4, Hit Ratio: 0.27324, Precision: 0.02057, Recall: 0.33899, NDCG: 0.16172\n",
      "--------------------------------------------------------------------------------\n",
      "training: 5 batch 100 batch_loss: -7.300293922424316\n",
      "training: 5 batch 200 batch_loss: -7.22706413269043\n",
      "training: 5 batch 300 batch_loss: -7.198413372039795\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 5, Hit Ratio: 0.27579, Precision: 0.02076, Recall: 0.34186, NDCG: 0.16258\n",
      "*Best Performance*\n",
      "Epoch: 5, Hit Ratio: 0.27579, Precision: 0.02076, Recall: 0.34186, NDCG: 0.16258\n",
      "--------------------------------------------------------------------------------\n",
      "training: 6 batch 100 batch_loss: -7.242102146148682\n",
      "training: 6 batch 200 batch_loss: -7.224437713623047\n",
      "training: 6 batch 300 batch_loss: -7.198686599731445\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 6, Hit Ratio: 0.27593, Precision: 0.02077, Recall: 0.34258, NDCG: 0.16364\n",
      "*Best Performance*\n",
      "Epoch: 6, Hit Ratio: 0.27593, Precision: 0.02077, Recall: 0.34258, NDCG: 0.16364\n",
      "--------------------------------------------------------------------------------\n",
      "training: 7 batch 100 batch_loss: -7.276487827301025\n",
      "training: 7 batch 200 batch_loss: -7.2624006271362305\n",
      "training: 7 batch 300 batch_loss: -7.18607234954834\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 7, Hit Ratio: 0.27629, Precision: 0.0208, Recall: 0.34288, NDCG: 0.16442\n",
      "*Best Performance*\n",
      "Epoch: 7, Hit Ratio: 0.27629, Precision: 0.0208, Recall: 0.34288, NDCG: 0.16442\n",
      "--------------------------------------------------------------------------------\n",
      "training: 8 batch 100 batch_loss: -7.285957336425781\n",
      "training: 8 batch 200 batch_loss: -7.256405830383301\n",
      "training: 8 batch 300 batch_loss: -7.222184658050537\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 8, Hit Ratio: 0.27827, Precision: 0.02094, Recall: 0.34583, NDCG: 0.16521\n",
      "*Best Performance*\n",
      "Epoch: 8, Hit Ratio: 0.27827, Precision: 0.02094, Recall: 0.34583, NDCG: 0.16521\n",
      "--------------------------------------------------------------------------------\n",
      "training: 9 batch 100 batch_loss: -7.269237041473389\n",
      "training: 9 batch 200 batch_loss: -7.254229545593262\n",
      "training: 9 batch 300 batch_loss: -7.231378555297852\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 9, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34838, NDCG: 0.16605\n",
      "*Best Performance*\n",
      "Epoch: 9, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34838, NDCG: 0.16605\n",
      "--------------------------------------------------------------------------------\n",
      "training: 10 batch 100 batch_loss: -7.288190841674805\n",
      "training: 10 batch 200 batch_loss: -7.249028205871582\n",
      "training: 10 batch 300 batch_loss: -7.244559288024902\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 10, Hit Ratio: 0.2799, Precision: 0.02107, Recall: 0.34755, NDCG: 0.16603\n",
      "*Best Performance*\n",
      "Epoch: 9, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34838, NDCG: 0.16605\n",
      "--------------------------------------------------------------------------------\n",
      "training: 11 batch 100 batch_loss: -7.267651081085205\n",
      "training: 11 batch 200 batch_loss: -7.262054443359375\n",
      "training: 11 batch 300 batch_loss: -7.226958751678467\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 11, Hit Ratio: 0.27884, Precision: 0.02099, Recall: 0.34628, NDCG: 0.16581\n",
      "*Best Performance*\n",
      "Epoch: 9, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34838, NDCG: 0.16605\n",
      "--------------------------------------------------------------------------------\n",
      "training: 12 batch 100 batch_loss: -7.290872573852539\n",
      "training: 12 batch 200 batch_loss: -7.2724714279174805\n",
      "training: 12 batch 300 batch_loss: -7.23700475692749\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 12, Hit Ratio: 0.27862, Precision: 0.02097, Recall: 0.34655, NDCG: 0.16645\n",
      "*Best Performance*\n",
      "Epoch: 9, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34838, NDCG: 0.16605\n",
      "--------------------------------------------------------------------------------\n",
      "training: 13 batch 100 batch_loss: -7.318774223327637\n",
      "training: 13 batch 200 batch_loss: -7.2671661376953125\n",
      "training: 13 batch 300 batch_loss: -7.272461891174316\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 13, Hit Ratio: 0.2787, Precision: 0.02098, Recall: 0.34605, NDCG: 0.16614\n",
      "*Best Performance*\n",
      "Epoch: 9, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34838, NDCG: 0.16605\n",
      "--------------------------------------------------------------------------------\n",
      "training: 14 batch 100 batch_loss: -7.270336151123047\n",
      "training: 14 batch 200 batch_loss: -7.244748115539551\n",
      "training: 14 batch 300 batch_loss: -7.208944320678711\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 14, Hit Ratio: 0.28018, Precision: 0.02109, Recall: 0.34778, NDCG: 0.16616\n",
      "*Best Performance*\n",
      "Epoch: 9, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34838, NDCG: 0.16605\n",
      "--------------------------------------------------------------------------------\n",
      "training: 15 batch 100 batch_loss: -7.284781455993652\n",
      "training: 15 batch 200 batch_loss: -7.255289554595947\n",
      "training: 15 batch 300 batch_loss: -7.242146015167236\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 15, Hit Ratio: 0.27855, Precision: 0.02097, Recall: 0.34597, NDCG: 0.16645\n",
      "*Best Performance*\n",
      "Epoch: 9, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34838, NDCG: 0.16605\n",
      "--------------------------------------------------------------------------------\n",
      "training: 16 batch 100 batch_loss: -7.2871832847595215\n",
      "training: 16 batch 200 batch_loss: -7.243527412414551\n",
      "training: 16 batch 300 batch_loss: -7.2360053062438965\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 16, Hit Ratio: 0.27983, Precision: 0.02106, Recall: 0.34838, NDCG: 0.16677\n",
      "*Best Performance*\n",
      "Epoch: 9, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34838, NDCG: 0.16605\n",
      "--------------------------------------------------------------------------------\n",
      "training: 17 batch 100 batch_loss: -7.270610809326172\n",
      "training: 17 batch 200 batch_loss: -7.304197788238525\n",
      "training: 17 batch 300 batch_loss: -7.2354512214660645\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 17, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34922, NDCG: 0.16769\n",
      "*Best Performance*\n",
      "Epoch: 17, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34922, NDCG: 0.16769\n",
      "--------------------------------------------------------------------------------\n",
      "training: 18 batch 100 batch_loss: -7.277872562408447\n",
      "training: 18 batch 200 batch_loss: -7.268777370452881\n",
      "training: 18 batch 300 batch_loss: -7.276451587677002\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 18, Hit Ratio: 0.28054, Precision: 0.02112, Recall: 0.3484, NDCG: 0.16769\n",
      "*Best Performance*\n",
      "Epoch: 17, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34922, NDCG: 0.16769\n",
      "--------------------------------------------------------------------------------\n",
      "training: 19 batch 100 batch_loss: -7.2962164878845215\n",
      "training: 19 batch 200 batch_loss: -7.269756317138672\n",
      "training: 19 batch 300 batch_loss: -7.229708194732666\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 19, Hit Ratio: 0.28054, Precision: 0.02112, Recall: 0.34836, NDCG: 0.16738\n",
      "*Best Performance*\n",
      "Epoch: 17, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34922, NDCG: 0.16769\n",
      "--------------------------------------------------------------------------------\n",
      "training: 20 batch 100 batch_loss: -7.278594970703125\n",
      "training: 20 batch 200 batch_loss: -7.286532878875732\n",
      "training: 20 batch 300 batch_loss: -7.24346923828125\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 20, Hit Ratio: 0.2799, Precision: 0.02107, Recall: 0.34808, NDCG: 0.16709\n",
      "*Best Performance*\n",
      "Epoch: 17, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34922, NDCG: 0.16769\n",
      "--------------------------------------------------------------------------------\n",
      "training: 21 batch 100 batch_loss: -7.267369270324707\n",
      "training: 21 batch 200 batch_loss: -7.272661209106445\n",
      "training: 21 batch 300 batch_loss: -7.2409539222717285\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 21, Hit Ratio: 0.27919, Precision: 0.02101, Recall: 0.34739, NDCG: 0.16746\n",
      "*Best Performance*\n",
      "Epoch: 17, Hit Ratio: 0.28068, Precision: 0.02113, Recall: 0.34922, NDCG: 0.16769\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rec.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"../runs/directau_private/model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "emb_user = dict([(k, v) for k, v in zip(rec.data.user.keys(), rec.best_user_emb.cpu().numpy())])\n",
    "with open(f\"../runs/directau_private/user_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_user, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "emb_item = dict([(k, v) for k, v in zip(rec.data.item.keys(), rec.best_item_emb.cpu().numpy())])\n",
    "with open(f\"../runs/directau_private/item_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_item, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5965973,
     "sourceId": 9839124,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6047721,
     "sourceId": 9860087,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "rapidsai",
   "language": "python",
   "name": "rapidsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
