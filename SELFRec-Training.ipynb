{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd SELFRec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from data.loader import FileIO\n",
    "from util.conf import ModelConf\n",
    "from util.sampler import next_batch_pairwise\n",
    "from util.loss_torch import bpr_loss, l2_reg_loss, InfoNCE\n",
    "\n",
    "from model.graph.LightGCN import *\n",
    "from model.graph.XSimGCL import *\n",
    "from model.graph.DirectAU import *\n",
    "from model.graph.SimGCL import *\n",
    "from SELFRec import SELFRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config(out_path):\n",
    "    # Save this dictionary as a YAML file\n",
    "    yaml_file_path = f\"{out_path}/config.yaml\"\n",
    "    with open(yaml_file_path, \"w\") as file:\n",
    "        yaml.dump(data, file, default_flow_style=False)\n",
    "    \n",
    "    # Load the YAML file to verify content\n",
    "    with open(yaml_file_path, \"r\") as file:\n",
    "        conf = yaml.safe_load(file)\n",
    "    \n",
    "    print(\"Loaded data from YAML file:\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path=\"../runs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/training_set.csv')\n",
    "predict_df = pd.read_csv('../data/public_testset.csv', names=['user_id'] + [f'item_id_{i}' for i in range(1,1001)])\n",
    "\n",
    "test_user_id = predict_df['user_id'].values\n",
    "\n",
    "item_columns = predict_df.columns[1:]  # Lấy tất cả cột trừ cột uid\n",
    "item_in_test_df = pd.unique(predict_df[item_columns].values.ravel())\n",
    "#Lọc bỏ user_id bị lẫn trong tập test\n",
    "user_list = df.UserId.unique()\n",
    "item_in_test_df = list(set(item_in_test_df).difference(set(user_list)))\n",
    "\n",
    "test_df = df[~df.ItemId.isin(item_in_test_df)].sample(frac=0.05, random_state=42)  # 90% for train\n",
    "# train_df = df\n",
    "train_df = df.drop(test_df.index).groupby('UserId').tail(20)  # Remaining 10% for test\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train and test sets as .txt files without headers, separated by a space\n",
    "train_df.to_csv(\"../data/train.txt\", index=False, header=False, sep=\" \")\n",
    "test_df.to_csv(\"../data/test.txt\", index=False, header=False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XSimGCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"XSimGCL\"\n",
    "model_path=f\"{checkpoint_path}/{model}\"\n",
    "os.makedirs(checkpoint_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"../data/train.txt\",\n",
    "    \"test.set\": \"../data/test.txt\",\n",
    "    \"model\": {\n",
    "        \"name\": \"XSimGCL\",\n",
    "        \"type\": \"graph\"\n",
    "    },\n",
    "    \"item.ranking.topN\": [10, 20],\n",
    "    \"embedding.size\": 1024,\n",
    "    \"max.epoch\": 1,\n",
    "    \"batch.size\": 2048,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "    \"XSimGCL\": {\n",
    "        \"n_layer\": 2,\n",
    "        \"l_star\": 1,\n",
    "        \"lambda\": 0.2,\n",
    "        \"eps\": 0.2,\n",
    "        \"tau\": 0.15\n",
    "    },\n",
    "    \"output\": model_path\n",
    "}\n",
    "\n",
    "save_config(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = ModelConf(f'{model_path}/config.yaml')\n",
    "rec = SELFRec(conf)\n",
    "rec.execute()\n",
    "rec.save()\n",
    "\n",
    "import pickle\n",
    "with open(f\"{model_path}/model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rec, f)\n",
    "\n",
    "emb_user = dict([(k, v) for k, v in zip(rec.data.user.keys(), rec.best_user_emb.cpu().numpy())])\n",
    "with open(f\"{model_path}/user_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_user, f)\n",
    "\n",
    "emb_item = dict([(k, v) for k, v in zip(rec.data.item.keys(), rec.best_item_emb.cpu().numpy())])\n",
    "with open(f\"{model_path}/item_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_item, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"LightGCN\"\n",
    "model_path=f\"{checkpoint_path}/{model}\"\n",
    "os.makedirs(checkpoint_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"../data/train.txt\",\n",
    "    \"test.set\": \"../data/test.txt\",\n",
    "    \"model\": {\n",
    "        \"name\": \"LightGCN\",\n",
    "        \"type\": \"graph\"\n",
    "    },\n",
    "    \"item.ranking.topN\": [10, 20],\n",
    "    \"embedding.size\": 1024,\n",
    "    \"max.epoch\": 100,\n",
    "    \"batch.size\": 2048,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "    \"LightGCN\": {\n",
    "        \"n_layer\": 2\n",
    "    },\n",
    "    \"output\": checkpoint_path\n",
    "}\n",
    "\n",
    "save_config(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = ModelConf(f'{model_path}/config.yaml')\n",
    "rec = SELFRec(conf)\n",
    "rec.execute()\n",
    "rec.save()\n",
    "\n",
    "import pickle\n",
    "with open(f\"{model_path}/model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rec, f)\n",
    "\n",
    "emb_user = dict([(k, v) for k, v in zip(rec.data.user.keys(), rec.best_user_emb.cpu().numpy())])\n",
    "with open(f\"{model_path}/user_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_user, f)\n",
    "\n",
    "emb_item = dict([(k, v) for k, v in zip(rec.data.item.keys(), rec.best_item_emb.cpu().numpy())])\n",
    "with open(f\"{model_path}/item_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_item, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"/kaggle/working/sample/train.txt\",\n",
    "    \"test.set\": \"/kaggle/working/sample/test.txt\",\n",
    "    \"model\":{\n",
    "      \"name\": \"SSL4Rec\",\n",
    "      \"type\": \"graph\"\n",
    "    },\n",
    "\n",
    "    \"item.ranking.topN\": [20],\n",
    "\n",
    "    \"embedding.size\": 256,\n",
    "    \"max.epoch\": 100,\n",
    "    \"batch.size\": 2048,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "\n",
    "    \"SSL4Rec\":{\n",
    "      \"tau\": 0.07,\n",
    "      \"alpha\": 0.1,\n",
    "      \"drop\": 0.1\n",
    "    },\n",
    "\n",
    "    \"output\": \"./results/\"\n",
    "}\n",
    "\n",
    "# Save this dictionary as a YAML file\n",
    "yaml_file_path = \"config.yaml\"\n",
    "with open(yaml_file_path, \"w\") as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "# Load the YAML file to verify content\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    conf = yaml.safe_load(file)\n",
    "\n",
    "print(\"Loaded data from YAML file:\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import FileIO\n",
    "\n",
    "\n",
    "class SELFRec(object):\n",
    "    def __init__(self, config):\n",
    "        self.social_data = []\n",
    "        self.feature_data = []\n",
    "        self.config = config\n",
    "        self.training_data = FileIO.load_data_set(config['training.set'], config['model']['type'])\n",
    "        self.test_data = FileIO.load_data_set(config['test.set'], config['model']['type'])\n",
    "\n",
    "        self.kwargs = {}\n",
    "#         if config.contain('social.data'):\n",
    "#             social_data = FileIO.load_social_data(self.config['social.data'])\n",
    "#             self.kwargs['social.data'] = social_data\n",
    "        # if config.contains('feature.data'):\n",
    "        #     self.social_data = FileIO.loadFeature(config,self.config['feature.data'])\n",
    "        print('Reading data and preprocessing...')\n",
    "\n",
    "    def execute(self):\n",
    "        # import the model module\n",
    "#         import_str = f\"from model.{self.config['model']['type']}.{self.config['model']['name']} import {self.config['model']['name']}\"\n",
    "#         exec(import_str)\n",
    "        recommender = f\"{self.config['model']['name']}(self.config,self.training_data,self.test_data,**self.kwargs)\"\n",
    "        return eval(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from util.sampler import next_batch_pairwise\n",
    "from util.loss_torch import l2_reg_loss, InfoNCE, batch_softmax_loss\n",
    "\n",
    "# Paper: Self-supervised Learning for Large-scale Item Recommendations. CIKM'21\n",
    "\n",
    "\"\"\" \n",
    "Note: This version of code conducts feature dropout on the item embeddings \n",
    "because items features are not always available in many academic datasets.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SSL4Rec(GraphRecommender):\n",
    "    def __init__(self, conf, training_set, test_set):\n",
    "        super(SSL4Rec, self).__init__(conf, training_set, test_set)\n",
    "        args = self.config['SSL4Rec']\n",
    "        self.cl_rate = float(args['alpha'])\n",
    "        self.tau = float(args['tau'])\n",
    "        self.drop_rate = float(args['drop'])\n",
    "        self.model = DNN_Encoder(self.data, self.emb_size, self.drop_rate, self.tau)\n",
    "\n",
    "    def train(self):\n",
    "        model = self.model.cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lRate)\n",
    "        for epoch in range(self.maxEpoch):\n",
    "            for n, batch in enumerate(next_batch_pairwise(self.data, self.batch_size)):\n",
    "                query_idx, item_idx, _neg = batch\n",
    "                model.train()\n",
    "                query_emb, item_emb = model(query_idx, item_idx)\n",
    "                rec_loss = batch_softmax_loss(query_emb, item_emb, self.tau)\n",
    "                cl_loss = self.cl_rate * model.cal_cl_loss(item_idx)\n",
    "                batch_loss = rec_loss + l2_reg_loss(self.reg, query_emb, item_emb) + cl_loss\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                if n % 100 == 0:\n",
    "                    print('training:', epoch + 1, 'batch', n, 'rec_loss:', rec_loss.item(), 'cl_loss', cl_loss.item())\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                self.query_emb, self.item_emb = self.model(list(range(self.data.user_num)),list(range(self.data.item_num)))\n",
    "            self.fast_evaluation(epoch)\n",
    "        self.query_emb, self.item_emb = self.best_query_emb, self.best_item_emb\n",
    "\n",
    "    def save(self):\n",
    "        with torch.no_grad():\n",
    "            self.best_query_emb, self.best_item_emb = self.model.forward(list(range(self.data.user_num)),list(range(self.data.item_num)))\n",
    "\n",
    "    def predict(self, u):\n",
    "        u = self.data.get_user_id(u)\n",
    "        score = torch.matmul(self.query_emb[u], self.item_emb.transpose(0, 1))\n",
    "        return score.cpu().numpy()\n",
    "\n",
    "\n",
    "class DNN_Encoder(nn.Module):\n",
    "    def __init__(self, data, emb_size, drop_rate, temperature):\n",
    "        super(DNN_Encoder, self).__init__()\n",
    "        self.data = data\n",
    "        self.emb_size = emb_size\n",
    "        self.tau = temperature\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(self.emb_size, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(self.emb_size, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        self.initial_user_emb = nn.Parameter(initializer(torch.empty(self.data.user_num, self.emb_size)))\n",
    "        self.initial_item_emb = nn.Parameter(initializer(torch.empty(self.data.item_num, self.emb_size)))\n",
    "\n",
    "    def forward(self, q, x):\n",
    "        q_emb = self.initial_user_emb[q]\n",
    "        i_emb = self.initial_item_emb[x]\n",
    "\n",
    "        q_emb = self.user_tower(q_emb)\n",
    "        i_emb = self.item_tower(i_emb)\n",
    "\n",
    "        return q_emb, i_emb\n",
    "\n",
    "    def item_encoding(self, x):\n",
    "        i_emb = self.initial_item_emb[x]\n",
    "        i1_emb = self.dropout(i_emb)\n",
    "        i2_emb = self.dropout(i_emb)\n",
    "\n",
    "        i1_emb = self.item_tower(i1_emb)\n",
    "        i2_emb = self.item_tower(i2_emb)\n",
    "\n",
    "        return i1_emb, i2_emb\n",
    "\n",
    "    def cal_cl_loss(self, idx):\n",
    "        item_view1, item_view_2 = self.item_encoding(idx)       \n",
    "        cl_loss = InfoNCE(item_view1, item_view_2, self.tau)\n",
    "        return cl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'SSL4Rec'\n",
    "conf = ModelConf('/kaggle/working/sample/config.yaml')\n",
    "rec = SELFRec(conf).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimGCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"SimGCL\"\n",
    "model_path=f\"{checkpoint_path}/{model}\"\n",
    "os.makedirs(checkpoint_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"./train.txt\",\n",
    "    \"test.set\": \"./test.txt\",\n",
    "    \"model\":{\n",
    "      \"name\": \"SimGCL\",\n",
    "      \"type\": \"graph\"\n",
    "    },\n",
    "\n",
    "    \"item.ranking.topN\": [10, 20],\n",
    "\n",
    "    \"embedding.size\": 1024,\n",
    "    \"max.epoch\": 50,\n",
    "    \"batch.size\": 2048,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "\n",
    "    \"SimGCL\": {\n",
    "      \"n_layer\": 3,\n",
    "      \"lambda\": 0.5,\n",
    "      \"eps\": 0.1\n",
    "    },\n",
    "\n",
    "    \"output\": model_path\n",
    "}\n",
    "\n",
    "save_config(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = ModelConf(f'{model_path}/config.yaml')\n",
    "rec = SELFRec(conf)\n",
    "rec.execute()\n",
    "rec.save()\n",
    "\n",
    "import pickle\n",
    "with open(f\"{model_path}/model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rec, f)\n",
    "\n",
    "emb_user = dict([(k, v) for k, v in zip(rec.data.user.keys(), rec.best_user_emb.cpu().numpy())])\n",
    "with open(f\"{model_path}/user_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_user, f)\n",
    "\n",
    "emb_item = dict([(k, v) for k, v in zip(rec.data.item.keys(), rec.best_item_emb.cpu().numpy())])\n",
    "with open(f\"{model_path}/item_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_item, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DirectAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"DirectAU\"\n",
    "model_path=f\"{checkpoint_path}/{model}\"\n",
    "os.makedirs(checkpoint_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the data as a Python dictionary\n",
    "data = {\n",
    "    \"training.set\": \"./train.txt\",\n",
    "    \"test.set\": \"./test.txt\",\n",
    "    \"model\":{\n",
    "      \"name\": \"DirectAU\",\n",
    "      \"type\": \"graph\"\n",
    "    },\n",
    "\n",
    "    \"item.ranking.topN\": [10, 20],\n",
    "\n",
    "    \"embedding.size\": 1024,\n",
    "    \"max.epoch\": 50,\n",
    "    \"batch.size\": 1024,\n",
    "    \"learning.rate\": 0.001,\n",
    "    \"reg.lambda\": 0.0001,\n",
    "\n",
    "    \"DirectAU\": {\n",
    "      \"n_layers\": 2,\n",
    "      \"gamma\": 2,\n",
    "    },\n",
    "\n",
    "    \"output\": model_path\n",
    "}\n",
    "\n",
    "save_config(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = ModelConf(f'{model_path}/config.yaml')\n",
    "rec = SELFRec(conf)\n",
    "rec.execute()\n",
    "rec.save()\n",
    "\n",
    "import pickle\n",
    "with open(f\"{model_path}/model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rec, f)\n",
    "\n",
    "emb_user = dict([(k, v) for k, v in zip(rec.data.user.keys(), rec.best_user_emb.cpu().numpy())])\n",
    "with open(f\"{model_path}/user_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_user, f)\n",
    "\n",
    "emb_item = dict([(k, v) for k, v in zip(rec.data.item.keys(), rec.best_item_emb.cpu().numpy())])\n",
    "with open(f\"{model_path}/item_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emb_item, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5965973,
     "sourceId": 9839124,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6047721,
     "sourceId": 9860087,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "rapidsai",
   "language": "python",
   "name": "rapidsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
