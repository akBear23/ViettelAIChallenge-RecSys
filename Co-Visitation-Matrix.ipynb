{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005198,
     "end_time": "2022-11-10T16:03:20.966987",
     "exception": false,
     "start_time": "2022-11-10T16:03:20.961789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Candidate ReRank Model using Handcrafted Rules\n",
    "In this notebook, we present a \"candidate rerank\" model using handcrafted rules. We can improve this model by engineering features, merging them unto items and users, and training a reranker model (such as XGB) to choose our final 20. Furthermore to tune and improve this notebook, we should build a local CV scheme to experiment new logic and/or models.\n",
    "\n",
    "UPDATE: I published a notebook to compute validation score [here][10] using Radek's scheme described [here][11].\n",
    "\n",
    "Note in this competition, a \"session\" actually means a unique \"user\". So our task is to predict what each of the `1,671,803` test \"users\" (i.e. \"sessions\") will do in the future. For each test \"user\" (i.e. \"session\") we must predict what they will `click`, `cart`, and `order` during the remainder of the week long test period.\n",
    "\n",
    "### Step 1 - Generate Candidates\n",
    "For each test user, we generate possible choices, i.e. candidates. In this notebook, we generate candidates from 5 sources:\n",
    "* User history of clicks, carts, orders\n",
    "* Most popular 20 clicks, carts, orders during test week\n",
    "* Co-visitation matrix of click/cart/order to cart/order with type weighting\n",
    "* Co-visitation matrix of cart/order to cart/order called buy2buy\n",
    "* Co-visitation matrix of click/cart/order to clicks with time weighting\n",
    "\n",
    "### Step 2 - ReRank and Choose 20\n",
    "Given the list of candidates, we must select 20 to be our predictions. In this notebook, we do this with a set of handcrafted rules. We can improve our predictions by training an XGBoost model to select for us. Our handcrafted rules give priority to:\n",
    "* Most recent previously visited items\n",
    "* Items previously visited multiple times\n",
    "* Items previously in cart or order\n",
    "* Co-visitation matrix of cart/order to cart/order\n",
    "* Current popular items\n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Nov-2022/c_r_model.png)\n",
    "  \n",
    "# Credits\n",
    "We thank many Kagglers who have shared ideas. We use co-visitation matrix idea from Vladimir [here][1]. We use groupby sort logic from Sinan in comment section [here][4]. We use duplicate prediction removal logic from Radek [here][5]. We use multiple visit logic from Pietro [here][2]. We use type weighting logic from Ingvaras [here][3]. We use leaky test data from my previous notebook [here][4]. And some ideas may have originated from Tawara [here][6] and KJ [here][7]. We use Colum2131's parquets [here][8]. Above image is from Ravi's discussion about candidate rerank models [here][9]\n",
    "\n",
    "[1]: https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix\n",
    "[2]: https://www.kaggle.com/code/pietromaldini1/multiple-clicks-vs-latest-items\n",
    "[3]: https://www.kaggle.com/code/ingvarasgalinskas/item-type-vs-multiple-clicks-vs-latest-items\n",
    "[4]: https://www.kaggle.com/code/cdeotte/test-data-leak-lb-boost\n",
    "[5]: https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic\n",
    "[6]: https://www.kaggle.com/code/ttahara/otto-mors-aid-frequency-baseline\n",
    "[7]: https://www.kaggle.com/code/whitelily/co-occurrence-baseline\n",
    "[8]: https://www.kaggle.com/datasets/columbia2131/otto-chunk-data-inparquet-format\n",
    "[9]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364721\n",
    "[10]: https://www.kaggle.com/cdeotte/compute-validation-score-cv-564\n",
    "[11]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "Below are notes about versions:\n",
    "* **Version 1 LB 0.573** Uses popular ideas from public notebooks and adds additional co-visitation matrices and additional logic. Has CV `0.563`. See validation notebook version 2 [here][1].\n",
    "* **Version 2 LB 573** Refactor logic for `suggest_buys(df)` to make it clear how new co-visitation matrices are reranking the candidates by adding to candidate weights. Also new logic boosts CV by `+0.0003`. Also LB is slightly better too. See validation notebook version 3 [here][1]\n",
    "* **Version 3** is the same as version 2 but 1.5x faster co-visitation matrix computation!\n",
    "* **Version 4 LB 575** Use top20 for clicks and top15 for carts and buys (instead of top40 and top40). This boosts CV `+0.0015` hooray! New CV is `0.5647`. See validation version 5 [here][1]\n",
    "* **Version 5** is the same as version 4 but 2x faster co-visitation matrix computation! (and 3x faster than version 1)\n",
    "* **Version 6** Stay tuned for more versions...\n",
    "\n",
    "[1]: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-564"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00373,
     "end_time": "2022-11-10T16:03:20.9748",
     "exception": false,
     "start_time": "2022-11-10T16:03:20.97107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 1 - Candidate Generation with RAPIDS\n",
    "For candidate generation, we build three co-visitation matrices. One computes the popularity of cart/order given a user's previous click/cart/order. We apply type weighting to this matrix. One computes the popularity of cart/order given a user's previous cart/order. We call this \"buy2buy\" matrix. One computes the popularity of clicks given a user previously click/cart/order.  We apply time weighting to this matrix. We will use RAPIDS cuDF GPU to compute these matrices quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = \"runs/co-visitation-matrix\"\n",
    "import os \n",
    "os.makedirs(outpath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.036143,
     "end_time": "2022-11-10T16:03:24.014816",
     "exception": false,
     "start_time": "2022-11-10T16:03:20.978673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 5\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "# import cudf, itertools\n",
    "# print('We will use RAPIDS version',cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00424,
     "end_time": "2022-11-10T16:03:24.023816",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.019576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute Three Co-visitation Matrices with RAPIDS\n",
    "We will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n",
    "* Use RAPIDS cuDF GPU instead of Pandas CPU\n",
    "* Read disk once and save in CPU RAM for later GPU multiple use\n",
    "* Process largest amount of data possible on GPU at one time\n",
    "* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n",
    "* Write result as parquet instead of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.063943,
     "end_time": "2022-11-10T16:03:24.091816",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.027873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/training_set.csv')\n",
    "\n",
    "user_list = df.groupby('UserId')['ItemId'].nunique()\n",
    "user_list_denoise = user_list[(user_list<=20) & (user_list>=3)].index.to_list()\n",
    "df = df[df.UserId.isin(user_list_denoise)]\n",
    "\n",
    "df['type'] = df['Purchase']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004089,
     "end_time": "2022-11-10T16:03:24.100502",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.096413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE PAIRS\n",
    "df = df.merge(df,on='UserId')\n",
    "df = df.loc[(df.ItemId_x != df.ItemId_y) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_weight = {0:1, 1:5}\n",
    "\n",
    "# ASSIGN WEIGHTS\n",
    "df = df[['UserId', 'ItemId_x', 'ItemId_y','type_y']].drop_duplicates(['UserId', 'ItemId_x', 'ItemId_y'])\n",
    "df['wgt'] = df.type_y.map(type_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['ItemId_x', 'ItemId_y','wgt']]\n",
    "df.wgt = df.wgt.astype('float32')\n",
    "df = df.groupby(['ItemId_x', 'ItemId_y']).wgt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 566.561189,
     "end_time": "2022-11-10T16:12:50.666123",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.104934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONVERT MATRIX TO DICTIONARY\n",
    "df = df.reset_index()\n",
    "df = df.sort_values(['ItemId_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 40\n",
    "df = df.reset_index(drop=True)\n",
    "df['n'] = df.groupby('ItemId_x').ItemId_y.cumcount()\n",
    "df = df.loc[df.n<15].drop('n',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_order = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_order.to_csv(f'{outpath}/cart_order_denoise.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03219,
     "end_time": "2022-11-10T16:12:50.730634",
     "exception": false,
     "start_time": "2022-11-10T16:12:50.698444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) \"Buy2Buy\" Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 113.735315,
     "end_time": "2022-11-10T16:14:44.498182",
     "exception": false,
     "start_time": "2022-11-10T16:12:50.762867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv('data/training_set.csv')\n",
    "\n",
    "user_list = df.groupby('UserId')['ItemId'].nunique()\n",
    "user_list_denoise = user_list[(user_list<=20) & (user_list>=3)].index.to_list()\n",
    "df = df[df.UserId.isin(user_list_denoise)]\n",
    "\n",
    "df['type'] = df['Purchase']\n",
    "df.shape\n",
    "\n",
    "df = df.loc[df['type'].isin([1])] # ONLY WANT CARTS AND ORDERS\n",
    "# CREATE PAIRS\n",
    "df = df.merge(df,on='UserId')\n",
    "df = df.loc[(df.ItemId_x != df.ItemId_y)] # 14 DAYS\n",
    "# ASSIGN WEIGHTS\n",
    "df = df[['UserId', 'ItemId_x', 'ItemId_y','type_y']].drop_duplicates(['UserId', 'ItemId_x', 'ItemId_y'])\n",
    "df['wgt'] = 1\n",
    "df = df[['ItemId_x', 'ItemId_y','wgt']]\n",
    "df.wgt = df.wgt.astype('float32')\n",
    "df = df.groupby(['ItemId_x', 'ItemId_y']).wgt.sum()\n",
    "\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "df = df.reset_index()\n",
    "df = df.sort_values(['ItemId_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 40\n",
    "df = df.reset_index(drop=True)\n",
    "df['n'] = df.groupby('ItemId_x').ItemId_y.cumcount()\n",
    "df = df.loc[df.n<15].drop('n',axis=1)\n",
    "# SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "\n",
    "buy_order = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_order.to_csv(f'{outpath}/buy_order_denoise.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## click_order_action_num_reverse_denoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/training_set.csv')\n",
    "user_list = df.groupby('UserId')['ItemId'].nunique()\n",
    "user_list_denoise = user_list[(user_list<=20) & (user_list>=3)].index.to_list()\n",
    "df = df[df.UserId.isin(user_list_denoise)]\n",
    "\n",
    "def add_action_num_reverse_chrono(df):\n",
    "    df['action_num_reverse_chrono'] = df.groupby('UserId').cumcount(ascending=False)\n",
    "    return df\n",
    "\n",
    "def add_session_length(df):\n",
    "    tmp = df.groupby('UserId')['ItemId'].nunique().reset_index().rename(columns={'ItemId': 'session_length'})\n",
    "    df = df.merge(tmp, on = 'UserId', how = 'left')\n",
    "    return df\n",
    "\n",
    "def add_log_recency_score(df):\n",
    "    linear_interpolation = 0.1 + ((1-0.1) / (df['session_length']-1)) * (df['session_length']-df['action_num_reverse_chrono']-1)\n",
    "    df['log_recency_score'] = pd.Series(2**linear_interpolation - 1).fillna(1)\n",
    "    return df\n",
    "\n",
    "def add_type_weighted_log_recency_score(df):\n",
    "    type_weights = {0:1, 1:5}\n",
    "    type_weighted_log_recency_score = pd.Series(df['Purchase'].apply(lambda x: type_weights[x]) * df['log_recency_score'])\n",
    "    df['type_weighted_log_recency_score'] = type_weighted_log_recency_score\n",
    "    return df\n",
    "\n",
    "def apply(df, pipeline):\n",
    "    for f in pipeline:\n",
    "        df = f(df)\n",
    "    return df\n",
    "\n",
    "pipeline = [add_action_num_reverse_chrono, add_session_length, add_log_recency_score, add_type_weighted_log_recency_score]\n",
    "\n",
    "df = apply(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['ts'] = df['action_num_reverse_chrono']\n",
    "# CREATE PAIRS\n",
    "df = df.merge(df,on='UserId')\n",
    "df = df.loc[(df.ItemId_x != df.ItemId_y)]\n",
    "# ASSIGN WEIGHTS\n",
    "df = df[['UserId', 'ItemId_x', 'ItemId_y','ts_x']].drop_duplicates(['UserId', 'ItemId_x', 'ItemId_y'])\n",
    "df['wgt'] = 1 + 3*(df.ts_x - 1)/100\n",
    "df = df[['ItemId_x', 'ItemId_y','wgt']]\n",
    "df.wgt = df.wgt.astype('float32')\n",
    "df = df.groupby(['ItemId_x', 'ItemId_y']).wgt.sum()\n",
    "\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "df = df.reset_index()\n",
    "df = df.sort_values(['ItemId_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 40\n",
    "df = df.reset_index(drop=True)\n",
    "df['n'] = df.groupby('ItemId_x').ItemId_y.cumcount()\n",
    "df = df.loc[df.n<15].drop('n',axis=1)\n",
    "# SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "\n",
    "click_order = df.copy()\n",
    "click_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_order.to_csv(f'{outpath}/click_order_action_num_reverse_denoise.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## click_order_log_recency_score_denoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/training_set.csv')\n",
    "user_list = df.groupby('UserId')['ItemId'].nunique()\n",
    "user_list_denoise = user_list[(user_list<=20) & (user_list>=3)].index.to_list()\n",
    "df = df[df.UserId.isin(user_list_denoise)]\n",
    "\n",
    "def add_action_num_reverse_chrono(df):\n",
    "    df['action_num_reverse_chrono'] = df.groupby('UserId').cumcount(ascending=False)\n",
    "    return df\n",
    "\n",
    "def add_session_length(df):\n",
    "    tmp = df.groupby('UserId')['ItemId'].nunique().reset_index().rename(columns={'ItemId': 'session_length'})\n",
    "    df = df.merge(tmp, on = 'UserId', how = 'left')\n",
    "    return df\n",
    "\n",
    "def add_log_recency_score(df):\n",
    "    linear_interpolation = 0.1 + ((1-0.1) / (df['session_length']-1)) * (df['session_length']-df['action_num_reverse_chrono']-1)\n",
    "    df['log_recency_score'] = pd.Series(2**linear_interpolation - 1).fillna(1)\n",
    "    return df\n",
    "\n",
    "def add_type_weighted_log_recency_score(df):\n",
    "    type_weights = {0:1, 1:5}\n",
    "    type_weighted_log_recency_score = pd.Series(df['Purchase'].apply(lambda x: type_weights[x]) * df['log_recency_score'])\n",
    "    df['type_weighted_log_recency_score'] = type_weighted_log_recency_score\n",
    "    return df\n",
    "\n",
    "def apply(df, pipeline):\n",
    "    for f in pipeline:\n",
    "        df = f(df)\n",
    "    return df\n",
    "\n",
    "pipeline = [add_action_num_reverse_chrono, add_session_length, add_log_recency_score, add_type_weighted_log_recency_score]\n",
    "\n",
    "df = apply(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['ts'] = df['log_recency_score']\n",
    "# CREATE PAIRS\n",
    "df = df.merge(df,on='UserId')\n",
    "df = df.loc[(df.ItemId_x != df.ItemId_y)]\n",
    "# ASSIGN WEIGHTS\n",
    "df = df[['UserId', 'ItemId_x', 'ItemId_y','ts_x']].drop_duplicates(['UserId', 'ItemId_x', 'ItemId_y'])\n",
    "df['wgt'] = df.ts_x \n",
    "df = df[['ItemId_x', 'ItemId_y','wgt']]\n",
    "df.wgt = df.wgt.astype('float32')\n",
    "df = df.groupby(['ItemId_x', 'ItemId_y']).wgt.sum()\n",
    "\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "df = df.reset_index()\n",
    "df = df.sort_values(['ItemId_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 40\n",
    "df = df.reset_index(drop=True)\n",
    "df['n'] = df.groupby('ItemId_x').ItemId_y.cumcount()\n",
    "df = df.loc[df.n<15].drop('n',axis=1)\n",
    "# SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "\n",
    "click_order = df.copy()\n",
    "click_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_order.to_csv(f'{outpath}/click_order_log_recency_score_denoise.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## click_order_type_weighted_log_recency_score_denoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/training_set.csv')\n",
    "user_list = df.groupby('UserId')['ItemId'].nunique()\n",
    "user_list_denoise = user_list[(user_list<=20) & (user_list>=3)].index.to_list()\n",
    "df = df[df.UserId.isin(user_list_denoise)]\n",
    "\n",
    "def add_action_num_reverse_chrono(df):\n",
    "    df['action_num_reverse_chrono'] = df.groupby('UserId').cumcount(ascending=False)\n",
    "    return df\n",
    "\n",
    "def add_session_length(df):\n",
    "    tmp = df.groupby('UserId')['ItemId'].nunique().reset_index().rename(columns={'ItemId': 'session_length'})\n",
    "    df = df.merge(tmp, on = 'UserId', how = 'left')\n",
    "    return df\n",
    "\n",
    "def add_log_recency_score(df):\n",
    "    linear_interpolation = 0.1 + ((1-0.1) / (df['session_length']-1)) * (df['session_length']-df['action_num_reverse_chrono']-1)\n",
    "    df['log_recency_score'] = pd.Series(2**linear_interpolation - 1).fillna(1)\n",
    "    return df\n",
    "\n",
    "def add_type_weighted_log_recency_score(df):\n",
    "    type_weights = {0:1, 1:5}\n",
    "    type_weighted_log_recency_score = pd.Series(df['Purchase'].apply(lambda x: type_weights[x]) * df['log_recency_score'])\n",
    "    df['type_weighted_log_recency_score'] = type_weighted_log_recency_score\n",
    "    return df\n",
    "\n",
    "def apply(df, pipeline):\n",
    "    for f in pipeline:\n",
    "        df = f(df)\n",
    "    return df\n",
    "\n",
    "pipeline = [add_action_num_reverse_chrono, add_session_length, add_log_recency_score, add_type_weighted_log_recency_score]\n",
    "\n",
    "df = apply(df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['ts'] = df['type_weighted_log_recency_score']\n",
    "# CREATE PAIRS\n",
    "df = df.merge(df,on='UserId')\n",
    "df = df.loc[(df.ItemId_x != df.ItemId_y)]\n",
    "# ASSIGN WEIGHTS\n",
    "df = df[['UserId', 'ItemId_x', 'ItemId_y','ts_x']].drop_duplicates(['UserId', 'ItemId_x', 'ItemId_y'])\n",
    "df['wgt'] = df.ts_x \n",
    "df = df[['ItemId_x', 'ItemId_y','wgt']]\n",
    "df.wgt = df.wgt.astype('float32')\n",
    "df = df.groupby(['ItemId_x', 'ItemId_y']).wgt.sum()\n",
    "\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "df = df.reset_index()\n",
    "df = df.sort_values(['ItemId_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 40\n",
    "df = df.reset_index(drop=True)\n",
    "df['n'] = df.groupby('ItemId_x').ItemId_y.cumcount()\n",
    "df = df.loc[df.n<15].drop('n',axis=1)\n",
    "# SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "\n",
    "click_order = df.copy()\n",
    "click_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_order.to_csv(f'{outpath}/click_order_type_weighted_log_recency_score_denoise.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Step 2 - ReRank (choose 20) using handcrafted rules\n",
    "For description of the handcrafted rules, read this notebook's intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/public_testset.csv', names = ['user_id']+[f'item_id_{i}' for i in range(1,1001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def pqt_to_dict(df):\n",
    "    return df.groupby('ItemId_x').ItemId_y.apply(list).to_dict()\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "\n",
    "top_20_buys = pqt_to_dict(cart_order)\n",
    "\n",
    "top_20_buy2buy = pqt_to_dict( buy_order )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len( top_20_buy2buy ), len( top_20_buys ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#type_weight_multipliers = {'clicks': 1, 'carts': 6, 'orders': 3}\n",
    "type_weight_multipliers = {0: 1, 1: 5}\n",
    "\n",
    "df = pd.read_csv('data/data_final.csv')\n",
    "df['type'] = df['Purchase']\n",
    "\n",
    "def suggest_buys(df):\n",
    "    # USER HISTORY AIDS AND TYPES\n",
    "    aids= df.ItemId.tolist()\n",
    "    types = df.type.tolist()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    df = df.loc[(df['type']==1)]\n",
    "    unique_buys = list(dict.fromkeys( df.ItemId.tolist()[::-1] ))\n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "    # RERANK CANDIDATES\n",
    "    result = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(20) if aid2 not in unique_aids] \n",
    "    # USE TOP20 TEST ORDERS\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Create Submission CSV\n",
    "Inferring test data with Pandas groupby is slow. We need to accelerate the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import itertools\n",
    "pred_df_buys = df.groupby([\"UserId\"]).apply(\n",
    "    lambda x: suggest_buys(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df_buys = pred_df_buys.reset_index()\n",
    "pred_df_buys.rename(columns = {0: \"list_item\"}, inplace = True)\n",
    "submit = pred_df_buys[pred_df_buys.UserId.isin(test.user_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để tạo ra một danh sách có độ dài 1000, điền thêm NaN nếu cần\n",
    "def pad_list(lst, length=1000):\n",
    "    return lst + [\"0\"] * (length - len(lst))\n",
    "\n",
    "# Tạo DataFrame với 1000 cột\n",
    "submit = pd.DataFrame(\n",
    "    submit['list_item'].apply(lambda x: pad_list(x)).tolist(),\n",
    "    index=submit['UserId'],\n",
    "    columns=[f'item_{i+1}' for i in range(1000)]\n",
    ").reset_index()\n",
    "\n",
    "# Đổi tên cột index thành user_id\n",
    "submit.rename(columns={'index': 'UserId'}, inplace=True)\n",
    "\n",
    "# Hiển thị DataFrame\n",
    "submit.to_csv(f'{outpath}/predict.csv', index = None, header = None)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 4493939,
     "sourceId": 38760,
     "sourceType": "competition"
    },
    {
     "datasetId": 2597726,
     "sourceId": 4436180,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6047721,
     "sourceId": 9933751,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5965973,
     "sourceId": 9954612,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30301,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
